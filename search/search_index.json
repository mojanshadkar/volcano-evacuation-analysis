{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Volcano Evacuation Analysis","text":""},{"location":"index.html#project-overview","title":"Project Overview","text":"<p>This project provides tools for analyzing and visualizing evacuation scenarios from active volcanoes. The analysis pipeline includes:</p> <ol> <li>Data Acquisition - Download DEM (Digital Elevation Model) and land cover data for the study area</li> <li>Cost Surface Generation - Calculate anisotropic cost surfaces based on terrain and land cover</li> <li>Evacuation Analysis - Determine optimal evacuation paths and travel times from different locations</li> </ol>"},{"location":"index.html#key-features","title":"Key Features","text":"<ul> <li>Anisotropic cost calculation considering both terrain slope and land cover</li> <li>Multi-directional path analysis (8 cardinal directions)</li> <li>Travel time estimation for different walking speeds</li> <li>Visualization of evacuation paths and travel times</li> <li>Safe zone analysis at different distances from the summit</li> </ul>"},{"location":"index.html#case-studies-mount-marapi-and-mount-awu","title":"Case Studies: Mount Marapi and Mount Awu","text":"<p>The current implementation focuses on Mount Marapi and Mount Awu in Indonesia, demonstrating how the tools can be applied to real-world evacuation planning.</p>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>See the Getting Started guide to begin using these tools for your own volcano evacuation analysis.</p>"},{"location":"enhanced-index.html","title":"Volcano Pedestrian Evacuation Analysis","text":""},{"location":"enhanced-index.html#project-overview","title":"Project Overview","text":"<p>This project provides a comprehensive toolkit for analyzing and visualizing pedestrian evacuation scenarios from active volcanoes. The analysis pipeline combines terrain characteristics, land cover data, and evacuation modeling to support emergency planning and risk reduction.</p> <p></p>"},{"location":"enhanced-index.html#complete-analysis-workflow","title":"Complete Analysis Workflow","text":"<p>Our toolkit implements a full evacuation analysis workflow:</p> <ol> <li>Data Acquisition - Download high-resolution DEM (Digital Elevation Model) and land cover data using Google Earth Engine</li> <li>Cost Surface Generation - Create anisotropic cost surfaces based on Tobler's hiking function and land cover characteristics</li> <li>Evacuation Analysis - Calculate optimal evacuation routes and travel times from critical locations</li> <li>Probability Analysis - Incorporate eruption probability thresholds for different VEI (Volcanic Explosivity Index) scenarios</li> </ol>"},{"location":"enhanced-index.html#key-features","title":"Key Features","text":"<ul> <li>Advanced Terrain Analysis</li> <li>Slope calculation in 8 cardinal and intercardinal directions</li> <li>Implementation of Tobler's hiking function for realistic walking speeds</li> <li> <p>Integration of land cover data to account for terrain traversability</p> </li> <li> <p>Sophisticated Path Finding</p> </li> <li>Dijkstra's algorithm for optimal route determination</li> <li>Multi-directional path analysis considering anisotropic movement</li> <li> <p>Evacuation time calculations for different population mobility scenarios (slow, medium, fast walking speeds)</p> </li> <li> <p>Comprehensive Visualization</p> </li> <li>Interactive maps of evacuation routes and travel times</li> <li>Statistical analysis of evacuation feasibility</li> <li> <p>Visual comparison of different evacuation scenarios</p> </li> <li> <p>Eruption Probability Integration</p> </li> <li>Safe zone definition based on probabilistic hazard thresholds</li> <li>Scenario analysis for different volcanic eruption intensities</li> <li>Comparison of evacuation strategies across multiple VEI levels</li> </ul>"},{"location":"enhanced-index.html#case-studies-mount-marapi-and-mount-awu","title":"Case Studies: Mount Marapi and Mount Awu","text":"<p>The toolkit has been applied to Mount Marapi (Sumatra) and Mount Awu (Sangihe Islands) in Indonesia, demonstrating its effectiveness in diverse volcanic settings. These case studies provide:</p> <ul> <li>Realistic evacuation time estimates from summit and camping areas</li> <li>Identification of optimal evacuation routes based on terrain characteristics</li> <li>Assessment of safe zone accessibility for different population groups</li> <li>Scenario comparisons for various eruption intensities</li> </ul>"},{"location":"enhanced-index.html#modular-architecture","title":"Modular Architecture","text":"<p>The toolkit is organized into three main module groups:</p> <ul> <li>Cost Calculation: Functions for generating cost surfaces from geographical data</li> <li>Evacuation Analysis: Tools for path finding and evacuation time analysis</li> <li>Probability Analysis: Methods for incorporating eruption probability thresholds</li> </ul> <p>Each module is thoroughly documented with detailed API references and usage examples.</p>"},{"location":"enhanced-index.html#getting-started","title":"Getting Started","text":"<p>To begin using the toolkit for your own volcano evacuation analysis:</p> <ol> <li>Check the Installation Requirements to set up your environment</li> <li>Follow the Workflow Guide for step-by-step instructions</li> <li>Explore the Demo Notebooks for practical examples</li> </ol> <p>For technical details, see the API Reference with full documentation of all functions and modules.</p>"},{"location":"getting-started.html","title":"Getting Started","text":"<p>Welcome to the Volcano Pedestrian Evacuation Analysis toolkit! This guide will help you get started with setting up and running volcanic evacuation analyses.</p>"},{"location":"getting-started.html#introduction","title":"Introduction","text":"<p>This toolkit provides a comprehensive set of tools for analyzing pedestrian evacuation scenarios from active volcanoes. It allows you to:</p> <ul> <li>Download topographic and land cover data for volcanic regions</li> <li>Generate cost surfaces that model the difficulty of traversing different terrain types</li> <li>Calculate optimal evacuation routes and evacuation times</li> <li>Analyze evacuation scenarios based on probability thresholds for different eruption intensities</li> <li>Visualize results with informative maps and charts</li> </ul>"},{"location":"getting-started.html#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.8+</li> <li>Operating System: Windows, macOS, or Linux</li> <li>RAM: 8GB minimum, 16GB recommended for larger study areas</li> <li>Storage: At least 10GB free space for data and results</li> <li>Internet Connection: Required for initial data download from Google Earth Engine</li> </ul>"},{"location":"getting-started.html#quick-start","title":"Quick Start","text":"<p>Follow these steps to get started with the toolkit:</p> <ol> <li> <p>Install Required Dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Set Up Google Earth Engine Account:</p> </li> <li>Visit Google Earth Engine and sign up for an account</li> <li> <p>Follow the authentication instructions in the Data Acquisition workflow</p> </li> <li> <p>Download Sample Data:</p> </li> <li>Run the data download notebook to acquire DEM and land cover data for your volcano of interest</li> <li> <p>Alternatively, use the pre-processed sample data provided in the repository</p> </li> <li> <p>Generate Cost Surface:</p> </li> <li>Run the cost surface generation workflow to create traversal cost layers</li> <li> <p>Examine the output to ensure it accurately represents the terrain conditions</p> </li> <li> <p>Perform Evacuation Analysis:</p> </li> <li>Execute the evacuation analysis to identify optimal routes and calculate evacuation times</li> <li>Review the visualization outputs to understand evacuation scenarios</li> </ol>"},{"location":"getting-started.html#workflow-overview","title":"Workflow Overview","text":"<p>The toolkit follows a modular workflow:</p> <ol> <li>Data Acquisition: Download DEM and land cover data</li> <li>Cost Surface Generation: Create terrain-based traversal cost surfaces</li> <li>Evacuation Analysis: Calculate evacuation routes and times</li> </ol> <p>Each module can be run independently with the outputs from the previous step, allowing for flexible workflow management and iterative analysis.</p>"},{"location":"getting-started.html#limitations","title":"Limitations","text":"<ul> <li>The analysis is focused on pedestrian evacuation (not vehicular)</li> <li>Travel speeds are based on Tobler's hiking function and land cover-based assumptions</li> <li>The model does not account for potential infrastructure damage during eruptions</li> <li>Evacuation behavior (such as panic or group dynamics) is not modeled</li> </ul>"},{"location":"getting-started.html#getting-help","title":"Getting Help","text":"<p>If you encounter any issues or have questions:</p> <ol> <li>Check the detailed documentation for each module</li> <li>Review example notebooks in the demo directory</li> <li>Submit an issue on our GitHub repository</li> </ol>"},{"location":"getting-started.html#next-steps","title":"Next Steps","text":"<p>To begin your evacuation analysis project, proceed to the Workflow Overview section for a detailed description of each step in the process.</p>"},{"location":"installation.html","title":"Installation Guide","text":"<p>This guide explains how to set up your environment for using the Volcano Pedestrian Evacuation Analysis toolkit.</p>"},{"location":"installation.html#prerequisites","title":"Prerequisites","text":"<p>Before installing the toolkit, ensure you have:</p> <ul> <li>Python 3.8+ installed on your system</li> <li>pip (Python package installer) or conda for managing packages</li> <li>Basic familiarity with command-line operations</li> </ul>"},{"location":"installation.html#environment-setup","title":"Environment Setup","text":"<p>We recommend creating a dedicated virtual environment to manage dependencies cleanly:</p> Using condaUsing venv <pre><code># Create a new conda environment\nconda create -n volcano-evacuation python=3.8\n\n# Activate the environment\nconda activate volcano-evacuation\n</code></pre> <pre><code># Create a new virtual environment\npython -m venv volcano-env\n\n# Activate the environment (Windows)\nvolcano-env\\Scripts\\activate\n\n# Activate the environment (macOS/Linux)\nsource volcano-env/bin/activate\n</code></pre>"},{"location":"installation.html#installing-required-packages","title":"Installing Required Packages","text":""},{"location":"installation.html#step-by-step-installation","title":"Step-by-Step Installation","text":""},{"location":"installation.html#1-core-scientific-computing","title":"1. Core Scientific Computing","text":"<p>These packages provide essential numerical and data processing capabilities:</p> <pre><code>pip install numpy scipy pandas matplotlib tqdm\n</code></pre>"},{"location":"installation.html#2-geospatial-processing","title":"2. Geospatial Processing","text":"<p>These libraries enable handling of geographic data formats and analysis:</p> <pre><code>pip install rasterio geopandas pyproj fiona affine\n</code></pre>"},{"location":"installation.html#3-google-earth-engine-integration","title":"3. Google Earth Engine Integration","text":"<p>For accessing remote sensing data (required for the Data Acquisition module):</p> <pre><code>pip install earthengine-api\n\n# Authenticate with Earth Engine\nearthengine authenticate\n</code></pre>"},{"location":"installation.html#package-overview","title":"Package Overview","text":"Category Package Purpose Core Computing numpy Efficient array operations and numerical computations scipy Scientific algorithms including sparse matrix operations pandas Data analysis and manipulation matplotlib Data visualization and plotting tqdm Progress bars for long-running operations Geospatial rasterio Reading, writing and processing raster data geopandas Working with vector geospatial data pyproj Coordinate system transformations fiona Low-level interface to vector data formats affine Working with affine transformations Remote Sensing earthengine-api Access to Google Earth Engine datasets"},{"location":"installation.html#verifying-your-installation","title":"Verifying Your Installation","text":"<p>Run the following script to verify that all required packages are installed correctly:</p> <pre><code># check_installation.py\nimport sys\n\nrequired_packages = [\n    \"numpy\", \"scipy\", \"pandas\", \"matplotlib\", \"tqdm\",\n    \"rasterio\", \"geopandas\", \"pyproj\", \"fiona\", \"affine\",\n    \"ee\"  # Google Earth Engine API\n]\n\nmissing = []\nfor package in required_packages:\n    try:\n        __import__(package)\n        print(f\"\u2713 {package}\")\n    except ImportError:\n        missing.append(package)\n        print(f\"\u2717 {package} (MISSING)\")\n\nif missing:\n    print(\"\\nMissing packages. Install them with:\")\n    print(f\"pip install {' '.join(missing)}\")\nelse:\n    print(\"\\nAll required packages are installed!\")\n</code></pre>"},{"location":"installation.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation.html#gdalrasterio-installation-issues","title":"GDAL/Rasterio Installation Issues","text":"<p>If you encounter problems installing rasterio or other GDAL-based packages:</p> WindowsLinux (Ubuntu/Debian)macOS <pre><code># Use pipwin to install pre-compiled binaries\npip install pipwin\npipwin install gdal\npipwin install rasterio\npipwin install fiona\n</code></pre> <pre><code># Install system dependencies first\nsudo apt-get update\nsudo apt-get install libgdal-dev gdal-bin\n\n# Then install Python packages\npip install gdal==$(gdal-config --version) --no-binary=gdal\npip install rasterio fiona\n</code></pre> <pre><code># Using Homebrew\nbrew install gdal\n\n# Then install Python packages\npip install gdal rasterio fiona\n</code></pre>"},{"location":"installation.html#google-earth-engine-authentication","title":"Google Earth Engine Authentication","text":"<p>If you encounter issues authenticating with Google Earth Engine:</p> <ol> <li>Ensure you have signed up for Earth Engine access</li> <li>Try re-running the authentication: <code>earthengine authenticate --quiet</code></li> <li>If browser authentication doesn't work, try the manual authentication option</li> </ol>"},{"location":"installation.html#next-steps","title":"Next Steps","text":"<p>Once you've completed the installation:</p> <ol> <li>Proceed to the Data Acquisition section to download required datasets</li> <li>Or explore the Demo Notebooks for examples of the toolkit in action</li> </ol>"},{"location":"api/index.html","title":"API Reference","text":"<p>This section provides documentation for the Python modules that make up the Volcano Pedestrian Evacuation Analysis toolkit. Here you can explore the functionality and implementation details of each module.</p>"},{"location":"api/index.html#documentation-and-source-code","title":"Documentation and Source Code","text":"<p>The API Reference is organized in two ways:</p> <ol> <li> <p>Documentation Pages: This section contains detailed documentation for each module, including function descriptions, parameters, and examples.</p> </li> <li> <p>Source Code: All Python modules are also available for direct download in the Source Code section, where you can get the complete implementation files.</p> </li> </ol>"},{"location":"api/index.html#how-to-use-these-modules","title":"How to Use These Modules","text":"<p>You can use these Python modules in several ways:</p> <ol> <li> <p>Read the Documentation: Browse through this section to understand the functionality and API details.</p> </li> <li> <p>Download the Source Code: Visit the Source Code section to download the <code>.py</code> files directly.</p> </li> <li> <p>Integration in Your Project: Place the downloaded <code>.py</code> files in your project directory or in a subdirectory (e.g., <code>/lib</code> or <code>/src</code>).</p> </li> <li> <p>Import in Your Code: Import the modules as needed in your Python scripts:    <pre><code># Example imports\nfrom cost_calculations import map_landcover_to_cost\nfrom dem_processing import calculate_slope\nfrom data_loading import read_shapefile\n</code></pre></p> </li> </ol>"},{"location":"api/index.html#module-organization","title":"Module Organization","text":"<p>The API is organized into three main components:</p>"},{"location":"api/index.html#cost-calculation","title":"Cost Calculation","text":"<p>These modules handle the creation of cost surfaces from DEM and land cover data:</p> <ul> <li>Data Loading (download .py): Functions for loading shapefiles and raster data</li> <li>DEM Processing (download .py): Functions for calculating slopes and walking speeds</li> <li>Cost Calculations (download .py): Core functions for creating and manipulating cost surfaces</li> <li>Plotting Utils (download .py): Visualization functions for raster data and analysis results</li> </ul>"},{"location":"api/index.html#evacuation-analysis","title":"Evacuation Analysis","text":"<p>These modules provide tools for calculating evacuation routes and times:</p> <ul> <li>IO Utilities (download .py): Input/output utilities</li> <li>Configuration (download .py): Configuration settings and parameters</li> <li>Analysis (download .py): Core evacuation analysis functions</li> <li>Grid Utilities (download .py): Utilities for working with raster grids</li> <li>Path Utilities (download .py): Utilities for path finding and graph operations</li> <li>Decomposition (download .py): Functions for analyzing factor contributions</li> <li>Visualization (download .py): Functions for visualizing evacuation analysis results</li> </ul>"},{"location":"api/index.html#probability-analysis","title":"Probability Analysis","text":"<p>These modules extend the evacuation analysis to include probability thresholds:</p> <ul> <li>Data Utils (download .py): Data handling utilities</li> <li>Raster Utilities (download .py): Raster processing and coordinate transformations</li> <li>Analysis (download .py): Probability-based evacuation analysis</li> <li>Graph Utilities (download .py): Graph construction and shortest path algorithms</li> <li>Visualization (download .py): Visualization functions for probability analysis</li> </ul>"},{"location":"api/index.html#dependencies","title":"Dependencies","text":"<p>Before using these modules, ensure you've installed all the required dependencies as specified in the Installation Guide page.</p>"},{"location":"api/index.html#api-version-and-compatibility","title":"API Version and Compatibility","text":"<p>These modules are compatible with Python 3.8+ and have been tested with the library versions specified in the installation requirements. When upgrading dependencies, always test your workflows to ensure compatibility.</p>"},{"location":"api/cost-calculation/cost-calculations.html","title":"Cost Calculations","text":"<p>The <code>cost_calculations.py</code> module provides functions for calculating and manipulating cost surfaces for least-cost path analysis.</p>"},{"location":"api/cost-calculation/cost-calculations.html#functions","title":"Functions","text":""},{"location":"api/cost-calculation/cost-calculations.html#map_landcover_to_cost","title":"<code>map_landcover_to_cost</code>","text":"<pre><code>def map_landcover_to_cost(landcover_data, land_cover_cost_mapping)\n</code></pre> <p>Maps land cover classes to their corresponding cost values.</p> <p>Parameters:</p> <ul> <li><code>landcover_data (numpy.ndarray)</code>: A 2D array representing land cover classes.</li> <li><code>land_cover_cost_mapping (dict)</code>: A dictionary where keys are land cover class identifiers and values are the corresponding cost values.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: A 2D array of the same shape as landcover_data, where each element is the cost value corresponding to the land cover class at that position.</li> </ul>"},{"location":"api/cost-calculation/cost-calculations.html#rasterize_layer","title":"<code>rasterize_layer</code>","text":"<pre><code>def rasterize_layer(geometry_list, out_shape, transform, burn_value, fill_value=np.nan)\n</code></pre> <p>Rasterizes a given geometry.</p> <p>Parameters:</p> <ul> <li><code>geometry_list (list)</code>: A list of geometries to rasterize.</li> <li><code>out_shape (tuple)</code>: The shape of the output array (height, width).</li> <li><code>transform (Affine)</code>: The affine transformation to apply.</li> <li><code>burn_value (float)</code>: The value to burn into the raster for the geometries.</li> <li><code>fill_value (float, optional)</code>: The value to fill the raster where there are no geometries. Default is np.nan.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: A 2D array with the rasterized geometries.</li> </ul>"},{"location":"api/cost-calculation/cost-calculations.html#update_cost_raster","title":"<code>update_cost_raster</code>","text":"<pre><code>def update_cost_raster(landcover_cost_data, stream_raster, hiking_path_raster)\n</code></pre> <p>Updates the cost raster based on landcover data, stream locations, and hiking paths.</p> <p>Parameters:</p> <ul> <li><code>landcover_cost_data (numpy.ndarray)</code>: A 2D array representing the cost associated with different landcover types.</li> <li><code>stream_raster (numpy.ndarray)</code>: A 2D array where non-NaN values indicate the presence of streams.</li> <li><code>hiking_path_raster (numpy.ndarray)</code>: A 2D array where non-NaN values indicate the presence of hiking paths.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: A 2D array with updated cost values where streams are marked as impassable (cost = 0) and hiking paths are marked as passable (cost = 1).</li> </ul>"},{"location":"api/cost-calculation/cost-calculations.html#adjust_cost_with_walking_speed","title":"<code>adjust_cost_with_walking_speed</code>","text":"<pre><code>def adjust_cost_with_walking_speed(normalized_walking_speed_array, combined_data)\n</code></pre> <p>Adjusts the cost based on the normalized walking speed.</p> <p>Parameters:</p> <ul> <li><code>normalized_walking_speed_array (numpy.ndarray)</code>: An array of normalized walking speeds.</li> <li><code>combined_data (numpy.ndarray)</code>: An array of combined data.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: An array of adjusted costs.</li> </ul>"},{"location":"api/cost-calculation/cost-calculations.html#invert_cost_array","title":"<code>invert_cost_array</code>","text":"<pre><code>def invert_cost_array(adjusted_cost_array)\n</code></pre> <p>Inverts the values in the given cost array.</p> <p>This function takes an array of adjusted costs and inverts each value. If a value is zero, it is replaced with NaN before inversion to avoid division by zero. After inversion, NaN values are replaced with a large number (1e6).</p> <p>Parameters:</p> <ul> <li><code>adjusted_cost_array (numpy.ndarray)</code>: A numpy array of adjusted costs.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: A numpy array with the inverted cost values.</li> </ul>"},{"location":"api/cost-calculation/cost-calculations.html#invert_walking_speed","title":"<code>invert_walking_speed</code>","text":"<pre><code>def invert_walking_speed(normalized_walking_speed_array)\n</code></pre> <p>Inverts walking speed values and handles zeros/NaNs appropriately.</p> <p>Parameters:</p> <ul> <li><code>normalized_walking_speed_array (numpy.ndarray)</code>: Array of normalized walking speeds.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: Array with inverted walking speed values.</li> </ul>"},{"location":"api/cost-calculation/cost-calculations.html#invert_cost_raster","title":"<code>invert_cost_raster</code>","text":"<pre><code>def invert_cost_raster(updated_cost_raster)\n</code></pre> <p>Inverts cost raster values and handles zeros/NaNs appropriately.</p> <p>Parameters:</p> <ul> <li><code>updated_cost_raster (numpy.ndarray)</code>: Cost raster to be inverted.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: Inverted cost raster where zero values are handled by converting to NaN and then to 1e6.</li> </ul>"},{"location":"api/cost-calculation/data-loading.html","title":"Data Loading","text":"<p>The <code>data_loading.py</code> module provides functions for loading geospatial data from shapefiles and raster files.</p>"},{"location":"api/cost-calculation/data-loading.html#functions","title":"Functions","text":""},{"location":"api/cost-calculation/data-loading.html#read_shapefile","title":"<code>read_shapefile</code>","text":"<pre><code>def read_shapefile(path)\n</code></pre> <p>Load a shapefile and return it as a GeoDataFrame.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: The file path to the shapefile.</li> </ul> <p>Returns:</p> <ul> <li><code>gpd.GeoDataFrame</code>: A GeoDataFrame containing the data from the shapefile.</li> </ul>"},{"location":"api/cost-calculation/data-loading.html#read_raster","title":"<code>read_raster</code>","text":"<pre><code>def read_raster(path)\n</code></pre> <p>Reads a raster file and returns its data, profile, transform, CRS, and nodata value.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: The file path to the raster file.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>data (numpy.ndarray)</code>: The raster data.</li> <li><code>profile (dict)</code>: The profile metadata of the raster.</li> <li><code>transform (affine.Affine)</code>: The affine transform of the raster.</li> <li><code>crs (rasterio.crs.CRS)</code>: The coordinate reference system of the raster.</li> <li><code>nodata (float or int)</code>: The nodata value of the raster.</li> </ul>"},{"location":"api/cost-calculation/dem-processing.html","title":"DEM Processing","text":"<p>The <code>dem_processing.py</code> module provides functions for processing Digital Elevation Models (DEMs) to calculate slope and walking speeds based on Tobler's Hiking Function.</p>"},{"location":"api/cost-calculation/dem-processing.html#functions","title":"Functions","text":""},{"location":"api/cost-calculation/dem-processing.html#calculate_slope","title":"<code>calculate_slope</code>","text":"<pre><code>def calculate_slope(dem_data, resolution_x, resolution_y, no_data)\n</code></pre> <p>Calculates the slope in eight directions from a DEM.</p> <p>Parameters:</p> <ul> <li><code>dem_data (numpy.ndarray)</code>: 2D array representing the DEM data.</li> <li><code>resolution_x (float)</code>: The resolution of the DEM in the x direction.</li> <li><code>resolution_y (float)</code>: The resolution of the DEM in the y direction.</li> <li><code>no_data (float)</code>: The value representing no data in the DEM.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: A 3D array where the first dimension represents the eight directions (North, South, East, West, North-East, North-West, South-East, South-West), and the other two dimensions represent the slope values for each cell in the DEM.</li> </ul>"},{"location":"api/cost-calculation/dem-processing.html#calculate_walking_speed","title":"<code>calculate_walking_speed</code>","text":"<pre><code>def calculate_walking_speed(slope_array)\n</code></pre> <p>Calculates walking speed using Tobler's Hiking Function.</p> <p>This function calculates the walking speed given an array of slope values. The walking speed is computed using the formula: <pre><code>speed = 6 * exp(-3.5 * abs(slope + 0.05))\n</code></pre></p> <p>Parameters:</p> <ul> <li><code>slope_array (numpy.ndarray)</code>: An array of slope values.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: An array of walking speeds corresponding to the input slope values.</li> </ul>"},{"location":"api/cost-calculation/dem-processing.html#get_max_velocity","title":"<code>get_max_velocity</code>","text":"<pre><code>def get_max_velocity()\n</code></pre> <p>Calculates the maximum velocity based on Tobler's Hiking Function.</p> <p>The maximum velocity occurs at a slope of zero.</p> <p>Returns:</p> <ul> <li><code>float</code>: The maximum walking velocity.</li> </ul>"},{"location":"api/cost-calculation/dem-processing.html#normalize_walking_speed","title":"<code>normalize_walking_speed</code>","text":"<pre><code>def normalize_walking_speed(walking_speed_array)\n</code></pre> <p>Normalize the walking speed array by dividing each element by the maximum velocity.</p> <p>The maximum velocity is dynamically calculated using Tobler's Hiking Function at a slope of zero.</p> <p>Parameters:</p> <ul> <li><code>walking_speed_array (numpy.ndarray)</code>: Array of walking speeds to be normalized.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: The normalized walking speed array.</li> </ul>"},{"location":"api/cost-calculation/plotting-utils.html","title":"Plotting Utils","text":"<p>The <code>plotting_utils.py</code> module provides functions for visualizing geospatial data including rasters, points, and analysis results.</p>"},{"location":"api/cost-calculation/plotting-utils.html#functions","title":"Functions","text":""},{"location":"api/cost-calculation/plotting-utils.html#plot_continuous_raster_with_points","title":"<code>plot_continuous_raster_with_points</code>","text":"<pre><code>def plot_continuous_raster_with_points(raster_data, extent, points_gdf, title, colorbar_label, output_file_path)\n</code></pre> <p>Plots a continuous raster with overlaid points.</p> <p>Parameters:</p> <ul> <li><code>raster_data (np.ndarray)</code>: The raster data to be plotted.</li> <li><code>extent (Any)</code>: The extent of the raster data.</li> <li><code>points_gdf (gpd.GeoDataFrame)</code>: GeoDataFrame containing the points to be overlaid.</li> <li><code>title (str)</code>: The title of the plot.</li> <li><code>colorbar_label (str)</code>: The label for the colorbar.</li> <li><code>output_file_path (str)</code>: The file path where the plot will be saved.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/cost-calculation/plotting-utils.html#plot_normalized_walking_speed","title":"<code>plot_normalized_walking_speed</code>","text":"<pre><code>def plot_normalized_walking_speed(raster_data, extent, points_gdf, title, output_file_path)\n</code></pre> <p>Plots the normalized walking speed raster with summit points.</p> <p>Parameters:</p> <ul> <li><code>raster_data (np.ndarray)</code>: The raster data representing normalized walking speeds.</li> <li><code>extent (Any)</code>: The extent of the raster data in the format (xmin, xmax, ymin, ymax).</li> <li><code>points_gdf (gpd.GeoDataFrame)</code>: A GeoDataFrame containing the summit points to be plotted.</li> <li><code>title (str)</code>: The title of the plot.</li> <li><code>output_file_path (str)</code>: The file path where the plot will be saved.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/cost-calculation/plotting-utils.html#plot_adjusted_cost_raster","title":"<code>plot_adjusted_cost_raster</code>","text":"<pre><code>def plot_adjusted_cost_raster(adjusted_cost_raster, extent, points_gdf, title, output_file_path)\n</code></pre> <p>Plots the adjusted cost raster with summit points.</p> <p>Parameters:</p> <ul> <li><code>adjusted_cost_raster (numpy.ndarray)</code>: A 2D array representing the adjusted cost raster data.</li> <li><code>extent (list or tuple)</code>: The bounding box in the form [xmin, xmax, ymin, ymax] for the raster.</li> <li><code>points_gdf (geopandas.GeoDataFrame)</code>: A GeoDataFrame containing the summit points with geometry column.</li> <li><code>title (str)</code>: The title of the plot.</li> <li><code>output_file_path (str)</code>: The file path where the plot image will be saved.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/cost-calculation/plotting-utils.html#plot_inverted_cost_raster","title":"<code>plot_inverted_cost_raster</code>","text":"<pre><code>def plot_inverted_cost_raster(inverted_cost_raster, extent, points_gdf, title, output_file_path)\n</code></pre> <p>Plots the inverted cost raster with summit points.</p> <p>Parameters:</p> <ul> <li><code>inverted_cost_raster (numpy.ndarray)</code>: 2D array representing the inverted cost raster.</li> <li><code>extent (list or tuple)</code>: The bounding box in data coordinates (left, right, bottom, top).</li> <li><code>points_gdf (geopandas.GeoDataFrame)</code>: GeoDataFrame containing the summit points with geometry column.</li> <li><code>title (str)</code>: Title of the plot.</li> <li><code>output_file_path (str)</code>: Path to save the output plot image.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/cost-calculation/plotting-utils.html#plot_walking_speed_vs_slope","title":"<code>plot_walking_speed_vs_slope</code>","text":"<pre><code>def plot_walking_speed_vs_slope(slope_array, walking_speed_array, directions, output_file_path)\n</code></pre> <p>Plots the walking speed versus slope for all eight directions in a subplot layout.</p> <p>Parameters:</p> <ul> <li><code>slope_array (numpy.ndarray)</code>: A 3D array representing slope values for eight directions. The first dimension represents directions: North, South, East, West, North-East, North-West, South-East, South-West.</li> <li><code>walking_speed_array (numpy.ndarray)</code>: A 3D array representing walking speed values for eight directions. The first dimension represents directions: North, South, East, West, North-East, North-West, South-East, South-West.</li> <li><code>directions (list)</code>: A list of direction labels.</li> <li><code>output_file_path (str)</code>: The file path where the combined plot will be saved.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/cost-calculation/plotting-utils.html#plot_north_east_speed_conservation","title":"<code>plot_north_east_speed_conservation</code>","text":"<pre><code>def plot_north_east_speed_conservation(normalized_walking_speed_array, extent, points_gdf, title, output_file_path)\n</code></pre> <p>Plots the normalized walking speed raster for North and East directions in a subplot.</p> <p>Parameters:</p> <ul> <li><code>normalized_walking_speed_array (np.ndarray)</code>: The 3D array of normalized walking speeds where first dimension represents directions (North: 0, East: 2)</li> <li><code>extent (Any)</code>: The extent of the raster data in the format (xmin, xmax, ymin, ymax).</li> <li><code>points_gdf (gpd.GeoDataFrame)</code>: A GeoDataFrame containing the summit points to be plotted.</li> <li><code>title (str)</code>: The main title of the plot.</li> <li><code>output_file_path (str)</code>: The file path where the plot will be saved.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/cost-calculation/New%20folder/cost-calculations.html","title":"Cost calculations","text":"<p>cost_calculations.py</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport rasterio\nfrom rasterio.features import rasterize\n</pre> import numpy as np import rasterio from rasterio.features import rasterize In\u00a0[\u00a0]: Copied! <pre>def map_landcover_to_cost(landcover_data, land_cover_cost_mapping):\n    \"\"\"\n    Maps land cover classes to their corresponding cost values.\n    Parameters:\n    landcover_data (numpy.ndarray): A 2D array representing land cover classes.\n    land_cover_cost_mapping (dict): A dictionary where keys are land cover class identifiers \n    and values are the corresponding cost values.\n    Returns:\n    numpy.ndarray: A 2D array of the same shape as landcover_data, where each element is the \n    cost value corresponding to the land cover class at that position.\n    \"\"\"\n  \n    cost_raster = np.zeros_like(landcover_data, dtype=np.float32)\n    for land_cover_class, cost_value in land_cover_cost_mapping.items():\n        cost_raster[landcover_data == land_cover_class] = cost_value\n    return cost_raster\n</pre> def map_landcover_to_cost(landcover_data, land_cover_cost_mapping):     \"\"\"     Maps land cover classes to their corresponding cost values.     Parameters:     landcover_data (numpy.ndarray): A 2D array representing land cover classes.     land_cover_cost_mapping (dict): A dictionary where keys are land cover class identifiers      and values are the corresponding cost values.     Returns:     numpy.ndarray: A 2D array of the same shape as landcover_data, where each element is the      cost value corresponding to the land cover class at that position.     \"\"\"        cost_raster = np.zeros_like(landcover_data, dtype=np.float32)     for land_cover_class, cost_value in land_cover_cost_mapping.items():         cost_raster[landcover_data == land_cover_class] = cost_value     return cost_raster In\u00a0[\u00a0]: Copied! <pre>def rasterize_layer(geometry_list, out_shape, transform, burn_value, fill_value=np.nan):\n    \"\"\"\n    Rasterizes a given geometry.\n\n    Parameters:\n    geometry_list (list): A list of geometries to rasterize.\n    out_shape (tuple): The shape of the output array (height, width).\n    transform (Affine): The affine transformation to apply.\n    burn_value (float): The value to burn into the raster for the geometries.\n    fill_value (float, optional): The value to fill the raster where there are no geometries. Default is np.nan.\n\n    Returns:\n    numpy.ndarray: A 2D array with the rasterized geometries.\n    \"\"\"\n    return rasterize(\n        [(geom, burn_value) for geom in geometry_list],\n        out_shape=out_shape,\n        transform=transform,\n        fill=fill_value,\n        all_touched=True,\n        dtype='float32'\n    )\n</pre> def rasterize_layer(geometry_list, out_shape, transform, burn_value, fill_value=np.nan):     \"\"\"     Rasterizes a given geometry.      Parameters:     geometry_list (list): A list of geometries to rasterize.     out_shape (tuple): The shape of the output array (height, width).     transform (Affine): The affine transformation to apply.     burn_value (float): The value to burn into the raster for the geometries.     fill_value (float, optional): The value to fill the raster where there are no geometries. Default is np.nan.      Returns:     numpy.ndarray: A 2D array with the rasterized geometries.     \"\"\"     return rasterize(         [(geom, burn_value) for geom in geometry_list],         out_shape=out_shape,         transform=transform,         fill=fill_value,         all_touched=True,         dtype='float32'     ) In\u00a0[\u00a0]: Copied! <pre>def update_cost_raster(landcover_cost_data, stream_raster, hiking_path_raster):\n    \"\"\"\n    Updates the cost raster based on landcover data, stream locations, and hiking paths.\n    Parameters:\n    landcover_cost_data (numpy.ndarray): A 2D array representing the cost associated with different landcover types.\n    stream_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of streams.\n    hiking_path_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of hiking paths.\n    Returns:\n    numpy.ndarray: A 2D array with updated cost values where streams are marked as impassable (cost = 0) \n    and hiking paths are marked as passable (cost = 1).\n    \"\"\"\n   \n    updated_cost_raster = landcover_cost_data.copy()\n    landcover_mask = np.isnan(landcover_cost_data)\n    updated_cost_raster[~landcover_mask &amp; ~np.isnan(stream_raster)] = 0  # Streams impassable\n    updated_cost_raster[~landcover_mask &amp; ~np.isnan(hiking_path_raster)] = 1  # Hiking paths passable\n    return updated_cost_raster\n</pre> def update_cost_raster(landcover_cost_data, stream_raster, hiking_path_raster):     \"\"\"     Updates the cost raster based on landcover data, stream locations, and hiking paths.     Parameters:     landcover_cost_data (numpy.ndarray): A 2D array representing the cost associated with different landcover types.     stream_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of streams.     hiking_path_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of hiking paths.     Returns:     numpy.ndarray: A 2D array with updated cost values where streams are marked as impassable (cost = 0)      and hiking paths are marked as passable (cost = 1).     \"\"\"         updated_cost_raster = landcover_cost_data.copy()     landcover_mask = np.isnan(landcover_cost_data)     updated_cost_raster[~landcover_mask &amp; ~np.isnan(stream_raster)] = 0  # Streams impassable     updated_cost_raster[~landcover_mask &amp; ~np.isnan(hiking_path_raster)] = 1  # Hiking paths passable     return updated_cost_raster In\u00a0[\u00a0]: Copied! <pre>def adjust_cost_with_walking_speed(normalized_walking_speed_array, combined_data):\n    \"\"\"\n    Adjusts the cost based on the normalized walking speed.\n\n    This function takes an array of normalized walking speeds and an array of combined data,\n    and returns an array where each element is the product of the corresponding elements\n    from the input arrays.\n\n    Parameters:\n    normalized_walking_speed_array (numpy.ndarray): An array of normalized walking speeds.\n    combined_data (numpy.ndarray): An array of combined data.\n\n    Returns:\n    numpy.ndarray: An array of adjusted costs.\n    \"\"\"\n\n    adjusted_cost_array = normalized_walking_speed_array * combined_data\n    return adjusted_cost_array\n</pre> def adjust_cost_with_walking_speed(normalized_walking_speed_array, combined_data):     \"\"\"     Adjusts the cost based on the normalized walking speed.      This function takes an array of normalized walking speeds and an array of combined data,     and returns an array where each element is the product of the corresponding elements     from the input arrays.      Parameters:     normalized_walking_speed_array (numpy.ndarray): An array of normalized walking speeds.     combined_data (numpy.ndarray): An array of combined data.      Returns:     numpy.ndarray: An array of adjusted costs.     \"\"\"      adjusted_cost_array = normalized_walking_speed_array * combined_data     return adjusted_cost_array In\u00a0[\u00a0]: Copied! <pre>def invert_cost_array(adjusted_cost_array):\n    \"\"\"\n    Inverts the values in the given cost array.\n    This function takes an array of adjusted costs and inverts each value. \n    If a value is zero, it is replaced with NaN before inversion to avoid \n    division by zero. After inversion, NaN values are replaced with a large \n    number (1e6).\n    Parameters:\n    adjusted_cost_array (numpy.ndarray): A numpy array of adjusted costs.\n    Returns:\n    numpy.ndarray: A numpy array with the inverted cost values.\n    \"\"\"\n  \n    def invert_values(combined_value):\n        combined_value[combined_value == 0] = np.nan\n        inverted_values = np.where(np.isnan(combined_value), np.nan, 1.0 / combined_value)\n        inverted_values[np.isnan(inverted_values)] = 1e6\n        return inverted_values\n\n    inverted_cost_array = np.apply_along_axis(invert_values, -1, adjusted_cost_array)\n    return inverted_cost_array\ndef invert_walking_speed(normalized_walking_speed_array):\n    \"\"\"\n    Inverts walking speed values and handles zeros/NaNs appropriately.\n    \"\"\"\n    inverted = np.zeros_like(normalized_walking_speed_array)\n    for i in range(8):\n        temp = normalized_walking_speed_array[i].copy()\n        temp[temp == 0] = np.nan\n        inverted[i] = np.where(np.isnan(temp), 1e6, 1.0 / temp)\n    return inverted\n</pre> def invert_cost_array(adjusted_cost_array):     \"\"\"     Inverts the values in the given cost array.     This function takes an array of adjusted costs and inverts each value.      If a value is zero, it is replaced with NaN before inversion to avoid      division by zero. After inversion, NaN values are replaced with a large      number (1e6).     Parameters:     adjusted_cost_array (numpy.ndarray): A numpy array of adjusted costs.     Returns:     numpy.ndarray: A numpy array with the inverted cost values.     \"\"\"        def invert_values(combined_value):         combined_value[combined_value == 0] = np.nan         inverted_values = np.where(np.isnan(combined_value), np.nan, 1.0 / combined_value)         inverted_values[np.isnan(inverted_values)] = 1e6         return inverted_values      inverted_cost_array = np.apply_along_axis(invert_values, -1, adjusted_cost_array)     return inverted_cost_array def invert_walking_speed(normalized_walking_speed_array):     \"\"\"     Inverts walking speed values and handles zeros/NaNs appropriately.     \"\"\"     inverted = np.zeros_like(normalized_walking_speed_array)     for i in range(8):         temp = normalized_walking_speed_array[i].copy()         temp[temp == 0] = np.nan         inverted[i] = np.where(np.isnan(temp), 1e6, 1.0 / temp)     return inverted In\u00a0[\u00a0]: Copied! <pre>def invert_cost_raster(updated_cost_raster):\n    \"\"\"\n    Inverts cost raster values and handles zeros/NaNs appropriately.\n    \"\"\"\n    inverted = updated_cost_raster.copy()\n    inverted[inverted == 0] = np.nan\n    return np.where(np.isnan(inverted), 1e6, 1.0 / inverted)\n</pre> def invert_cost_raster(updated_cost_raster):     \"\"\"     Inverts cost raster values and handles zeros/NaNs appropriately.     \"\"\"     inverted = updated_cost_raster.copy()     inverted[inverted == 0] = np.nan     return np.where(np.isnan(inverted), 1e6, 1.0 / inverted)"},{"location":"api/cost-calculation/New%20folder/data-loading.html","title":"Data loading","text":"<p>data_loading.py</p> In\u00a0[\u00a0]: Copied! <pre>import fiona\nimport geopandas as gpd\nimport rasterio\nimport numpy as np\nfrom affine import Affine\n</pre> import fiona import geopandas as gpd import rasterio import numpy as np from affine import Affine In\u00a0[\u00a0]: Copied! <pre>def read_shapefile(path):\n    \"\"\"\n    Load a shapefile and return it as a GeoDataFrame.\n    Parameters:\n    path (str): The file path to the shapefile.\n    Returns:\n    gpd.GeoDataFrame: A GeoDataFrame containing the data from the shapefile.\n    \"\"\"\n    \n    with fiona.open(path) as src:\n        return gpd.GeoDataFrame.from_features(src, crs=src.crs)\n</pre> def read_shapefile(path):     \"\"\"     Load a shapefile and return it as a GeoDataFrame.     Parameters:     path (str): The file path to the shapefile.     Returns:     gpd.GeoDataFrame: A GeoDataFrame containing the data from the shapefile.     \"\"\"          with fiona.open(path) as src:         return gpd.GeoDataFrame.from_features(src, crs=src.crs) In\u00a0[\u00a0]: Copied! <pre>def read_raster(path):\n    \"\"\"\n    Reads a raster file and returns its data, profile, transform, CRS, and nodata value.\n    Parameters:\n    path (str): The file path to the raster file.\n    Returns:\n    tuple: A tuple containing:\n        - data (numpy.ndarray): The raster data.\n        - profile (dict): The profile metadata of the raster.\n        - transform (affine.Affine): The affine transform of the raster.\n        - crs (rasterio.crs.CRS): The coordinate reference system of the raster.\n        - nodata (float or int): The nodata value of the raster.\n    \"\"\"\n    \n    with rasterio.open(path) as dataset:\n        data = dataset.read(1)\n        profile = dataset.profile\n        transform = dataset.transform\n        crs = dataset.crs\n        nodata = dataset.nodata\n    return data, profile, transform, crs, nodata\n</pre> def read_raster(path):     \"\"\"     Reads a raster file and returns its data, profile, transform, CRS, and nodata value.     Parameters:     path (str): The file path to the raster file.     Returns:     tuple: A tuple containing:         - data (numpy.ndarray): The raster data.         - profile (dict): The profile metadata of the raster.         - transform (affine.Affine): The affine transform of the raster.         - crs (rasterio.crs.CRS): The coordinate reference system of the raster.         - nodata (float or int): The nodata value of the raster.     \"\"\"          with rasterio.open(path) as dataset:         data = dataset.read(1)         profile = dataset.profile         transform = dataset.transform         crs = dataset.crs         nodata = dataset.nodata     return data, profile, transform, crs, nodata"},{"location":"api/cost-calculation/New%20folder/dem-processing.html","title":"Dem processing","text":"<p>dem_processing.py</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def calculate_slope(dem_data, resolution_x, resolution_y, no_data):\n    \"\"\"\n    Calculates the slope in eight directions from a DEM.\n\n    Parameters:\n    dem_data (numpy.ndarray): 2D array representing the DEM data.\n    resolution_x (float): The resolution of the DEM in the x direction.\n    resolution_y (float): The resolution of the DEM in the y direction.\n    no_data (float): The value representing no data in the DEM.\n\n    Returns:\n    numpy.ndarray: A 3D array where the first dimension represents the eight directions \n    (North, South, East, West, North-East, North-West, South-East, South-West),\n    and the other two dimensions represent the slope values for each cell in the DEM.\n    \"\"\"\n    rows, cols = dem_data.shape\n    slope_array = np.full((8, rows, cols), np.nan)\n\n    for i in range(1, rows - 1):\n        for j in range(1, cols - 1):\n            if dem_data[i, j] == no_data:\n                continue\n\n            dz_n  = dem_data[i - 1, j] - dem_data[i, j]  # North\n            dz_s  = dem_data[i + 1, j] - dem_data[i, j]  # South\n            dz_e  = dem_data[i, j + 1] - dem_data[i, j]  # East\n            dz_w  = dem_data[i, j - 1] - dem_data[i, j]  # West\n            dz_ne = dem_data[i - 1, j + 1] - dem_data[i, j]  # North-East\n            dz_nw = dem_data[i - 1, j - 1] - dem_data[i, j]  # North-West\n            dz_se = dem_data[i + 1, j + 1] - dem_data[i, j]  # South-East\n            dz_sw = dem_data[i + 1, j - 1] - dem_data[i, j]  # South-West\n\n            d_h = resolution_x\n            d_d = np.sqrt(resolution_x**2 + resolution_y**2)\n\n            slope_array[0, i, j] = (dz_n / d_h)   # North\n            slope_array[1, i, j] = (dz_s / d_h)   # South\n            slope_array[2, i, j] = (dz_e / d_h)   # East\n            slope_array[3, i, j] = (dz_w / d_h)   # West\n            slope_array[4, i, j] = (dz_ne / d_d)  # North-East\n            slope_array[5, i, j] = (dz_nw / d_d)  # North-West\n            slope_array[6, i, j] = (dz_se / d_d)  # South-East\n            slope_array[7, i, j] = (dz_sw / d_d)  # South-West\n\n    return slope_array\n</pre> def calculate_slope(dem_data, resolution_x, resolution_y, no_data):     \"\"\"     Calculates the slope in eight directions from a DEM.      Parameters:     dem_data (numpy.ndarray): 2D array representing the DEM data.     resolution_x (float): The resolution of the DEM in the x direction.     resolution_y (float): The resolution of the DEM in the y direction.     no_data (float): The value representing no data in the DEM.      Returns:     numpy.ndarray: A 3D array where the first dimension represents the eight directions      (North, South, East, West, North-East, North-West, South-East, South-West),     and the other two dimensions represent the slope values for each cell in the DEM.     \"\"\"     rows, cols = dem_data.shape     slope_array = np.full((8, rows, cols), np.nan)      for i in range(1, rows - 1):         for j in range(1, cols - 1):             if dem_data[i, j] == no_data:                 continue              dz_n  = dem_data[i - 1, j] - dem_data[i, j]  # North             dz_s  = dem_data[i + 1, j] - dem_data[i, j]  # South             dz_e  = dem_data[i, j + 1] - dem_data[i, j]  # East             dz_w  = dem_data[i, j - 1] - dem_data[i, j]  # West             dz_ne = dem_data[i - 1, j + 1] - dem_data[i, j]  # North-East             dz_nw = dem_data[i - 1, j - 1] - dem_data[i, j]  # North-West             dz_se = dem_data[i + 1, j + 1] - dem_data[i, j]  # South-East             dz_sw = dem_data[i + 1, j - 1] - dem_data[i, j]  # South-West              d_h = resolution_x             d_d = np.sqrt(resolution_x**2 + resolution_y**2)              slope_array[0, i, j] = (dz_n / d_h)   # North             slope_array[1, i, j] = (dz_s / d_h)   # South             slope_array[2, i, j] = (dz_e / d_h)   # East             slope_array[3, i, j] = (dz_w / d_h)   # West             slope_array[4, i, j] = (dz_ne / d_d)  # North-East             slope_array[5, i, j] = (dz_nw / d_d)  # North-West             slope_array[6, i, j] = (dz_se / d_d)  # South-East             slope_array[7, i, j] = (dz_sw / d_d)  # South-West      return slope_array In\u00a0[\u00a0]: Copied! <pre>def calculate_walking_speed(slope_array):\n    \"\"\"\n    Calculates walking speed using Tobler's Hiking Function.\n    This function calculates the walking speed given an array of slope values.\n    The walking speed is computed using the formula:\n    speed = 6 * exp(-3.5 * abs(slope + 0.05))\n    Parameters:\n    slope_array (numpy.ndarray): An array of slope values.\n    Returns:\n    numpy.ndarray: An array of walking speeds corresponding to the input slope values.\n    \"\"\"\n    return 6 * np.exp(-3.5 * np.abs(slope_array + 0.05))\n</pre> def calculate_walking_speed(slope_array):     \"\"\"     Calculates walking speed using Tobler's Hiking Function.     This function calculates the walking speed given an array of slope values.     The walking speed is computed using the formula:     speed = 6 * exp(-3.5 * abs(slope + 0.05))     Parameters:     slope_array (numpy.ndarray): An array of slope values.     Returns:     numpy.ndarray: An array of walking speeds corresponding to the input slope values.     \"\"\"     return 6 * np.exp(-3.5 * np.abs(slope_array + 0.05)) In\u00a0[\u00a0]: Copied! <pre>def get_max_velocity():\n    \"\"\"\n    Calculates the maximum velocity based on Tobler's Hiking Function.\n    The maximum velocity occurs at a slope of zero.\n    Returns:\n    float: The maximum walking velocity.\n    \"\"\"\n    slope_zero = 0\n    return 6 * np.exp(-3.5 * np.abs(slope_zero + 0.05))\n</pre> def get_max_velocity():     \"\"\"     Calculates the maximum velocity based on Tobler's Hiking Function.     The maximum velocity occurs at a slope of zero.     Returns:     float: The maximum walking velocity.     \"\"\"     slope_zero = 0     return 6 * np.exp(-3.5 * np.abs(slope_zero + 0.05)) In\u00a0[\u00a0]: Copied! <pre>def normalize_walking_speed(walking_speed_array):\n    \"\"\"\n    Normalize the walking speed array by dividing each element by the maximum velocity.\n    The maximum velocity is dynamically calculated using Tobler's Hiking Function at a slope of zero.\n    Parameters:\n    walking_speed_array (numpy.ndarray): Array of walking speeds to be normalized.\n    Returns:\n    numpy.ndarray: The normalized walking speed array.\n    \"\"\"\n    max_velocity = get_max_velocity()\n    return walking_speed_array / max_velocity\n</pre> def normalize_walking_speed(walking_speed_array):     \"\"\"     Normalize the walking speed array by dividing each element by the maximum velocity.     The maximum velocity is dynamically calculated using Tobler's Hiking Function at a slope of zero.     Parameters:     walking_speed_array (numpy.ndarray): Array of walking speeds to be normalized.     Returns:     numpy.ndarray: The normalized walking speed array.     \"\"\"     max_velocity = get_max_velocity()     return walking_speed_array / max_velocity"},{"location":"api/cost-calculation/New%20folder/plotting-utils.html","title":"Plotting utils","text":"<p>plotting_utils.py</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport numpy.ma as ma\nimport rasterio\nfrom rasterio.plot import plotting_extent\nfrom typing import Any\nimport geopandas as gpd\nfrom matplotlib.colors import Normalize, ListedColormap, LinearSegmentedColormap\nfrom matplotlib.colors import TwoSlopeNorm\n</pre> import matplotlib.pyplot as plt import matplotlib.patches as mpatches import numpy as np import numpy.ma as ma import rasterio from rasterio.plot import plotting_extent from typing import Any import geopandas as gpd from matplotlib.colors import Normalize, ListedColormap, LinearSegmentedColormap from matplotlib.colors import TwoSlopeNorm In\u00a0[\u00a0]: Copied! <pre>def plot_continuous_raster_with_points(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, colorbar_label: str, output_file_path: str) -&gt; None:\n    \"\"\"\n    Plots a continuous raster with overlaid points.\n\n    Parameters:\n    raster_data (np.ndarray): The raster data to be plotted.\n    extent (Any): The extent of the raster data.\n    points_gdf (gpd.GeoDataFrame): GeoDataFrame containing the points to be overlaid.\n    title (str): The title of the plot.\n    colorbar_label (str): The label for the colorbar.\n    output_file_path (str): The file path where the plot will be saved.\n\n    Returns:\n    None\n    \"\"\"\n    # A4 width is 210mm = 8.27 inches\n    a4_width = 8.27\n    \n    # Create figure with two subplots in 1 row, 2 columns with A4 width\n    # Add extra width to accommodate the colorbar\n    fig, axes = plt.subplots(1, 2, figsize=(a4_width + 1.5, a4_width/2))\n    \n    # If we're plotting a single raster, simply use the first axis and hide the second\n    if not isinstance(raster_data, list):\n        # Display the raster on the first subplot\n        cax = axes[0].imshow(raster_data, extent=extent, cmap='viridis', interpolation='none')\n        axes[0].set_title(title, fontsize=10, fontweight='bold')\n        axes[0].scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")\n        axes[0].legend()\n        \n        # Hide the second subplot\n        axes[1].axis('off')\n        \n        # Add colorbar\n        cbar = fig.colorbar(cax, ax=axes[0], fraction=0.046, pad=0.04)\n        cbar.set_label(colorbar_label)\n    else:\n        # For comparison plots with two rasters\n        # Find common color scale if needed\n        if len(raster_data) == 2:\n            vmin = min(np.nanmin(raster_data[0]), np.nanmin(raster_data[1]))\n            vmax = max(np.nanmax(raster_data[0]), np.nanmax(raster_data[1]))\n            \n            # If we have a list of titles, use them\n            titles = title if isinstance(title, list) and len(title) == 2 else [f\"{title} (1)\", f\"{title} (2)\"]\n            \n            # Determine the global extent to use for both plots\n            if isinstance(extent, list):\n                # If different extents provided, find the union\n                global_extent = [\n                    min(extent[0][0], extent[1][0]),  # xmin\n                    max(extent[0][1], extent[1][1]),  # xmax\n                    min(extent[0][2], extent[1][2]),  # ymin\n                    max(extent[0][3], extent[1][3])   # ymax\n                ]\n            else:\n                # Use the same extent for both\n                global_extent = extent\n            \n            # Plot first raster\n            cax1 = axes[0].imshow(raster_data[0], extent=global_extent, \n                               cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)\n            axes[0].set_title(titles[0], fontsize=12, fontweight='bold')\n            pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[0]\n            axes[0].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")\n            axes[0].legend()\n            \n            # Plot second raster\n            cax2 = axes[1].imshow(raster_data[1], extent=global_extent, \n                               cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)\n            axes[1].set_title(titles[1], fontsize=12, fontweight='bold')\n            pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[1]\n            axes[1].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")\n            axes[1].legend()\n            \n            # Add colorbar on the right side of the second subplot\n            # Use specific positioning to ensure it's outside the plot\n            cbar = fig.colorbar(cax2, ax=axes[1], pad=0.05)\n            cbar.set_label(colorbar_label, fontsize=10)\n            \n            # Make sure both plots have the same aspect ratio\n            for ax in axes:\n                ax.set_aspect('equal')\n                \n                # Improve x-axis tick formatting\n                ax.ticklabel_format(style='plain', axis='x')\n                ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n                \n                # Improve y-axis tick formatting\n                ax.ticklabel_format(style='plain', axis='y')\n                \n                # Adjust tick label font size\n                ax.tick_params(axis='both', labelsize=9)\n                \n                # Ensure there's enough space for tick labels\n                plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n    \n    # Adjust spacing to make room for x-axis labels\n    plt.tight_layout()\n    \n    # Save and show the plot\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_continuous_raster_with_points(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, colorbar_label: str, output_file_path: str) -&gt; None:     \"\"\"     Plots a continuous raster with overlaid points.      Parameters:     raster_data (np.ndarray): The raster data to be plotted.     extent (Any): The extent of the raster data.     points_gdf (gpd.GeoDataFrame): GeoDataFrame containing the points to be overlaid.     title (str): The title of the plot.     colorbar_label (str): The label for the colorbar.     output_file_path (str): The file path where the plot will be saved.      Returns:     None     \"\"\"     # A4 width is 210mm = 8.27 inches     a4_width = 8.27          # Create figure with two subplots in 1 row, 2 columns with A4 width     # Add extra width to accommodate the colorbar     fig, axes = plt.subplots(1, 2, figsize=(a4_width + 1.5, a4_width/2))          # If we're plotting a single raster, simply use the first axis and hide the second     if not isinstance(raster_data, list):         # Display the raster on the first subplot         cax = axes[0].imshow(raster_data, extent=extent, cmap='viridis', interpolation='none')         axes[0].set_title(title, fontsize=10, fontweight='bold')         axes[0].scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")         axes[0].legend()                  # Hide the second subplot         axes[1].axis('off')                  # Add colorbar         cbar = fig.colorbar(cax, ax=axes[0], fraction=0.046, pad=0.04)         cbar.set_label(colorbar_label)     else:         # For comparison plots with two rasters         # Find common color scale if needed         if len(raster_data) == 2:             vmin = min(np.nanmin(raster_data[0]), np.nanmin(raster_data[1]))             vmax = max(np.nanmax(raster_data[0]), np.nanmax(raster_data[1]))                          # If we have a list of titles, use them             titles = title if isinstance(title, list) and len(title) == 2 else [f\"{title} (1)\", f\"{title} (2)\"]                          # Determine the global extent to use for both plots             if isinstance(extent, list):                 # If different extents provided, find the union                 global_extent = [                     min(extent[0][0], extent[1][0]),  # xmin                     max(extent[0][1], extent[1][1]),  # xmax                     min(extent[0][2], extent[1][2]),  # ymin                     max(extent[0][3], extent[1][3])   # ymax                 ]             else:                 # Use the same extent for both                 global_extent = extent                          # Plot first raster             cax1 = axes[0].imshow(raster_data[0], extent=global_extent,                                 cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)             axes[0].set_title(titles[0], fontsize=12, fontweight='bold')             pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[0]             axes[0].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")             axes[0].legend()                          # Plot second raster             cax2 = axes[1].imshow(raster_data[1], extent=global_extent,                                 cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)             axes[1].set_title(titles[1], fontsize=12, fontweight='bold')             pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[1]             axes[1].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")             axes[1].legend()                          # Add colorbar on the right side of the second subplot             # Use specific positioning to ensure it's outside the plot             cbar = fig.colorbar(cax2, ax=axes[1], pad=0.05)             cbar.set_label(colorbar_label, fontsize=10)                          # Make sure both plots have the same aspect ratio             for ax in axes:                 ax.set_aspect('equal')                                  # Improve x-axis tick formatting                 ax.ticklabel_format(style='plain', axis='x')                 ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))                                  # Improve y-axis tick formatting                 ax.ticklabel_format(style='plain', axis='y')                                  # Adjust tick label font size                 ax.tick_params(axis='both', labelsize=9)                                  # Ensure there's enough space for tick labels                 plt.setp(ax.get_xticklabels(), rotation=45, ha='right')          # Adjust spacing to make room for x-axis labels     plt.tight_layout()          # Save and show the plot     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_normalized_walking_speed(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, output_file_path: str) -&gt; None:\n\n       \n    \"\"\"\n    Plots the normalized walking speed raster with summit points.\n     Parameters:\n        raster_data (np.ndarray): The raster data representing normalized walking speeds.\n        extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).\n        points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.\n        title (str): The title of the plot.\n        output_file_path (str): The file path where the plot will be saved.\n\n        Returns:\n        None\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import ListedColormap\n    import numpy.ma as ma\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    cmap = plt.cm.viridis\n    masked_raster_data = ma.masked_invalid(raster_data)\n    cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')\n    ax.set_facecolor(\"white\")\n    ax.set_title(title, fontsize=10, fontweight='bold')\n    cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    cbar.set_label(\"Speed Conservation Values of Slope\")\n    ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n    ax.legend(loc='upper right')\n    plt.tight_layout()\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_normalized_walking_speed(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, output_file_path: str) -&gt; None:              \"\"\"     Plots the normalized walking speed raster with summit points.      Parameters:         raster_data (np.ndarray): The raster data representing normalized walking speeds.         extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).         points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.         title (str): The title of the plot.         output_file_path (str): The file path where the plot will be saved.          Returns:         None     \"\"\"     import matplotlib.pyplot as plt     from matplotlib.colors import ListedColormap     import numpy.ma as ma      fig, ax = plt.subplots(figsize=(10, 8))     cmap = plt.cm.viridis     masked_raster_data = ma.masked_invalid(raster_data)     cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')     ax.set_facecolor(\"white\")     ax.set_title(title, fontsize=10, fontweight='bold')     cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)     cbar.set_label(\"Speed Conservation Values of Slope\")     ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)     ax.legend(loc='upper right')     plt.tight_layout()     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_adjusted_cost_raster(adjusted_cost_raster, extent, points_gdf, title, output_file_path):\n\n        \n    \"\"\"\n    Plots the adjusted cost raster with summit points.\n    Parameters:\n        adjusted_cost_raster (numpy.ndarray): A 2D array representing the adjusted cost raster data.\n        extent (list or tuple): The bounding box in the form [xmin, xmax, ymin, ymax] for the raster.\n        points_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing the summit points with geometry column.\n        title (str): The title of the plot.\n        output_file_path (str): The file path where the plot image will be saved.\n\n        Returns:\n        None\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import Normalize\n    import numpy.ma as ma\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    cmap = plt.cm.Blues\n    masked_raster_data = ma.masked_invalid(adjusted_cost_raster)\n    cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')\n    ax.set_facecolor(\"white\")\n    ax.set_title(title, fontsize=10, fontweight='bold')\n    cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    cbar.set_label(\"Walking Speed (m/s)\")\n    ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n    ax.legend(loc='upper right')\n    plt.tight_layout()\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_adjusted_cost_raster(adjusted_cost_raster, extent, points_gdf, title, output_file_path):               \"\"\"     Plots the adjusted cost raster with summit points.     Parameters:         adjusted_cost_raster (numpy.ndarray): A 2D array representing the adjusted cost raster data.         extent (list or tuple): The bounding box in the form [xmin, xmax, ymin, ymax] for the raster.         points_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing the summit points with geometry column.         title (str): The title of the plot.         output_file_path (str): The file path where the plot image will be saved.          Returns:         None     \"\"\"     import matplotlib.pyplot as plt     from matplotlib.colors import Normalize     import numpy.ma as ma      fig, ax = plt.subplots(figsize=(10, 8))     cmap = plt.cm.Blues     masked_raster_data = ma.masked_invalid(adjusted_cost_raster)     cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')     ax.set_facecolor(\"white\")     ax.set_title(title, fontsize=10, fontweight='bold')     cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)     cbar.set_label(\"Walking Speed (m/s)\")     ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)     ax.legend(loc='upper right')     plt.tight_layout()     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_inverted_cost_raster(inverted_cost_raster, extent, points_gdf, title, output_file_path):\n\n     \n    \"\"\"\n    Plots the inverted cost raster with summit points.\n       Parameters:\n        inverted_cost_raster (numpy.ndarray): 2D array representing the inverted cost raster.\n        extent (list or tuple): The bounding box in data coordinates (left, right, bottom, top).\n        points_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the summit points with geometry column.\n        title (str): Title of the plot.\n        output_file_path (str): Path to save the output plot image.\n\n        Returns:\n        None\n    \"\"\"\n\n\n    # Create masks\n    mask_nan = np.isnan(inverted_cost_raster)\n    mask_special = (inverted_cost_raster == 1e6)\n    mask_continuous = (~mask_nan &amp; ~mask_special)\n\n    # Create masked arrays\n    continuous_data = ma.masked_where(~mask_continuous, inverted_cost_raster)\n    special_data = ma.masked_where(~mask_special, inverted_cost_raster)\n\n    # Determine normalization range\n    clean_data = continuous_data[~mask_nan &amp; ~mask_special]\n    vmin, vmax = np.percentile(clean_data, [2, 98])\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    cax = ax.imshow(continuous_data, extent=extent, cmap='Blues', norm=Normalize(vmin=vmin, vmax=vmax))\n    ax.imshow(special_data, extent=extent, cmap=ListedColormap(['blue', 'none']), alpha=0.7)\n    ax.set_facecolor(\"white\")\n    ax.set_title(title, fontsize=16, fontweight='bold')\n    cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    cbar.set_label('Cost (excluding 1e6)', fontsize=12)\n    ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n    ax.legend(loc='upper right')\n    plt.tight_layout()\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_inverted_cost_raster(inverted_cost_raster, extent, points_gdf, title, output_file_path):            \"\"\"     Plots the inverted cost raster with summit points.        Parameters:         inverted_cost_raster (numpy.ndarray): 2D array representing the inverted cost raster.         extent (list or tuple): The bounding box in data coordinates (left, right, bottom, top).         points_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the summit points with geometry column.         title (str): Title of the plot.         output_file_path (str): Path to save the output plot image.          Returns:         None     \"\"\"       # Create masks     mask_nan = np.isnan(inverted_cost_raster)     mask_special = (inverted_cost_raster == 1e6)     mask_continuous = (~mask_nan &amp; ~mask_special)      # Create masked arrays     continuous_data = ma.masked_where(~mask_continuous, inverted_cost_raster)     special_data = ma.masked_where(~mask_special, inverted_cost_raster)      # Determine normalization range     clean_data = continuous_data[~mask_nan &amp; ~mask_special]     vmin, vmax = np.percentile(clean_data, [2, 98])      fig, ax = plt.subplots(figsize=(10, 8))     cax = ax.imshow(continuous_data, extent=extent, cmap='Blues', norm=Normalize(vmin=vmin, vmax=vmax))     ax.imshow(special_data, extent=extent, cmap=ListedColormap(['blue', 'none']), alpha=0.7)     ax.set_facecolor(\"white\")     ax.set_title(title, fontsize=16, fontweight='bold')     cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)     cbar.set_label('Cost (excluding 1e6)', fontsize=12)     ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)     ax.legend(loc='upper right')     plt.tight_layout()     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_walking_speed_vs_slope(slope_array, walking_speed_array, directions, output_file_path):\n    \"\"\"\n    Plots the walking speed versus slope for all eight directions in a subplot layout.\n\n    Parameters:\n    slope_array (numpy.ndarray): A 3D array representing slope values for eight directions.\n                                  The first dimension represents directions: North, South, East, West,\n                                  North-East, North-West, South-East, South-West.\n    walking_speed_array (numpy.ndarray): A 3D array representing walking speed values for eight directions.\n                                         The first dimension represents directions: North, South, East, West,\n                                         North-East, North-West, South-East, South-West.\n    directions (list): A list of direction labels.\n    output_file_path (str): The file path where the combined plot will be saved.\n\n    Returns:\n    None\n    \"\"\"\n    fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n    fig.suptitle(\"Walking Speed vs Slope for All 8 Directions\", fontsize=16)\n\n    for direction in range(8):\n        # Flatten the slope and walking speed arrays for the current direction\n        slope_values = slope_array[direction].flatten()\n        walking_speed_values = walking_speed_array[direction].flatten()\n\n        # Filter out NaN values from both slope and walking speed arrays\n        valid_mask = ~np.isnan(slope_values) &amp; ~np.isnan(walking_speed_values)\n        slope_values = slope_values[valid_mask]\n        walking_speed_values = walking_speed_values[valid_mask]\n\n        # Get the corresponding axis for the current direction\n        ax = axes[direction // 2, direction % 2]\n        \n        # Create a scatter plot of walking speed vs. slope\n        ax.scatter(slope_values, walking_speed_values, alpha=0.5, s=1)\n        ax.set_title(f\"{directions[direction]} Direction\")\n        ax.set_xlabel(\"Slope\")\n        ax.set_ylabel(\"Walking Speed (m/s)\")\n\n    # Adjust layout to prevent overlapping\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(output_file_path, dpi=300)\n    plt.show()\n    plt.close(fig)\n</pre> def plot_walking_speed_vs_slope(slope_array, walking_speed_array, directions, output_file_path):     \"\"\"     Plots the walking speed versus slope for all eight directions in a subplot layout.      Parameters:     slope_array (numpy.ndarray): A 3D array representing slope values for eight directions.                                   The first dimension represents directions: North, South, East, West,                                   North-East, North-West, South-East, South-West.     walking_speed_array (numpy.ndarray): A 3D array representing walking speed values for eight directions.                                          The first dimension represents directions: North, South, East, West,                                          North-East, North-West, South-East, South-West.     directions (list): A list of direction labels.     output_file_path (str): The file path where the combined plot will be saved.      Returns:     None     \"\"\"     fig, axes = plt.subplots(4, 2, figsize=(15, 20))     fig.suptitle(\"Walking Speed vs Slope for All 8 Directions\", fontsize=16)      for direction in range(8):         # Flatten the slope and walking speed arrays for the current direction         slope_values = slope_array[direction].flatten()         walking_speed_values = walking_speed_array[direction].flatten()          # Filter out NaN values from both slope and walking speed arrays         valid_mask = ~np.isnan(slope_values) &amp; ~np.isnan(walking_speed_values)         slope_values = slope_values[valid_mask]         walking_speed_values = walking_speed_values[valid_mask]          # Get the corresponding axis for the current direction         ax = axes[direction // 2, direction % 2]                  # Create a scatter plot of walking speed vs. slope         ax.scatter(slope_values, walking_speed_values, alpha=0.5, s=1)         ax.set_title(f\"{directions[direction]} Direction\")         ax.set_xlabel(\"Slope\")         ax.set_ylabel(\"Walking Speed (m/s)\")      # Adjust layout to prevent overlapping     plt.tight_layout(rect=[0, 0.03, 1, 0.95])     plt.savefig(output_file_path, dpi=300)     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_north_east_speed_conservation(normalized_walking_speed_array, extent, points_gdf, title, output_file_path):\n    \"\"\"\n    Plots the normalized walking speed raster for North and East directions in a subplot.\n    \n    Parameters:\n        normalized_walking_speed_array (np.ndarray): The 3D array of normalized walking speeds where\n                                                    first dimension represents directions \n                                                    (North: 0, East: 2)\n        extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).\n        points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.\n        title (str): The main title of the plot.\n        output_file_path (str): The file path where the plot will be saved.\n        \n    Returns:\n        None\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy.ma as ma\n    \n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n    fig.suptitle(title, fontsize=14, fontweight='bold')\n    \n    # Direction labels\n    directions = ['North', 'East']\n    indices = [0, 2]  # North is at index 0, East is at index 2 in the array\n    \n    # Create a common color scale for both plots\n    vmin = np.nanmin([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])\n    vmax = np.nanmax([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])\n    cmap = plt.cm.viridis\n    \n    # Loop through the two directions\n    for i, (ax, direction, idx) in enumerate(zip(axes, directions, indices)):\n        # Get the data for this direction\n        raster_data = normalized_walking_speed_array[idx]\n        \n        # Mask invalid data\n        masked_raster_data = ma.masked_invalid(raster_data)\n        \n        # Plot the data\n        cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, \n                        interpolation='none', vmin=vmin, vmax=vmax)\n        \n        # Set background color for NaN values\n        ax.set_facecolor(\"white\")\n        \n        # Add title for this subplot\n        ax.set_title(f\"{direction} Direction\", fontsize=12, fontweight='bold')\n        \n        # Add summit points\n        ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, \n                  color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n        \n        # Add legend\n        ax.legend(loc='upper right')\n    \n    # Add a colorbar that applies to both subplots\n    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n    cbar = plt.colorbar(cax, cax=cbar_ax)\n    cbar.set_label(\"Speed Conservation Values of Slope\")\n    \n    # Adjust layout\n    plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Adjust layout to give space for colorbar\n    \n    # Save the figure\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_north_east_speed_conservation(normalized_walking_speed_array, extent, points_gdf, title, output_file_path):     \"\"\"     Plots the normalized walking speed raster for North and East directions in a subplot.          Parameters:         normalized_walking_speed_array (np.ndarray): The 3D array of normalized walking speeds where                                                     first dimension represents directions                                                      (North: 0, East: 2)         extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).         points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.         title (str): The main title of the plot.         output_file_path (str): The file path where the plot will be saved.              Returns:         None     \"\"\"     import matplotlib.pyplot as plt     import numpy.ma as ma          # Create a figure with two subplots     fig, axes = plt.subplots(1, 2, figsize=(16, 8))     fig.suptitle(title, fontsize=14, fontweight='bold')          # Direction labels     directions = ['North', 'East']     indices = [0, 2]  # North is at index 0, East is at index 2 in the array          # Create a common color scale for both plots     vmin = np.nanmin([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])     vmax = np.nanmax([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])     cmap = plt.cm.viridis          # Loop through the two directions     for i, (ax, direction, idx) in enumerate(zip(axes, directions, indices)):         # Get the data for this direction         raster_data = normalized_walking_speed_array[idx]                  # Mask invalid data         masked_raster_data = ma.masked_invalid(raster_data)                  # Plot the data         cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap,                          interpolation='none', vmin=vmin, vmax=vmax)                  # Set background color for NaN values         ax.set_facecolor(\"white\")                  # Add title for this subplot         ax.set_title(f\"{direction} Direction\", fontsize=12, fontweight='bold')                  # Add summit points         ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y,                    color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)                  # Add legend         ax.legend(loc='upper right')          # Add a colorbar that applies to both subplots     cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]     cbar = plt.colorbar(cax, cax=cbar_ax)     cbar.set_label(\"Speed Conservation Values of Slope\")          # Adjust layout     plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Adjust layout to give space for colorbar          # Save the figure     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig)"},{"location":"api/evacuation-analysis/analysis.html","title":"Analysis","text":"<p>The <code>analysis.py</code> module provides functions for conducting evacuation analysis using Dijkstra's algorithm and processing travel times.</p>"},{"location":"api/evacuation-analysis/analysis.html#functions","title":"Functions","text":""},{"location":"api/evacuation-analysis/analysis.html#run_dijkstra_analysis","title":"<code>run_dijkstra_analysis</code>","text":"<pre><code>def run_dijkstra_analysis(graph_csr, source_nodes)\n</code></pre> <p>Run Dijkstra's algorithm for all source nodes.</p> <p>This function computes the shortest paths from a set of source nodes to all other nodes in a given graph using Dijkstra's algorithm. The graph is represented in Compressed Sparse Row (CSR) format.</p> <p>Parameters:</p> <ul> <li><code>graph_csr (scipy.sparse.csr_matrix)</code>: The graph in CSR format where each entry represents the weight of the edge between nodes.</li> <li><code>source_nodes (array-like)</code>: A list or array of source node indices from which to calculate the shortest paths.</li> </ul> <p>Returns:</p> <ul> <li><code>distances (numpy.ndarray)</code>: A 2D array where distances[i, j] represents the shortest distance from source_nodes[i] to node j.</li> <li><code>predecessors (numpy.ndarray)</code>: A 2D array where predecessors[i, j] represents the predecessor node on the shortest path from source_nodes[i] to node j. A value of -9999 indicates no path exists.</li> </ul>"},{"location":"api/evacuation-analysis/analysis.html#process_travel_times","title":"<code>process_travel_times</code>","text":"<pre><code>def process_travel_times(base_path, source_name, dataset_key, speed_name, speed_value, meta)\n</code></pre> <p>Process and save travel time rasters for a given speed value.</p> <p>This function loads a base raster, processes it to calculate travel times based on the specified speed, and saves the resulting raster to disk.</p> <p>Parameters:</p> <ul> <li><code>base_path (str)</code>: The file path to the base raster.</li> <li><code>source_name (str)</code>: The name of the source for which travel times are being processed.</li> <li><code>dataset_key (str)</code>: A key identifying the dataset, used in the output filename.</li> <li><code>speed_name (str)</code>: The name of the speed category (e.g., 'slow', 'moderate', 'fast').</li> <li><code>speed_value (float)</code>: The speed value to be used in processing the raster.</li> <li><code>meta (dict)</code>: Metadata for the output raster file.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: The processed travel time array.</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the base raster file does not exist.</li> <li><code>ValueError</code>: If there is an issue with the raster data or processing.</li> </ul>"},{"location":"api/evacuation-analysis/analysis.html#analyze_safe_zones","title":"<code>analyze_safe_zones</code>","text":"<pre><code>def analyze_safe_zones(distance_from_summit, travel_time_data, safe_zone_distances, source_names)\n</code></pre> <p>Analyze minimum travel times within specified safe zones from different source locations.</p> <p>Parameters:</p> <ul> <li><code>distance_from_summit (numpy.ndarray)</code>: A 2D array representing the distance from the summit.</li> <li><code>travel_time_data (dict)</code>: A dictionary containing travel time data for different speeds and sources.   The structure is expected to be:   <pre><code>{\n    'speed_name': {\n        'source_name': {\n            'cost_array_flat': numpy.ndarray,  # Flattened travel times\n            'cost_array': numpy.ndarray        # 2D travel times\n        }\n    }\n}\n</code></pre></li> <li><code>safe_zone_distances (list)</code>: A list of distances defining the safe zones in meters.</li> <li><code>source_names (list)</code>: A list of source names corresponding to the travel time data.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing two dictionaries:</li> <li><code>results</code>: A dictionary with the minimum travel times (in hours) for each speed and safe zone.     The structure is:     <pre><code>{\n    'speed_name': {\n        safe_zone_distance: [min_time1, min_time2, ...]\n    }\n}\n</code></pre>     where the list indices correspond to the order of source_names.</li> <li><code>min_coords</code>: A dictionary with the coordinates of the minimum travel times for each speed and safe zone.     The structure is:     <pre><code>{\n    'speed_name': {\n        safe_zone_distance: [(min_r1, min_c1), (min_r2, min_c2), ...]\n    }\n}\n</code></pre>     where each tuple contains the row and column indices in the original 2D array.</li> </ul>"},{"location":"api/evacuation-analysis/config.html","title":"Configuration","text":"<p>The <code>config.py</code> module establishes environment variables, defines walking speeds, and specifies file paths for various geospatial datasets used in the volcanic evacuation analysis.</p>"},{"location":"api/evacuation-analysis/config.html#constants","title":"Constants","text":""},{"location":"api/evacuation-analysis/config.html#walking-speeds","title":"Walking Speeds","text":"<pre><code>WALKING_SPEEDS = {\n   'slow': 0.91,\n   'medium': 1.22,\n   'fast': 1.52\n}\n</code></pre> <p>Dictionary mapping speed categories to values in meters per second.</p>"},{"location":"api/evacuation-analysis/config.html#file-paths","title":"File Paths","text":"<pre><code>DATA_FOLDER = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results'\n</code></pre> <p>Base directory for python results.</p> <pre><code>COST_PATHS = {\n    'final': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_original.tif'),\n    'modify_landcover': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_modified.tif'),\n    'walking_speed': os.path.join(DATA_FOLDER, 'inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif'),\n    'base_cost': os.path.join(DATA_FOLDER, 'invert_landcovercost_original_Awu.tif')\n}\n</code></pre> <p>Dictionary mapping cost raster names to their file paths: - 'final': Main cost raster with original hiking path - 'modify_landcover': Cost raster with modified land cover - 'walking_speed': Walking speed cost raster - 'base_cost': Base cost raster</p> <pre><code>SUMMIT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\"\n</code></pre> <p>File path to the Mt. Marapi summit location (GPKG format).</p> <pre><code>CAMP_SPOT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\"\n</code></pre> <p>File path to camping locations (GPKG format).</p> <pre><code>HIKING_PATH = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp'\n</code></pre> <p>File path to the original hiking path (GPKG format).</p> <pre><code>landcover_100m = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_LandCover_2019_100m_Buffer_UTM.tif'\n</code></pre> <p>File path to land cover data with 100m buffer (TIF format).</p> <pre><code>DEM_100m = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_DEM_100m_Buffer_UTM.tif\"\n</code></pre> <p>File path to digital elevation model with 100m buffer (TIF format).</p>"},{"location":"api/evacuation-analysis/config.html#source-names-and-safe-zone-distances","title":"Source Names and Safe Zone Distances","text":"<pre><code>SOURCE_NAMES = ['summit', 'camp1']\n</code></pre> <p>List of source location names for evacuation analysis.</p> <pre><code>SAFE_ZONE_DISTANCES = list(range(500, 5000, 500))\n</code></pre> <p>List of safe zone distances from 500m to 4500m in 500m increments.</p>"},{"location":"api/evacuation-analysis/config.html#movement-directions","title":"Movement Directions","text":"<pre><code>DIRECTIONS = [\n   (-1, 0),   # Up\n   (1, 0),    # Down\n   (0, 1),    # Right\n   (0, -1),   # Left\n   (-1, 1),   # Up-Right\n   (-1, -1),  # Up-Left\n   (1, 1),    # Down-Right\n   (1, -1)    # Down-Left\n]\n</code></pre> <p>List of tuples representing the eight possible movement directions for path analysis (cardinal and intercardinal directions).</p>"},{"location":"api/evacuation-analysis/config.html#environment-variables","title":"Environment Variables","text":"<pre><code>os.environ['USE_PATH_FOR_GDAL_PYTHON'] = \"True\"\n</code></pre> <p>Set to \"True\" to use the system path for GDAL Python bindings.</p> <pre><code>os.environ['PROJ_LIB'] = pyproj.datadir.get_data_dir()\n</code></pre> <p>Set to pyproj's data directory for projection information.</p>"},{"location":"api/evacuation-analysis/decomposition.html","title":"Decomposition","text":"<p>The <code>decomposition.py</code> module provides functions for analyzing the relative contributions of different factors (slope and land cover) to evacuation paths.</p>"},{"location":"api/evacuation-analysis/decomposition.html#functions","title":"Functions","text":""},{"location":"api/evacuation-analysis/decomposition.html#reconstruct_path","title":"<code>reconstruct_path</code>","text":"<pre><code>def reconstruct_path(predecessors, target, source)\n</code></pre> <p>Reconstruct the shortest path from source to target using the predecessor array.</p> <p>This function traces back through a predecessor array produced by a shortest path algorithm (like Dijkstra's) to reconstruct the complete path from source to target.</p> <p>Parameters:</p> <ul> <li><code>predecessors (numpy.ndarray)</code>: A 1D array where predecessors[i] contains the predecessor node of node i on the shortest path from the source. This array should be for a single source node.</li> <li><code>target (int)</code>: The target node index (destination) for which to reconstruct the path.</li> <li><code>source (int)</code>: The source node index (starting point) from which the path begins.</li> </ul> <p>Returns:</p> <ul> <li><code>path (list)</code>: A list of node indices representing the shortest path from source to target, inclusive of both endpoints. The path is ordered from source to target.</li> </ul>"},{"location":"api/evacuation-analysis/decomposition.html#expand_single_band","title":"<code>expand_single_band</code>","text":"<pre><code>def expand_single_band(cost_array)\n</code></pre> <p>Expand a single-band raster into an 8-band array representing different movement directions.</p> <p>This function takes a single-band cost raster and expands it into 8 bands corresponding to the 8 movement directions (4 cardinal and 4 diagonal). The diagonal directions (bands 4-7) are multiplied by sqrt(2) to account for the increased distance when moving diagonally.</p> <p>Parameters:</p> <ul> <li><code>cost_array (numpy.ndarray)</code>: Input cost raster with shape (1, rows, cols), where the first dimension represents a single band.</li> </ul> <p>Returns:</p> <ul> <li><code>expanded (numpy.ndarray)</code>: An 8-band raster with shape (8, rows, cols), where each band represents the cost in one of the 8 movement directions. Bands 0-3 represent cardinal directions, while bands 4-7 represent diagonal directions with costs multiplied by sqrt(2).</li> </ul>"},{"location":"api/evacuation-analysis/decomposition.html#run_decomposition_analysis","title":"<code>run_decomposition_analysis</code>","text":"<pre><code>def run_decomposition_analysis(dataset_info, min_coords_all)\n</code></pre> <p>Perform a decomposition analysis to quantify the relative contributions of slope and land cover to the optimal evacuation paths from the summit.</p> <p>This function analyzes the shortest paths identified for different safe zone thresholds and calculates the percentage contribution of slope (walking speed) and land cover factors to the overall path cost. The analysis uses three cost rasters: the final combined cost, the walking speed (slope) cost, and the base (land cover) cost.</p> <p>Parameters:</p> <ul> <li><code>dataset_info (dict)</code>: Dictionary containing information about the datasets, including:</li> <li> <p>'final': dict containing:</p> <ul> <li>'pred_summit': ndarray - Predecessor array for the summit source</li> <li>'cols': int - Number of columns in the raster</li> <li>'summit_raster_coords': tuple or list - Coordinates of the summit in the raster</li> </ul> </li> <li> <p><code>min_coords_all (dict)</code>: Nested dictionary containing the minimum travel time coordinates for each dataset, speed, and safe zone distance:   <pre><code>{\n    dataset_key: {\n        speed_name: {\n            safe_zone_distance: [(row, col), ...]\n        }\n    }\n}\n</code></pre></p> </li> </ul> <p>Returns:</p> <ul> <li><code>list of dict</code>: A list of dictionaries, each representing a row in the results table with keys:</li> <li>\"Safe Zone Threshold (m)\": int - The safe zone distance threshold</li> <li>\"Slope Contribution (%)\": float - Percentage contribution of slope to the path cost</li> <li>\"Landcover Contribution (%)\": float - Percentage contribution of land cover to the path cost</li> </ul>"},{"location":"api/evacuation-analysis/grid-utils.html","title":"Grid Utilities","text":"<p>The <code>grid_utils.py</code> module provides functions for working with raster grids, including coordinate transformations and distance calculations.</p>"},{"location":"api/evacuation-analysis/grid-utils.html#functions","title":"Functions","text":""},{"location":"api/evacuation-analysis/grid-utils.html#coords_to_raster","title":"<code>coords_to_raster</code>","text":"<pre><code>def coords_to_raster(gdf, transform, bounds, res)\n</code></pre> <p>Convert geographic coordinates to raster row and column indices.</p> <p>This function takes a GeoDataFrame containing point geometries and converts the geographic coordinates (x, y) of each point to raster indices (row, col), based on the provided raster transform, bounds, and resolution.</p> <p>Parameters:</p> <ul> <li><code>gdf (geopandas.GeoDataFrame)</code>: A GeoDataFrame containing point geometries to be converted to raster indices.</li> <li><code>transform (affine.Affine)</code>: The affine transform of the raster, defining the relationship between pixel coordinates and geographic coordinates.</li> <li><code>bounds (rasterio.coords.BoundingBox)</code>: The geographic bounds of the raster (left, bottom, right, top).</li> <li><code>res (tuple)</code>: A tuple (x_res, y_res) containing the pixel resolution in the x and y directions (typically in map units per pixel).</li> </ul> <p>Returns:</p> <ul> <li><code>raster_coords (list)</code>: A list of tuples (row, col) representing the raster indices corresponding to each point in the input GeoDataFrame. Points that fall outside the raster bounds are excluded from the returned list.</li> </ul>"},{"location":"api/evacuation-analysis/grid-utils.html#to_1d","title":"<code>to_1d</code>","text":"<pre><code>def to_1d(r, c, cols)\n</code></pre> <p>Convert 2D raster coordinates (row, column) to a 1D array index.</p> <p>This function maps 2D coordinates in a raster or grid to the corresponding 1D array index, assuming row-major order (C-style).</p> <p>Parameters:</p> <ul> <li><code>r (int)</code>: The row index (zero-based).</li> <li><code>c (int)</code>: The column index (zero-based).</li> <li><code>cols (int)</code>: The total number of columns in the 2D array or grid.</li> </ul> <p>Returns:</p> <ul> <li><code>int</code>: The 1D array index corresponding to the given 2D coordinates.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; to_1d(2, 3, 5)\n13\n# In a 5-column grid, the position at row 2, column 3 maps to index 13\n</code></pre>"},{"location":"api/evacuation-analysis/grid-utils.html#raster_coord_to_map_coords","title":"<code>raster_coord_to_map_coords</code>","text":"<pre><code>def raster_coord_to_map_coords(row, col, transform)\n</code></pre> <p>Convert raster coordinates (row, column) to geographic map coordinates (x, y).</p> <p>This function transforms pixel coordinates in a raster to their corresponding geographic coordinates using the raster's affine transform.</p> <p>Parameters:</p> <ul> <li><code>row (int)</code>: The row index in the raster (zero-based).</li> <li><code>col (int)</code>: The column index in the raster (zero-based).</li> <li><code>transform (affine.Affine)</code>: The affine transform of the raster, defining the relationship between pixel coordinates and geographic coordinates.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple (x, y) containing the geographic coordinates corresponding to the center of the specified raster cell.</li> </ul>"},{"location":"api/evacuation-analysis/grid-utils.html#process_raster","title":"<code>process_raster</code>","text":"<pre><code>def process_raster(cost_array, walking_speed)\n</code></pre> <p>Convert base cost raster to travel time in hours.</p> <p>This function transforms a cost raster into a travel time raster by applying the following steps: 1. Multiply by cell size (100m) to convert to distance units 2. Divide by walking speed to convert to time in seconds 3. Convert seconds to hours 4. Replace infinite values with -1 (indicating inaccessible areas)</p> <p>Parameters:</p> <ul> <li><code>cost_array (numpy.ndarray)</code>: Input cost raster array. Values typically represent the inverse of the travel cost or difficulty of traversing each cell.</li> <li><code>walking_speed (float)</code>: Walking speed in meters per second (m/s). Used to convert distance to time.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: Array of travel times in hours. The array has the same shape as the input cost_array, with infinite values replaced by -1.</li> </ul>"},{"location":"api/evacuation-analysis/grid-utils.html#calculate_distance_from_summit","title":"<code>calculate_distance_from_summit</code>","text":"<pre><code>def calculate_distance_from_summit(summit_coords, rows, cols, cell_size=100)\n</code></pre> <p>Compute Euclidean distance (in meters) from the summit for each cell in a raster.</p> <p>This function creates a distance raster where each cell contains the straight-line distance from that cell to the summit location. The distance is calculated using the Euclidean formula and converted to meters using the specified cell size.</p> <p>Parameters:</p> <ul> <li><code>summit_coords (tuple)</code>: A tuple (row, col) containing the raster coordinates of the summit.</li> <li><code>rows (int)</code>: The number of rows in the output raster.</li> <li><code>cols (int)</code>: The number of columns in the output raster.</li> <li><code>cell_size (float, optional)</code>: The cell size in meters. Default is 100 meters.</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: A 2D array of shape (rows, cols) containing the Euclidean distance from each cell to the summit in meters.</li> </ul>"},{"location":"api/evacuation-analysis/io-utils.html","title":"IO Utilities","text":"<p>The <code>io_utils.py</code> module provides functions for reading and writing geospatial data files, including shapefiles, rasters, and output formats for analyses.</p>"},{"location":"api/evacuation-analysis/io-utils.html#functions","title":"Functions","text":""},{"location":"api/evacuation-analysis/io-utils.html#read_shapefile","title":"<code>read_shapefile</code>","text":"<pre><code>def read_shapefile(path, crs_epsg=32651)\n</code></pre> <p>Read a shapefile into a GeoDataFrame with proper coordinate reference system handling.</p> <p>This function reads a shapefile from the specified path using fiona and GeoPandas, and ensures that a coordinate reference system (CRS) is assigned. If the shapefile does not have a defined CRS, it will assign the specified EPSG code.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: The file path to the shapefile to be read.</li> <li><code>crs_epsg (int, optional)</code>: The EPSG code to assign if the shapefile lacks a CRS. Default is 32651 (WGS 84 / UTM zone 51N), which is appropriate for Indonesia/Sumatra region.</li> </ul> <p>Returns:</p> <ul> <li><code>geopandas.GeoDataFrame</code>: A GeoDataFrame containing the features from the shapefile with the proper CRS.</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the shapefile does not exist at the specified path.</li> <li><code>fiona.errors.DriverError</code>: If the file exists but cannot be read as a shapefile.</li> </ul>"},{"location":"api/evacuation-analysis/io-utils.html#read_raster","title":"<code>read_raster</code>","text":"<pre><code>def read_raster(path)\n</code></pre> <p>Read a raster file and return its data and associated metadata.</p> <p>This function opens a raster file using rasterio and extracts the raster data, metadata, spatial reference information, and other key properties needed for geospatial analysis.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: The file path to the raster file to be read.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>data (numpy.ndarray)</code>: The raster data with shape (bands, rows, cols)</li> <li><code>meta (dict)</code>: A copy of the raster metadata</li> <li><code>transform (affine.Affine)</code>: The affine transform defining the relationship between pixel coordinates and geographic coordinates</li> <li><code>nodata (float or int)</code>: The value used to represent no data or null values</li> <li><code>bounds (rasterio.coords.BoundingBox)</code>: The geographic bounds of the raster (left, bottom, right, top)</li> <li><code>resolution (tuple)</code>: A tuple (x_res, y_res) containing the pixel size in the x and y directions</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the raster file does not exist at the specified path.</li> <li><code>rasterio.errors.RasterioIOError</code>: If the file exists but cannot be read as a raster.</li> </ul>"},{"location":"api/evacuation-analysis/io-utils.html#save_raster","title":"<code>save_raster</code>","text":"<pre><code>def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1)\n</code></pre> <p>Save a numpy array as a raster file with specified metadata.</p> <p>This function writes a numpy array to disk as a geospatial raster file with appropriate metadata, including coordinate reference system, transformation, and no-data value.</p> <p>Parameters:</p> <ul> <li><code>output_path (str)</code>: The file path where the raster will be saved.</li> <li><code>data (numpy.ndarray)</code>: The raster data to be saved. If 3D, the first dimension should be bands. If 2D, it will be treated as a single-band raster.</li> <li><code>meta (dict)</code>: Metadata dictionary containing information such as width, height, transform, and CRS. This is typically obtained from another raster.</li> <li><code>dtype (rasterio data type, optional)</code>: The data type to use for the output raster. Default is rasterio.float32.</li> <li><code>nodata (int or float, optional)</code>: The value to use for no-data or missing values in the output raster. Default is -1.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code>: The function does not return a value, but writes the raster to disk.</li> </ul>"},{"location":"api/evacuation-analysis/io-utils.html#load_raster","title":"<code>load_raster</code>","text":"<pre><code>def load_raster(file_path)\n</code></pre> <p>Load a single-band raster file and return its data and metadata.</p> <p>This function opens a raster file using rasterio and extracts the first band of data along with its metadata. Unlike the read_raster function which reads all bands, this function is optimized for single-band rasters.</p> <p>Parameters:</p> <ul> <li><code>file_path (str)</code>: The file path to the raster file to be loaded.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>data (numpy.ndarray)</code>: The raster data from the first band with shape (rows, cols)</li> <li><code>meta (dict)</code>: A copy of the raster metadata</li> </ul> <p>Raises:</p> <ul> <li><code>FileNotFoundError</code>: If the raster file does not exist at the specified path.</li> <li><code>rasterio.errors.RasterioIOError</code>: If the file exists but cannot be read as a raster.</li> </ul>"},{"location":"api/evacuation-analysis/io-utils.html#save_analysis_report","title":"<code>save_analysis_report</code>","text":"<pre><code>def save_analysis_report(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances)\n</code></pre> <p>Save evacuation analysis results to a formatted text report file.</p> <p>This function generates a human-readable text report summarizing the minimum travel times from various sources to safe zones for different walking speeds.</p> <p>Parameters:</p> <ul> <li><code>output_path (str)</code>: The file path where the report will be saved.</li> <li><code>results (dict)</code>: A nested dictionary containing travel time results: <code>{speed_name: {safe_zone_distance: [times_per_source]}}</code> where times_per_source is a list of travel times (in hours) for each source.</li> <li><code>min_coords (dict)</code>: A nested dictionary containing the coordinates of minimum travel times: <code>{speed_name: {safe_zone_distance: [coords_per_source]}}</code> where coords_per_source is a list of (row, col) tuples for each source.</li> <li><code>source_names (list)</code>: A list of source location names corresponding to indices in the results arrays.</li> <li><code>walking_speeds (dict)</code>: Dictionary mapping speed names (str) to speed values (float) in meters per second.</li> <li><code>safe_zone_distances (list)</code>: List of safe zone distances used in the analysis.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code>: The function does not return a value, but writes results to a text file.</li> </ul>"},{"location":"api/evacuation-analysis/io-utils.html#save_metrics_csv","title":"<code>save_metrics_csv</code>","text":"<pre><code>def save_metrics_csv(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances)\n</code></pre> <p>Save evacuation analysis metrics to a CSV file for further processing.</p> <p>This function exports the analysis results in a tabular format suitable for import into spreadsheet software or data analysis tools. Each row represents a unique combination of walking speed, safe zone distance, and source location.</p> <p>Parameters:</p> <ul> <li><code>output_path (str)</code>: The file path where the CSV will be saved.</li> <li><code>results (dict)</code>: A nested dictionary containing travel time results: <code>{speed_name: {safe_zone_distance: [times_per_source]}}</code> where times_per_source is a list of travel times (in hours) for each source.</li> <li><code>min_coords (dict)</code>: A nested dictionary containing the coordinates of minimum travel times: <code>{speed_name: {safe_zone_distance: [coords_per_source]}}</code> where coords_per_source is a list of (row, col) tuples for each source.</li> <li><code>source_names (list)</code>: A list of source location names corresponding to indices in the results arrays.</li> <li><code>walking_speeds (dict)</code>: Dictionary mapping speed names (str) to speed values (float) in meters per second.</li> <li><code>safe_zone_distances (list)</code>: List of safe zone distances used in the analysis.</li> </ul> <p>Returns:</p> <ul> <li><code>None</code>: The function does not return a value, but writes results to a CSV file.</li> </ul>"},{"location":"api/evacuation-analysis/path-utils.html","title":"Path Utilities","text":"<p>The <code>path_utils.py</code> module provides functions for working with path finding and graph operations in evacuation analysis.</p>"},{"location":"api/evacuation-analysis/path-utils.html#functions","title":"Functions","text":""},{"location":"api/evacuation-analysis/path-utils.html#calculate_path_metrics","title":"<code>calculate_path_metrics</code>","text":"<pre><code>def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols)\n</code></pre> <p>Calculate metrics for the shortest path between source and target nodes.</p> <p>This function reconstructs the shortest path using the predecessor array and computes various metrics including the number of pixels in the path, the cost of each step, and the total path cost.</p> <p>Parameters:</p> <ul> <li><code>predecessors (numpy.ndarray)</code>: A 1D array where predecessors[i] contains the predecessor node of node i on the shortest path from the source.</li> <li><code>source_node (int)</code>: The source node index (starting point) of the path.</li> <li><code>target_node (int or float)</code>: The target node index (destination) of the path. Can be -9999 or NaN to indicate an invalid target.</li> <li><code>graph_csr (scipy.sparse.csr_matrix)</code>: The graph in CSR format where each entry (i,j) represents the cost/weight of the edge from node i to node j.</li> <li><code>rows (int)</code>: The number of rows in the original raster grid.</li> <li><code>cols (int)</code>: The number of columns in the original raster grid.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>pixel_count (int)</code>: The number of pixels in the path.</li> <li><code>cell_costs (list)</code>: A list of the costs for each step in the path.</li> <li><code>total_cost (float)</code>: The sum of all cell costs along the path.</li> </ul>"},{"location":"api/evacuation-analysis/path-utils.html#reconstruct_path","title":"<code>reconstruct_path</code>","text":"<pre><code>def reconstruct_path(pred, source_node, target_node, cols)\n</code></pre> <p>Reconstruct the shortest path from source to target and convert to 2D coordinates.</p> <p>This function traces back through a predecessor array to reconstruct the path from source to target, then converts the 1D node indices to 2D grid coordinates.</p> <p>Parameters:</p> <ul> <li><code>pred (numpy.ndarray)</code>: A 1D array where pred[i] contains the predecessor node of node i on the shortest path from the source.</li> <li><code>source_node (int)</code>: The source node index (starting point) of the path.</li> <li><code>target_node (int or float)</code>: The target node index (destination) of the path. Can be -9999 or NaN to indicate an invalid target.</li> <li><code>cols (int)</code>: The number of columns in the grid, used to convert 1D indices to 2D coordinates.</li> </ul> <p>Returns:</p> <ul> <li><code>list</code>: A list of tuples (row, col) representing the 2D coordinates of each point along the path from source to target. Returns an empty list if no valid path exists.</li> </ul>"},{"location":"api/evacuation-analysis/path-utils.html#build_adjacency_matrix","title":"<code>build_adjacency_matrix</code>","text":"<pre><code>def build_adjacency_matrix(cost_array, rows, cols, directions)\n</code></pre> <p>Build a sparse adjacency matrix representing the graph for path finding.</p> <p>This function creates a graph representation of the raster grid where nodes are grid cells and edges represent possible movements between adjacent cells. Edge weights are derived from the cost array, with diagonal movements adjusted by a factor of sqrt(2) to account for the increased distance.</p> <p>Parameters:</p> <ul> <li><code>cost_array (numpy.ndarray)</code>: A 3D array of shape (n_directions, rows, cols) containing the cost values for moving in each direction from each cell. The first dimension corresponds to the directions defined in the directions parameter.</li> <li><code>rows (int)</code>: The number of rows in the grid.</li> <li><code>cols (int)</code>: The number of columns in the grid.</li> <li><code>directions (list of tuples)</code>: A list of (dr, dc) tuples defining the possible movement directions. Typically includes the 8 neighboring directions (cardinal and diagonal).</li> </ul> <p>Returns:</p> <ul> <li><code>scipy.sparse.csr_matrix</code>: A sparse CSR matrix representation of the graph, where each entry (i, j) represents the cost of moving from node i to node j. The matrix has dimensions (rowscols, rowscols).</li> </ul>"},{"location":"api/evacuation-analysis/visualization.html","title":"Visualization","text":"<p>The <code>visualization.py</code> module provides functions for visualizing evacuation analysis results, including travel time comparisons, cost surfaces, and decomposition analysis.</p>"},{"location":"api/evacuation-analysis/visualization.html#functions","title":"Functions","text":""},{"location":"api/evacuation-analysis/visualization.html#load_raster","title":"<code>load_raster</code>","text":"<pre><code>def load_raster(path)\n</code></pre> <p>Load a single-band raster file and return its data and metadata.</p> <p>This helper function opens a raster file using rasterio and extracts the first band of data along with essential metadata for visualization.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: The file path to the raster file to be loaded.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>array (numpy.ndarray)</code>: The raster data from the first band with shape (rows, cols)</li> <li><code>metadata (dict)</code>: A dictionary containing:<ul> <li>'transform' (affine.Affine): The affine transform</li> <li>'crs' (CRS): The coordinate reference system</li> <li>'nodata' (float or int): The no-data value</li> </ul> </li> </ul>"},{"location":"api/evacuation-analysis/visualization.html#raster_coord_to_map_coords","title":"<code>raster_coord_to_map_coords</code>","text":"<pre><code>def raster_coord_to_map_coords(row, col, transform)\n</code></pre> <p>Convert raster coordinates (row, column) to map coordinates (x, y).</p> <p>This function transforms pixel coordinates in a raster to their corresponding geographic coordinates using the raster's affine transform.</p> <p>Parameters:</p> <ul> <li><code>row (int)</code>: The row index in the raster (zero-based).</li> <li><code>col (int)</code>: The column index in the raster (zero-based).</li> <li><code>transform (affine.Affine or list/tuple)</code>: The affine transform of the raster, defining the relationship between pixel coordinates and geographic coordinates.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple (x, y) containing the geographic coordinates corresponding to the specified raster cell.</li> </ul>"},{"location":"api/evacuation-analysis/visualization.html#plot_","title":"`plot_","text":""},{"location":"api/evacuation-analysis/New%20folder/analysis.html","title":"Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport time\nimport numpy as np\nimport rasterio\nfrom scipy.sparse.csgraph import dijkstra\nfrom grid_utils import process_raster, to_1d\nfrom io_utils import save_raster, load_raster\n</pre> import os import time import numpy as np import rasterio from scipy.sparse.csgraph import dijkstra from grid_utils import process_raster, to_1d from io_utils import save_raster, load_raster In\u00a0[\u00a0]: Copied! <pre>def run_dijkstra_analysis(graph_csr, source_nodes):\n    \"\"\"\n    Run Dijkstra's algorithm for all source nodes.\n    \n    This function computes the shortest paths from a set of source nodes to all other nodes\n    in a given graph using Dijkstra's algorithm. The graph is represented in Compressed Sparse\n    Row (CSR) format.\n    \n    Parameters:\n    -----------\n    graph_csr (scipy.sparse.csr_matrix): The graph in CSR format where each entry represents\n                                        the weight of the edge between nodes.\n    \n    source_nodes (array-like): A list or array of source node indices from which to calculate\n                              the shortest paths.\n    \n    Returns:\n    --------\n    distances (numpy.ndarray): A 2D array where distances[i, j] represents the shortest distance\n                              from source_nodes[i] to node j.\n    \n    predecessors (numpy.ndarray): A 2D array where predecessors[i, j] represents the predecessor\n                                 node on the shortest path from source_nodes[i] to node j.\n                                 A value of -9999 indicates no path exists.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The function uses scipy's implementation of Dijkstra's algorithm.\n    - The graph is treated as directed.\n    \"\"\"\n    print(\"\\nRunning Dijkstra's algorithm...\")\n    start_time = time.time()\n    distances, predecessors = dijkstra(csgraph=graph_csr, directed=True,\n                                      indices=source_nodes, return_predecessors=True)\n    end_time = time.time()\n    print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds\")\n    return distances, predecessors\n</pre> def run_dijkstra_analysis(graph_csr, source_nodes):     \"\"\"     Run Dijkstra's algorithm for all source nodes.          This function computes the shortest paths from a set of source nodes to all other nodes     in a given graph using Dijkstra's algorithm. The graph is represented in Compressed Sparse     Row (CSR) format.          Parameters:     -----------     graph_csr (scipy.sparse.csr_matrix): The graph in CSR format where each entry represents                                         the weight of the edge between nodes.          source_nodes (array-like): A list or array of source node indices from which to calculate                               the shortest paths.          Returns:     --------     distances (numpy.ndarray): A 2D array where distances[i, j] represents the shortest distance                               from source_nodes[i] to node j.          predecessors (numpy.ndarray): A 2D array where predecessors[i, j] represents the predecessor                                  node on the shortest path from source_nodes[i] to node j.                                  A value of -9999 indicates no path exists.          Notes:     ------     - Progress and timing information is printed to standard output.     - The function uses scipy's implementation of Dijkstra's algorithm.     - The graph is treated as directed.     \"\"\"     print(\"\\nRunning Dijkstra's algorithm...\")     start_time = time.time()     distances, predecessors = dijkstra(csgraph=graph_csr, directed=True,                                       indices=source_nodes, return_predecessors=True)     end_time = time.time()     print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds\")     return distances, predecessors In\u00a0[\u00a0]: Copied! <pre>def process_travel_times(base_path, source_name, dataset_key, speed_name, speed_value, meta):\n    \"\"\"\n    Process and save travel time rasters for a given speed value.\n    \n    This function loads a base raster, processes it to calculate travel times \n    based on the specified speed, and saves the resulting raster to disk.\n    \n    Parameters:\n    -----------\n    base_path (str): The file path to the base raster.\n    \n    source_name (str): The name of the source for which travel times are being processed.\n    \n    dataset_key (str): A key identifying the dataset, used in the output filename.\n    \n    speed_name (str): The name of the speed category (e.g., 'slow', 'moderate', 'fast').\n    \n    speed_value (float): The speed value to be used in processing the raster.\n    \n    meta (dict): Metadata for the output raster file.\n    \n    Returns:\n    --------\n    numpy.ndarray: The processed travel time array.\n    \n    Raises:\n    -------\n    FileNotFoundError: If the base raster file does not exist.\n    \n    ValueError: If there is an issue with the raster data or processing.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - Output file is saved as 'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n      in the same directory as the input file.\n    - Output raster uses float32 data type with -1 as the nodata value.\n    \"\"\"\n    print(f\"Processing travel times for {source_name} at {speed_name} speed...\")\n    \n    cost_array_base, cost_meta_base = load_raster(base_path)\n    travel_time_array = process_raster(cost_array_base, speed_value)\n    \n    out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n    out_path = os.path.join(os.path.dirname(base_path), out_filename)\n    save_raster(out_path, travel_time_array, meta, dtype=rasterio.float32, nodata=-1)\n    \n    return travel_time_array\n</pre> def process_travel_times(base_path, source_name, dataset_key, speed_name, speed_value, meta):     \"\"\"     Process and save travel time rasters for a given speed value.          This function loads a base raster, processes it to calculate travel times      based on the specified speed, and saves the resulting raster to disk.          Parameters:     -----------     base_path (str): The file path to the base raster.          source_name (str): The name of the source for which travel times are being processed.          dataset_key (str): A key identifying the dataset, used in the output filename.          speed_name (str): The name of the speed category (e.g., 'slow', 'moderate', 'fast').          speed_value (float): The speed value to be used in processing the raster.          meta (dict): Metadata for the output raster file.          Returns:     --------     numpy.ndarray: The processed travel time array.          Raises:     -------     FileNotFoundError: If the base raster file does not exist.          ValueError: If there is an issue with the raster data or processing.          Notes:     ------     - Progress and timing information is printed to standard output.     - Output file is saved as 'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'       in the same directory as the input file.     - Output raster uses float32 data type with -1 as the nodata value.     \"\"\"     print(f\"Processing travel times for {source_name} at {speed_name} speed...\")          cost_array_base, cost_meta_base = load_raster(base_path)     travel_time_array = process_raster(cost_array_base, speed_value)          out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'     out_path = os.path.join(os.path.dirname(base_path), out_filename)     save_raster(out_path, travel_time_array, meta, dtype=rasterio.float32, nodata=-1)          return travel_time_array In\u00a0[\u00a0]: Copied! <pre>def analyze_safe_zones(distance_from_summit, travel_time_data, safe_zone_distances, source_names):\n    \"\"\"\n    Analyze minimum travel times within specified safe zones from different source locations.\n    \n    Parameters:\n    -----------\n    distance_from_summit (numpy.ndarray): A 2D array representing the distance from the summit.\n    \n    travel_time_data (dict): A dictionary containing travel time data for different speeds and sources.\n                             The structure is expected to be:\n                             {\n                                 'speed_name': {\n                                     'source_name': {\n                                         'cost_array_flat': numpy.ndarray,  # Flattened travel times\n                                         'cost_array': numpy.ndarray        # 2D travel times\n                                     }\n                                 }\n                             }\n    \n    safe_zone_distances (list): A list of distances defining the safe zones in meters.\n    \n    source_names (list): A list of source names corresponding to the travel time data.\n    \n    Returns:\n    --------\n    tuple: A tuple containing two dictionaries:\n           - results: A dictionary with the minimum travel times (in hours) for each speed and safe zone.\n                      The structure is:\n                      {\n                          'speed_name': {\n                              safe_zone_distance: [min_time1, min_time2, ...]\n                          }\n                      }\n                      where the list indices correspond to the order of source_names.\n           \n           - min_coords: A dictionary with the coordinates of the minimum travel times for each speed and safe zone.\n                         The structure is:\n                         {\n                             'speed_name': {\n                                 safe_zone_distance: [(min_r1, min_c1), (min_r2, min_c2), ...]\n                             }\n                         }\n                         where each tuple contains the row and column indices in the original 2D array.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output during execution.\n    - Safe zones are defined as areas where distance_from_summit &gt;= safe_zone_distance.\n    - If no valid times are found within a safe zone, np.nan is used for both time and coordinates.\n    - All travel times are in hours.\n    \"\"\"\n    print(\"\\nAnalyzing safe zones...\")\n    \n    results = {}\n    min_coords = {}\n    \n    for speed_name in travel_time_data.keys():\n        print(f\"\\nProcessing {speed_name} speed...\")\n        results[speed_name] = {}\n        min_coords[speed_name] = {}\n        distance_from_summit_1d = distance_from_summit.ravel()\n        \n        for safe_zone in safe_zone_distances:\n            safe_zone_mask = (distance_from_summit_1d &gt;= safe_zone)\n            min_times_in_safe_zone = []\n            coords_in_safe_zone = []\n            \n            for source_name in source_names:\n                cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']\n                valid_times = cost_array_flat[safe_zone_mask]\n                valid_times = valid_times[~np.isnan(valid_times)]\n                \n                if len(valid_times) &gt; 0:\n                    min_time = np.min(valid_times)\n                    cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']\n                    safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)\n                    valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)\n                    min_idx = np.nanargmin(valid_times_2d)\n                    min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)\n                else:\n                    min_time = np.nan\n                    min_r, min_c = (np.nan, np.nan)\n                    \n                min_times_in_safe_zone.append(min_time)\n                coords_in_safe_zone.append((min_r, min_c))\n                print(f\"{source_name} at {safe_zone}m: min time = {min_time:.2f} hrs\")\n                \n            results[speed_name][safe_zone] = min_times_in_safe_zone\n            min_coords[speed_name][safe_zone] = coords_in_safe_zone\n            \n    return results, min_coords\n</pre> def analyze_safe_zones(distance_from_summit, travel_time_data, safe_zone_distances, source_names):     \"\"\"     Analyze minimum travel times within specified safe zones from different source locations.          Parameters:     -----------     distance_from_summit (numpy.ndarray): A 2D array representing the distance from the summit.          travel_time_data (dict): A dictionary containing travel time data for different speeds and sources.                              The structure is expected to be:                              {                                  'speed_name': {                                      'source_name': {                                          'cost_array_flat': numpy.ndarray,  # Flattened travel times                                          'cost_array': numpy.ndarray        # 2D travel times                                      }                                  }                              }          safe_zone_distances (list): A list of distances defining the safe zones in meters.          source_names (list): A list of source names corresponding to the travel time data.          Returns:     --------     tuple: A tuple containing two dictionaries:            - results: A dictionary with the minimum travel times (in hours) for each speed and safe zone.                       The structure is:                       {                           'speed_name': {                               safe_zone_distance: [min_time1, min_time2, ...]                           }                       }                       where the list indices correspond to the order of source_names.                        - min_coords: A dictionary with the coordinates of the minimum travel times for each speed and safe zone.                          The structure is:                          {                              'speed_name': {                                  safe_zone_distance: [(min_r1, min_c1), (min_r2, min_c2), ...]                              }                          }                          where each tuple contains the row and column indices in the original 2D array.          Notes:     ------     - Progress and timing information is printed to standard output during execution.     - Safe zones are defined as areas where distance_from_summit &gt;= safe_zone_distance.     - If no valid times are found within a safe zone, np.nan is used for both time and coordinates.     - All travel times are in hours.     \"\"\"     print(\"\\nAnalyzing safe zones...\")          results = {}     min_coords = {}          for speed_name in travel_time_data.keys():         print(f\"\\nProcessing {speed_name} speed...\")         results[speed_name] = {}         min_coords[speed_name] = {}         distance_from_summit_1d = distance_from_summit.ravel()                  for safe_zone in safe_zone_distances:             safe_zone_mask = (distance_from_summit_1d &gt;= safe_zone)             min_times_in_safe_zone = []             coords_in_safe_zone = []                          for source_name in source_names:                 cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']                 valid_times = cost_array_flat[safe_zone_mask]                 valid_times = valid_times[~np.isnan(valid_times)]                                  if len(valid_times) &gt; 0:                     min_time = np.min(valid_times)                     cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']                     safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)                     valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)                     min_idx = np.nanargmin(valid_times_2d)                     min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)                 else:                     min_time = np.nan                     min_r, min_c = (np.nan, np.nan)                                      min_times_in_safe_zone.append(min_time)                 coords_in_safe_zone.append((min_r, min_c))                 print(f\"{source_name} at {safe_zone}m: min time = {min_time:.2f} hrs\")                              results[speed_name][safe_zone] = min_times_in_safe_zone             min_coords[speed_name][safe_zone] = coords_in_safe_zone                  return results, min_coords"},{"location":"api/evacuation-analysis/New%20folder/config.html","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nConfiguration settings for volcanic evacuation analysis.\n\nThis module establishes environment variables, defines walking speeds,\nand specifies file paths for various geospatial datasets used in the \nvolcanic evacuation analysis for Mt. Marapi.\n\nConstants:\n----------\nWALKING_SPEEDS : dict\n   Dictionary mapping speed categories to values in meters per second:\n   - 'slow': 0.91 m/s\n   - 'medium': 1.22 m/s\n   - 'fast': 1.52 m/s\n\nDATA_FOLDER : str\n   Base directory for python results.\n\nCOST_PATHS : dict\n   Dictionary mapping cost raster names to their file paths:\n   - 'final': Main cost raster with original hiking path\n   - 'modify_landcover': Cost raster with modified land cover\n   - 'walking_speed': Walking speed cost raster\n   - 'base_cost': Base cost raster\n\nSUMMIT_PATH : str\n   File path to the Mt. Marapi summit location (GPKG format).\n\nCAMP_SPOT_PATH : str\n   File path to camping locations (GPKG format).\n\nHIKING_PATH : str\n   File path to the original hiking path (GPKG format).\n\nLANDCOVER_100M : str\n   File path to land cover data with 100m buffer (TIF format).\n\nDEM_100M : str\n   File path to digital elevation model with 100m buffer (TIF format).\n\nSOURCE_NAMES : list\n   List of source location names for evacuation analysis:\n   ['summit', 'camp1', 'camp2', 'camp3', 'camp4']\n\nSAFE_ZONE_DISTANCES : list\n   List of safe zone distances from 500m to 4500m in 500m increments.\n\nDIRECTIONS : list\n   List of tuples representing the eight possible movement directions\n   for path analysis (cardinal and intercardinal directions).\n\nEnvironment Variables:\n---------------------\nUSE_PATH_FOR_GDAL_PYTHON : str\n   Set to \"True\" to use the system path for GDAL Python bindings.\n\nPROJ_LIB : str\n   Set to pyproj's data directory for projection information.\n\"\"\"\n</pre> \"\"\" Configuration settings for volcanic evacuation analysis.  This module establishes environment variables, defines walking speeds, and specifies file paths for various geospatial datasets used in the  volcanic evacuation analysis for Mt. Marapi.  Constants: ---------- WALKING_SPEEDS : dict    Dictionary mapping speed categories to values in meters per second:    - 'slow': 0.91 m/s    - 'medium': 1.22 m/s    - 'fast': 1.52 m/s  DATA_FOLDER : str    Base directory for python results.  COST_PATHS : dict    Dictionary mapping cost raster names to their file paths:    - 'final': Main cost raster with original hiking path    - 'modify_landcover': Cost raster with modified land cover    - 'walking_speed': Walking speed cost raster    - 'base_cost': Base cost raster  SUMMIT_PATH : str    File path to the Mt. Marapi summit location (GPKG format).  CAMP_SPOT_PATH : str    File path to camping locations (GPKG format).  HIKING_PATH : str    File path to the original hiking path (GPKG format).  LANDCOVER_100M : str    File path to land cover data with 100m buffer (TIF format).  DEM_100M : str    File path to digital elevation model with 100m buffer (TIF format).  SOURCE_NAMES : list    List of source location names for evacuation analysis:    ['summit', 'camp1', 'camp2', 'camp3', 'camp4']  SAFE_ZONE_DISTANCES : list    List of safe zone distances from 500m to 4500m in 500m increments.  DIRECTIONS : list    List of tuples representing the eight possible movement directions    for path analysis (cardinal and intercardinal directions).  Environment Variables: --------------------- USE_PATH_FOR_GDAL_PYTHON : str    Set to \"True\" to use the system path for GDAL Python bindings.  PROJ_LIB : str    Set to pyproj's data directory for projection information. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport pyproj\n# Set environment variables\nos.environ['USE_PATH_FOR_GDAL_PYTHON'] = \"True\"\nos.environ['PROJ_LIB'] = pyproj.datadir.get_data_dir()\n# Walking speeds (m/s)\nWALKING_SPEEDS = {\n   'slow': 0.91,\n   'medium': 1.22,\n   'fast': 1.52\n}\n# Define your paths directly\nDATA_FOLDER = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results'\n# Cost raster datasets\n# Modified to include 4 datasets instead of 2.\n# Cost raster datasets\nCOST_PATHS = {\n    'final': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_original.tif'),\n    'modify_landcover': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_modified.tif'),\n    'walking_speed': os.path.join(DATA_FOLDER, 'inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif'),\n    'base_cost': os.path.join(DATA_FOLDER, 'invert_landcovercost_original_Awu.tif')\n}\n# Input paths\nSUMMIT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\"\nCAMP_SPOT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\"\nHIKING_PATH = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp'\nlandcover_100m = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_LandCover_2019_100m_Buffer_UTM.tif'\nDEM_100m = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_DEM_100m_Buffer_UTM.tif\"\n# Source names\nSOURCE_NAMES = ['summit', 'camp1']\n# Safe zone distances\nSAFE_ZONE_DISTANCES = list(range(500, 5000, 500))\n# Movement directions\nDIRECTIONS = [\n   (-1, 0),   # Up\n   (1, 0),    # Down\n   (0, 1),    # Right\n   (0, -1),   # Left\n   (-1, 1),   # Up-Right\n   (-1, -1),  # Up-Left\n   (1, 1),    # Down-Right\n   (1, -1)    # Down-Left\n]\n</pre> import os import pyproj # Set environment variables os.environ['USE_PATH_FOR_GDAL_PYTHON'] = \"True\" os.environ['PROJ_LIB'] = pyproj.datadir.get_data_dir() # Walking speeds (m/s) WALKING_SPEEDS = {    'slow': 0.91,    'medium': 1.22,    'fast': 1.52 } # Define your paths directly DATA_FOLDER = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results' # Cost raster datasets # Modified to include 4 datasets instead of 2. # Cost raster datasets COST_PATHS = {     'final': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_original.tif'),     'modify_landcover': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_modified.tif'),     'walking_speed': os.path.join(DATA_FOLDER, 'inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif'),     'base_cost': os.path.join(DATA_FOLDER, 'invert_landcovercost_original_Awu.tif') } # Input paths SUMMIT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\" CAMP_SPOT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\" HIKING_PATH = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp' landcover_100m = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_LandCover_2019_100m_Buffer_UTM.tif' DEM_100m = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_DEM_100m_Buffer_UTM.tif\" # Source names SOURCE_NAMES = ['summit', 'camp1'] # Safe zone distances SAFE_ZONE_DISTANCES = list(range(500, 5000, 500)) # Movement directions DIRECTIONS = [    (-1, 0),   # Up    (1, 0),    # Down    (0, 1),    # Right    (0, -1),   # Left    (-1, 1),   # Up-Right    (-1, -1),  # Up-Left    (1, 1),    # Down-Right    (1, -1)    # Down-Left ]"},{"location":"api/evacuation-analysis/New%20folder/decomposition.html","title":"Decomposition","text":"<p>decomposition.py</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom config import COST_PATHS, DIRECTIONS\nfrom io_utils import read_raster\nfrom grid_utils import to_1d\n</pre> import numpy as np from config import COST_PATHS, DIRECTIONS from io_utils import read_raster from grid_utils import to_1d In\u00a0[\u00a0]: Copied! <pre>def reconstruct_path(predecessors, target, source):\n    \"\"\"\n    Reconstruct the shortest path from source to target using the predecessor array.\n    \n    This function traces back through a predecessor array produced by a shortest path\n    algorithm (like Dijkstra's) to reconstruct the complete path from source to target.\n    \n    Parameters:\n    -----------\n    predecessors : numpy.ndarray\n        A 1D array where predecessors[i] contains the predecessor node of node i\n        on the shortest path from the source. This array should be for a single\n        source node.\n    \n    target : int\n        The target node index (destination) for which to reconstruct the path.\n    \n    source : int\n        The source node index (starting point) from which the path begins.\n    \n    Returns:\n    --------\n    path : list\n        A list of node indices representing the shortest path from source to target,\n        inclusive of both endpoints. The path is ordered from source to target.\n    \n    Notes:\n    ------\n    - The function assumes that node indices are represented as integers.\n    - A special value (-9999) in the predecessor array indicates \"no predecessor\".\n    - If there is no valid path from source to target, the returned path will\n      only contain the target node.\n    - The path includes both the source and target nodes.\n    \"\"\"\n    path = []\n    node = target\n    # Use a special value (-9999) to indicate \"no predecessor\"\n    while node != source and node != -9999:\n        path.append(node)\n        node = predecessors[node]\n    path.append(source)\n    path.reverse()\n    return path\n</pre> def reconstruct_path(predecessors, target, source):     \"\"\"     Reconstruct the shortest path from source to target using the predecessor array.          This function traces back through a predecessor array produced by a shortest path     algorithm (like Dijkstra's) to reconstruct the complete path from source to target.          Parameters:     -----------     predecessors : numpy.ndarray         A 1D array where predecessors[i] contains the predecessor node of node i         on the shortest path from the source. This array should be for a single         source node.          target : int         The target node index (destination) for which to reconstruct the path.          source : int         The source node index (starting point) from which the path begins.          Returns:     --------     path : list         A list of node indices representing the shortest path from source to target,         inclusive of both endpoints. The path is ordered from source to target.          Notes:     ------     - The function assumes that node indices are represented as integers.     - A special value (-9999) in the predecessor array indicates \"no predecessor\".     - If there is no valid path from source to target, the returned path will       only contain the target node.     - The path includes both the source and target nodes.     \"\"\"     path = []     node = target     # Use a special value (-9999) to indicate \"no predecessor\"     while node != source and node != -9999:         path.append(node)         node = predecessors[node]     path.append(source)     path.reverse()     return path In\u00a0[\u00a0]: Copied! <pre>def expand_single_band(cost_array):\n    \"\"\"\n    Expand a single-band raster into an 8-band array representing different movement directions.\n    \n    This function takes a single-band cost raster and expands it into 8 bands corresponding to\n    the 8 movement directions (4 cardinal and 4 diagonal). The diagonal directions (bands 4-7)\n    are multiplied by sqrt(2) to account for the increased distance when moving diagonally.\n    \n    Parameters:\n    -----------\n    cost_array : numpy.ndarray\n        Input cost raster with shape (1, rows, cols), where the first dimension\n        represents a single band.\n    \n    Returns:\n    --------\n    expanded : numpy.ndarray\n        An 8-band raster with shape (8, rows, cols), where each band represents\n        the cost in one of the 8 movement directions. Bands 0-3 represent cardinal\n        directions, while bands 4-7 represent diagonal directions with costs\n        multiplied by sqrt(2).\n    \n    Notes:\n    ------\n    - The function assumes that the input cost_array has shape (1, rows, cols).\n    - The cardinal directions are ordered as: Up, Down, Right, Left\n    - The diagonal directions are ordered as: Up-Right, Up-Left, Down-Right, Down-Left\n    - The returned array preserves the data type of the input array.\n    \"\"\"\n    single_band = cost_array[0]\n    rows, cols = single_band.shape\n    expanded = np.zeros((8, rows, cols), dtype=single_band.dtype)\n    for i in range(8):\n        expanded[i] = single_band.copy()\n        if i &gt;= 4:  # Diagonal directions\n            expanded[i] *= np.sqrt(2)\n    return expanded\n</pre> def expand_single_band(cost_array):     \"\"\"     Expand a single-band raster into an 8-band array representing different movement directions.          This function takes a single-band cost raster and expands it into 8 bands corresponding to     the 8 movement directions (4 cardinal and 4 diagonal). The diagonal directions (bands 4-7)     are multiplied by sqrt(2) to account for the increased distance when moving diagonally.          Parameters:     -----------     cost_array : numpy.ndarray         Input cost raster with shape (1, rows, cols), where the first dimension         represents a single band.          Returns:     --------     expanded : numpy.ndarray         An 8-band raster with shape (8, rows, cols), where each band represents         the cost in one of the 8 movement directions. Bands 0-3 represent cardinal         directions, while bands 4-7 represent diagonal directions with costs         multiplied by sqrt(2).          Notes:     ------     - The function assumes that the input cost_array has shape (1, rows, cols).     - The cardinal directions are ordered as: Up, Down, Right, Left     - The diagonal directions are ordered as: Up-Right, Up-Left, Down-Right, Down-Left     - The returned array preserves the data type of the input array.     \"\"\"     single_band = cost_array[0]     rows, cols = single_band.shape     expanded = np.zeros((8, rows, cols), dtype=single_band.dtype)     for i in range(8):         expanded[i] = single_band.copy()         if i &gt;= 4:  # Diagonal directions             expanded[i] *= np.sqrt(2)     return expanded In\u00a0[\u00a0]: Copied! <pre>def run_decomposition_analysis(dataset_info, min_coords_all):\n    \"\"\"\n    Perform a decomposition analysis to quantify the relative contributions of slope and land cover\n    to the optimal evacuation paths from the summit.\n    \n    This function analyzes the shortest paths identified for different safe zone thresholds and\n    calculates the percentage contribution of slope (walking speed) and land cover factors to\n    the overall path cost. The analysis uses three cost rasters: the final combined cost,\n    the walking speed (slope) cost, and the base (land cover) cost.\n    \n    Parameters:\n    -----------\n    dataset_info : dict\n        Dictionary containing information about the datasets, including:\n        - 'final': dict containing:\n            - 'pred_summit': ndarray - Predecessor array for the summit source\n            - 'cols': int - Number of columns in the raster\n            - 'summit_raster_coords': tuple or list - Coordinates of the summit in the raster\n    \n    min_coords_all : dict\n        Nested dictionary containing the minimum travel time coordinates for each dataset,\n        speed, and safe zone distance:\n        {\n            dataset_key: {\n                speed_name: {\n                    safe_zone_distance: [(row, col), ...]\n                }\n            }\n        }\n    \n    Returns:\n    --------\n    list of dict\n        A list of dictionaries, each representing a row in the results table with keys:\n        - \"Safe Zone Threshold (m)\": int - The safe zone distance threshold\n        - \"Slope Contribution (%)\": float - Percentage contribution of slope to the path cost\n        - \"Landcover Contribution (%)\": float - Percentage contribution of land cover to the path cost\n    \n    Notes:\n    ------\n    - The function assumes that the summit is at index 0 in the source list.\n    - Contributions are calculated using the logarithmic sum of costs along the path.\n    - The 'medium' speed category is used for analysis.\n    - Safe zone distances from 500m to 4500m in 500m increments are analyzed.\n    - Paths with missing or invalid cost values are skipped.\n    \"\"\"\n    print(\"\\n=== Running Decomposition Analysis ===\")\n    \n    # 1. Read final cost raster\n    final_array, meta_final, transform_final, nodata_final, bounds_final, res_final = read_raster(COST_PATHS['final'])\n    if final_array.shape[0] == 1:\n        print(\"Expanding single-band final raster to 8 directions...\")\n        final_array = expand_single_band(final_array)\n\n    # 2. Read walking speed (slope) cost raster\n    walking_speed_array, meta_ws, transform_ws, nodata_ws, bounds_ws, res_ws = read_raster(COST_PATHS['walking_speed'])\n    if walking_speed_array.shape[0] == 1:\n        print(\"Expanding single-band walking_speed raster to 8 directions...\")\n        walking_speed_array = expand_single_band(walking_speed_array)\n\n    # 3. Read base cost (landcover) raster\n    base_cost_array, meta_base, transform_base, nodata_base, bounds_base, res_base = read_raster(COST_PATHS['base_cost'])\n    if base_cost_array.shape[0] == 1:\n        print(\"Expanding single-band base_cost raster to 8 directions...\")\n        base_cost_array = expand_single_band(base_cost_array)\n\n    # 4. Retrieve the predecessor array from the final dataset (summit source index 0)\n    pred_final = dataset_info['final']['pred_summit']\n    cols = dataset_info['final']['cols']\n\n    # 5. Retrieve the summit raster coords\n    if isinstance(dataset_info['final']['summit_raster_coords'], tuple):\n        summit_idx = dataset_info['final']['summit_raster_coords']\n    else:\n        summit_idx = dataset_info['final']['summit_raster_coords'][0]\n    summit_node = to_1d(summit_idx[0], summit_idx[1], cols)\n\n    # We assume the same safe zone thresholds used previously\n    safe_zone_distances = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]\n\n    # For the final dataset, we pick a walking speed from min_coords_all, e.g. 'medium'\n    selected_speed = 'medium'\n    print(f\"Decomposition analysis for 'final' dataset, source: summit, speed: {selected_speed}\")\n\n    # Prepare a list for table rows\n    table_data = []\n\n    # Create a lookup for directions =&gt; band index\n    direction_indices = {d: i for i, d in enumerate(DIRECTIONS)}\n\n    # 6. Loop over each safe zone threshold, reconstruct path, compute contributions\n    for safe_zone in safe_zone_distances:\n        # Summits are at source index 0 in your code\n        safe_zone_coord = min_coords_all['final'][selected_speed][safe_zone][0]  # (row, col)\n        # Skip if no valid coordinate\n        if np.isnan(safe_zone_coord[0]) or np.isnan(safe_zone_coord[1]):\n            print(f\"Safe zone {safe_zone} m: No valid pixel found.\")\n            continue\n\n        target_node = to_1d(int(safe_zone_coord[0]), int(safe_zone_coord[1]), cols)\n        path_nodes = reconstruct_path(pred_final, target_node, summit_node)\n\n        sum_log_slope = 0.0\n        sum_log_land = 0.0\n        valid_path = True\n\n        for i in range(len(path_nodes) - 1):\n            current = path_nodes[i]\n            nxt = path_nodes[i+1]\n            r, c = divmod(current, cols)\n            nr, nc = divmod(nxt, cols)\n            dr = nr - r\n            dc = nc - c\n            direction = (dr, dc)\n            if direction not in direction_indices:\n                print(f\"Unexpected movement direction {direction} from {current} to {nxt}\")\n                valid_path = False\n                break\n\n            band = direction_indices[direction]\n            slope_val = walking_speed_array[band, r, c]\n            land_val = base_cost_array[band, r, c]\n            # Check positivity for log\n            if slope_val &lt;= 0 or land_val &lt;= 0:\n                print(f\"Invalid cost at {r},{c}, slope={slope_val}, land={land_val}\")\n                valid_path = False\n                break\n\n            sum_log_slope += np.log(slope_val)\n            sum_log_land += np.log(land_val)\n\n        if not valid_path:\n            print(f\"Safe zone {safe_zone} m: path issue, skipping.\")\n            continue\n\n        total_log = sum_log_slope + sum_log_land\n        perc_slope = (sum_log_slope / total_log) * 100\n        perc_land = (sum_log_land / total_log) * 100\n\n        # Store one row for the table\n        row_dict = {\n            \"Safe Zone Threshold (m)\": safe_zone,\n            \"Slope Contribution (%)\": round(perc_slope, 2),\n            \"Landcover Contribution (%)\": round(perc_land, 2)\n        }\n        table_data.append(row_dict)\n\n    return table_data\n</pre> def run_decomposition_analysis(dataset_info, min_coords_all):     \"\"\"     Perform a decomposition analysis to quantify the relative contributions of slope and land cover     to the optimal evacuation paths from the summit.          This function analyzes the shortest paths identified for different safe zone thresholds and     calculates the percentage contribution of slope (walking speed) and land cover factors to     the overall path cost. The analysis uses three cost rasters: the final combined cost,     the walking speed (slope) cost, and the base (land cover) cost.          Parameters:     -----------     dataset_info : dict         Dictionary containing information about the datasets, including:         - 'final': dict containing:             - 'pred_summit': ndarray - Predecessor array for the summit source             - 'cols': int - Number of columns in the raster             - 'summit_raster_coords': tuple or list - Coordinates of the summit in the raster          min_coords_all : dict         Nested dictionary containing the minimum travel time coordinates for each dataset,         speed, and safe zone distance:         {             dataset_key: {                 speed_name: {                     safe_zone_distance: [(row, col), ...]                 }             }         }          Returns:     --------     list of dict         A list of dictionaries, each representing a row in the results table with keys:         - \"Safe Zone Threshold (m)\": int - The safe zone distance threshold         - \"Slope Contribution (%)\": float - Percentage contribution of slope to the path cost         - \"Landcover Contribution (%)\": float - Percentage contribution of land cover to the path cost          Notes:     ------     - The function assumes that the summit is at index 0 in the source list.     - Contributions are calculated using the logarithmic sum of costs along the path.     - The 'medium' speed category is used for analysis.     - Safe zone distances from 500m to 4500m in 500m increments are analyzed.     - Paths with missing or invalid cost values are skipped.     \"\"\"     print(\"\\n=== Running Decomposition Analysis ===\")          # 1. Read final cost raster     final_array, meta_final, transform_final, nodata_final, bounds_final, res_final = read_raster(COST_PATHS['final'])     if final_array.shape[0] == 1:         print(\"Expanding single-band final raster to 8 directions...\")         final_array = expand_single_band(final_array)      # 2. Read walking speed (slope) cost raster     walking_speed_array, meta_ws, transform_ws, nodata_ws, bounds_ws, res_ws = read_raster(COST_PATHS['walking_speed'])     if walking_speed_array.shape[0] == 1:         print(\"Expanding single-band walking_speed raster to 8 directions...\")         walking_speed_array = expand_single_band(walking_speed_array)      # 3. Read base cost (landcover) raster     base_cost_array, meta_base, transform_base, nodata_base, bounds_base, res_base = read_raster(COST_PATHS['base_cost'])     if base_cost_array.shape[0] == 1:         print(\"Expanding single-band base_cost raster to 8 directions...\")         base_cost_array = expand_single_band(base_cost_array)      # 4. Retrieve the predecessor array from the final dataset (summit source index 0)     pred_final = dataset_info['final']['pred_summit']     cols = dataset_info['final']['cols']      # 5. Retrieve the summit raster coords     if isinstance(dataset_info['final']['summit_raster_coords'], tuple):         summit_idx = dataset_info['final']['summit_raster_coords']     else:         summit_idx = dataset_info['final']['summit_raster_coords'][0]     summit_node = to_1d(summit_idx[0], summit_idx[1], cols)      # We assume the same safe zone thresholds used previously     safe_zone_distances = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]      # For the final dataset, we pick a walking speed from min_coords_all, e.g. 'medium'     selected_speed = 'medium'     print(f\"Decomposition analysis for 'final' dataset, source: summit, speed: {selected_speed}\")      # Prepare a list for table rows     table_data = []      # Create a lookup for directions =&gt; band index     direction_indices = {d: i for i, d in enumerate(DIRECTIONS)}      # 6. Loop over each safe zone threshold, reconstruct path, compute contributions     for safe_zone in safe_zone_distances:         # Summits are at source index 0 in your code         safe_zone_coord = min_coords_all['final'][selected_speed][safe_zone][0]  # (row, col)         # Skip if no valid coordinate         if np.isnan(safe_zone_coord[0]) or np.isnan(safe_zone_coord[1]):             print(f\"Safe zone {safe_zone} m: No valid pixel found.\")             continue          target_node = to_1d(int(safe_zone_coord[0]), int(safe_zone_coord[1]), cols)         path_nodes = reconstruct_path(pred_final, target_node, summit_node)          sum_log_slope = 0.0         sum_log_land = 0.0         valid_path = True          for i in range(len(path_nodes) - 1):             current = path_nodes[i]             nxt = path_nodes[i+1]             r, c = divmod(current, cols)             nr, nc = divmod(nxt, cols)             dr = nr - r             dc = nc - c             direction = (dr, dc)             if direction not in direction_indices:                 print(f\"Unexpected movement direction {direction} from {current} to {nxt}\")                 valid_path = False                 break              band = direction_indices[direction]             slope_val = walking_speed_array[band, r, c]             land_val = base_cost_array[band, r, c]             # Check positivity for log             if slope_val &lt;= 0 or land_val &lt;= 0:                 print(f\"Invalid cost at {r},{c}, slope={slope_val}, land={land_val}\")                 valid_path = False                 break              sum_log_slope += np.log(slope_val)             sum_log_land += np.log(land_val)          if not valid_path:             print(f\"Safe zone {safe_zone} m: path issue, skipping.\")             continue          total_log = sum_log_slope + sum_log_land         perc_slope = (sum_log_slope / total_log) * 100         perc_land = (sum_log_land / total_log) * 100          # Store one row for the table         row_dict = {             \"Safe Zone Threshold (m)\": safe_zone,             \"Slope Contribution (%)\": round(perc_slope, 2),             \"Landcover Contribution (%)\": round(perc_land, 2)         }         table_data.append(row_dict)      return table_data"},{"location":"api/evacuation-analysis/New%20folder/grid-utils.html","title":"Grid utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport time\nfrom rasterio.transform import xy\n</pre> import numpy as np import time from rasterio.transform import xy In\u00a0[\u00a0]: Copied! <pre>def coords_to_raster(gdf, transform, bounds, res):\n    \"\"\"\n    Convert geographic coordinates to raster row and column indices.\n    \n    This function takes a GeoDataFrame containing point geometries and converts\n    the geographic coordinates (x, y) of each point to raster indices (row, col),\n    based on the provided raster transform, bounds, and resolution.\n    \n    Parameters:\n    -----------\n    gdf : geopandas.GeoDataFrame\n        A GeoDataFrame containing point geometries to be converted to raster indices.\n    \n    transform : affine.Affine\n        The affine transform of the raster, defining the relationship between\n        pixel coordinates and geographic coordinates.\n    \n    bounds : rasterio.coords.BoundingBox\n        The geographic bounds of the raster (left, bottom, right, top).\n    \n    res : tuple\n        A tuple (x_res, y_res) containing the pixel resolution in the x and y\n        directions (typically in map units per pixel).\n    \n    Returns:\n    --------\n    raster_coords : list\n        A list of tuples (row, col) representing the raster indices corresponding\n        to each point in the input GeoDataFrame. Points that fall outside the\n        raster bounds are excluded from the returned list.\n    \n    Notes:\n    ------\n    - Points falling outside the raster bounds are reported but not included in the output.\n    - The function assumes that the CRS of the GeoDataFrame matches the CRS of the raster.\n    - Progress and timing information is printed to standard output.\n    - Row indices increase from top to bottom, and column indices increase from left to right.\n    \"\"\"\n    print(\"Converting coordinates to raster indices...\")\n    raster_coords = []\n    for point in gdf.geometry:\n        x, y = point.x, point.y\n        if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):\n            print(f\"Point {x}, {y} is out of raster bounds.\")\n            continue\n        col = int((x - bounds.left) / res[0])\n        row = int((bounds.top - y) / res[1])\n        raster_coords.append((row, col))\n    return raster_coords\n</pre> def coords_to_raster(gdf, transform, bounds, res):     \"\"\"     Convert geographic coordinates to raster row and column indices.          This function takes a GeoDataFrame containing point geometries and converts     the geographic coordinates (x, y) of each point to raster indices (row, col),     based on the provided raster transform, bounds, and resolution.          Parameters:     -----------     gdf : geopandas.GeoDataFrame         A GeoDataFrame containing point geometries to be converted to raster indices.          transform : affine.Affine         The affine transform of the raster, defining the relationship between         pixel coordinates and geographic coordinates.          bounds : rasterio.coords.BoundingBox         The geographic bounds of the raster (left, bottom, right, top).          res : tuple         A tuple (x_res, y_res) containing the pixel resolution in the x and y         directions (typically in map units per pixel).          Returns:     --------     raster_coords : list         A list of tuples (row, col) representing the raster indices corresponding         to each point in the input GeoDataFrame. Points that fall outside the         raster bounds are excluded from the returned list.          Notes:     ------     - Points falling outside the raster bounds are reported but not included in the output.     - The function assumes that the CRS of the GeoDataFrame matches the CRS of the raster.     - Progress and timing information is printed to standard output.     - Row indices increase from top to bottom, and column indices increase from left to right.     \"\"\"     print(\"Converting coordinates to raster indices...\")     raster_coords = []     for point in gdf.geometry:         x, y = point.x, point.y         if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):             print(f\"Point {x}, {y} is out of raster bounds.\")             continue         col = int((x - bounds.left) / res[0])         row = int((bounds.top - y) / res[1])         raster_coords.append((row, col))     return raster_coords In\u00a0[\u00a0]: Copied! <pre>def to_1d(r, c, cols):\n    \"\"\"\n    Convert 2D raster coordinates (row, column) to a 1D array index.\n    \n    This function maps 2D coordinates in a raster or grid to the corresponding\n    1D array index, assuming row-major order (C-style).\n    \n    Parameters:\n    -----------\n    r : int\n        The row index (zero-based).\n    \n    c : int\n        The column index (zero-based).\n    \n    cols : int\n        The total number of columns in the 2D array or grid.\n    \n    Returns:\n    --------\n    int\n        The 1D array index corresponding to the given 2D coordinates.\n    \n    Notes:\n    ------\n    - The function assumes zero-based indexing for both input and output.\n    - The function assumes row-major order (C-style), where elements within a row\n      are stored contiguously.\n    - No bounds checking is performed; it's the caller's responsibility to ensure\n      r and c are valid indices.\n    \n    Examples:\n    ---------\n    &gt;&gt;&gt; to_1d(2, 3, 5)\n    13\n    # In a 5-column grid, the position at row 2, column 3 maps to index 13\n    \"\"\"\n    return r * cols + c\n</pre> def to_1d(r, c, cols):     \"\"\"     Convert 2D raster coordinates (row, column) to a 1D array index.          This function maps 2D coordinates in a raster or grid to the corresponding     1D array index, assuming row-major order (C-style).          Parameters:     -----------     r : int         The row index (zero-based).          c : int         The column index (zero-based).          cols : int         The total number of columns in the 2D array or grid.          Returns:     --------     int         The 1D array index corresponding to the given 2D coordinates.          Notes:     ------     - The function assumes zero-based indexing for both input and output.     - The function assumes row-major order (C-style), where elements within a row       are stored contiguously.     - No bounds checking is performed; it's the caller's responsibility to ensure       r and c are valid indices.          Examples:     ---------     &gt;&gt;&gt; to_1d(2, 3, 5)     13     # In a 5-column grid, the position at row 2, column 3 maps to index 13     \"\"\"     return r * cols + c In\u00a0[\u00a0]: Copied! <pre>def raster_coord_to_map_coords(row, col, transform):\n    \"\"\"\n    Convert raster coordinates (row, column) to geographic map coordinates (x, y).\n    \n    This function transforms pixel coordinates in a raster to their corresponding\n    geographic coordinates using the raster's affine transform.\n    \n    Parameters:\n    -----------\n    row : int\n        The row index in the raster (zero-based).\n    \n    col : int\n        The column index in the raster (zero-based).\n    \n    transform : affine.Affine\n        The affine transform of the raster, defining the relationship between\n        pixel coordinates and geographic coordinates.\n    \n    Returns:\n    --------\n    tuple\n        A tuple (x, y) containing the geographic coordinates corresponding to\n        the center of the specified raster cell.\n    \n    Notes:\n    ------\n    - The function uses the 'xy' function (likely from rasterio.transform) to perform\n      the coordinate transformation.\n    - By default, the coordinates returned correspond to the center of the pixel.\n    - The coordinate reference system (CRS) of the returned coordinates matches\n      the CRS of the input transform.\n    \"\"\"\n    x, y = xy(transform, row, col, offset='center')\n    return x, y\n</pre> def raster_coord_to_map_coords(row, col, transform):     \"\"\"     Convert raster coordinates (row, column) to geographic map coordinates (x, y).          This function transforms pixel coordinates in a raster to their corresponding     geographic coordinates using the raster's affine transform.          Parameters:     -----------     row : int         The row index in the raster (zero-based).          col : int         The column index in the raster (zero-based).          transform : affine.Affine         The affine transform of the raster, defining the relationship between         pixel coordinates and geographic coordinates.          Returns:     --------     tuple         A tuple (x, y) containing the geographic coordinates corresponding to         the center of the specified raster cell.          Notes:     ------     - The function uses the 'xy' function (likely from rasterio.transform) to perform       the coordinate transformation.     - By default, the coordinates returned correspond to the center of the pixel.     - The coordinate reference system (CRS) of the returned coordinates matches       the CRS of the input transform.     \"\"\"     x, y = xy(transform, row, col, offset='center')     return x, y In\u00a0[\u00a0]: Copied! <pre>def process_raster(cost_array, walking_speed):\n    \"\"\"\n    Convert base cost raster to travel time in hours.\n    \n    This function transforms a cost raster into a travel time raster by applying\n    the following steps:\n    1. Multiply by cell size (100m) to convert to distance units\n    2. Divide by walking speed to convert to time in seconds\n    3. Convert seconds to hours\n    4. Replace infinite values with -1 (indicating inaccessible areas)\n    \n    Parameters:\n    -----------\n    cost_array : numpy.ndarray\n        Input cost raster array. Values typically represent the inverse of the\n        travel cost or difficulty of traversing each cell.\n    \n    walking_speed : float\n        Walking speed in meters per second (m/s). Used to convert distance to time.\n    \n    Returns:\n    --------\n    numpy.ndarray\n        Array of travel times in hours. The array has the same shape as the input\n        cost_array, with infinite values replaced by -1.\n    \n    Notes:\n    ------\n    - The function assumes a fixed cell size of 100 meters.\n    - Timing information is printed to standard output.\n    - Infinite values in the output (resulting from division by zero or from input)\n      are replaced with -1 to represent inaccessible areas.\n    - The returned array maintains the same shape and data type as the input array.\n    \"\"\"\n    cost_array = cost_array * 100   # multiply by cell size (100 m)\n    cost_array = cost_array / walking_speed  # convert to seconds (m / (m/s) = s)\n    cost_array = cost_array / 3600  # convert seconds to hours\n    cost_array[np.isinf(cost_array)] = -1\n    return cost_array\n</pre> def process_raster(cost_array, walking_speed):     \"\"\"     Convert base cost raster to travel time in hours.          This function transforms a cost raster into a travel time raster by applying     the following steps:     1. Multiply by cell size (100m) to convert to distance units     2. Divide by walking speed to convert to time in seconds     3. Convert seconds to hours     4. Replace infinite values with -1 (indicating inaccessible areas)          Parameters:     -----------     cost_array : numpy.ndarray         Input cost raster array. Values typically represent the inverse of the         travel cost or difficulty of traversing each cell.          walking_speed : float         Walking speed in meters per second (m/s). Used to convert distance to time.          Returns:     --------     numpy.ndarray         Array of travel times in hours. The array has the same shape as the input         cost_array, with infinite values replaced by -1.          Notes:     ------     - The function assumes a fixed cell size of 100 meters.     - Timing information is printed to standard output.     - Infinite values in the output (resulting from division by zero or from input)       are replaced with -1 to represent inaccessible areas.     - The returned array maintains the same shape and data type as the input array.     \"\"\"     cost_array = cost_array * 100   # multiply by cell size (100 m)     cost_array = cost_array / walking_speed  # convert to seconds (m / (m/s) = s)     cost_array = cost_array / 3600  # convert seconds to hours     cost_array[np.isinf(cost_array)] = -1     return cost_array In\u00a0[\u00a0]: Copied! <pre>def calculate_distance_from_summit(summit_coords, rows, cols, cell_size=100):\n    \"\"\"\n    Compute Euclidean distance (in meters) from the summit for each cell in a raster.\n    \n    This function creates a distance raster where each cell contains the straight-line\n    distance from that cell to the summit location. The distance is calculated using\n    the Euclidean formula and converted to meters using the specified cell size.\n    \n    Parameters:\n    -----------\n    summit_coords : tuple\n        A tuple (row, col) containing the raster coordinates of the summit.\n    \n    rows : int\n        The number of rows in the output raster.\n    \n    cols : int\n        The number of columns in the output raster.\n    \n    cell_size : float, optional\n        The cell size in meters. Default is 100 meters.\n    \n    Returns:\n    --------\n    numpy.ndarray\n        A 2D array of shape (rows, cols) containing the Euclidean distance from\n        each cell to the summit in meters.\n    \n    Notes:\n    ------\n    - The function assumes a regular grid with square cells of uniform size.\n    - The output array is of type numpy.float32.\n    - The calculation is performed using raster coordinates, with the result scaled\n      by the cell size to get distances in meters.\n    - Progress and timing information is printed to standard output.\n    - The computation uses a nested loop approach which may be slow for large rasters.\n      Consider using vectorized operations for better performance.\n    \"\"\"\n    print(\"Calculating distance from summit...\")\n    summit_row, summit_col = summit_coords\n    distance_from_summit = np.zeros((rows, cols), dtype=np.float32)\n    for r in range(rows):\n        for c in range(cols):\n            distance = np.sqrt((r - summit_row) ** 2 + (c - summit_col) ** 2) * cell_size\n            distance_from_summit[r, c] = distance\n    return distance_from_summit\n</pre> def calculate_distance_from_summit(summit_coords, rows, cols, cell_size=100):     \"\"\"     Compute Euclidean distance (in meters) from the summit for each cell in a raster.          This function creates a distance raster where each cell contains the straight-line     distance from that cell to the summit location. The distance is calculated using     the Euclidean formula and converted to meters using the specified cell size.          Parameters:     -----------     summit_coords : tuple         A tuple (row, col) containing the raster coordinates of the summit.          rows : int         The number of rows in the output raster.          cols : int         The number of columns in the output raster.          cell_size : float, optional         The cell size in meters. Default is 100 meters.          Returns:     --------     numpy.ndarray         A 2D array of shape (rows, cols) containing the Euclidean distance from         each cell to the summit in meters.          Notes:     ------     - The function assumes a regular grid with square cells of uniform size.     - The output array is of type numpy.float32.     - The calculation is performed using raster coordinates, with the result scaled       by the cell size to get distances in meters.     - Progress and timing information is printed to standard output.     - The computation uses a nested loop approach which may be slow for large rasters.       Consider using vectorized operations for better performance.     \"\"\"     print(\"Calculating distance from summit...\")     summit_row, summit_col = summit_coords     distance_from_summit = np.zeros((rows, cols), dtype=np.float32)     for r in range(rows):         for c in range(cols):             distance = np.sqrt((r - summit_row) ** 2 + (c - summit_col) ** 2) * cell_size             distance_from_summit[r, c] = distance     return distance_from_summit"},{"location":"api/evacuation-analysis/New%20folder/io-utils.html","title":"Io utils","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport csv\nimport time\nimport fiona\nimport geopandas as gpd\nimport rasterio\nimport numpy as np\nimport pandas as pd\n</pre> import os import csv import time import fiona import geopandas as gpd import rasterio import numpy as np import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def read_shapefile(path, crs_epsg=32651):\n    \"\"\"\n    Read a shapefile into a GeoDataFrame with proper coordinate reference system handling.\n    \n    This function reads a shapefile from the specified path using fiona and GeoPandas,\n    and ensures that a coordinate reference system (CRS) is assigned. If the shapefile\n    does not have a defined CRS, it will assign the specified EPSG code.\n    \n    Parameters:\n    -----------\n    path : str\n        The file path to the shapefile to be read.\n    \n    crs_epsg : int, optional\n        The EPSG code to assign if the shapefile lacks a CRS. Default is 32651\n        (WGS 84 / UTM zone 51N), which is appropriate for Indonesia/Sumatra region.\n    \n    Returns:\n    --------\n    geopandas.GeoDataFrame\n        A GeoDataFrame containing the features from the shapefile with the proper CRS.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The function uses fiona directly to open the file and then creates a GeoDataFrame\n      from the features.\n    - If the shapefile has a defined CRS, it will be preserved; otherwise, the specified\n      EPSG code will be assigned.\n    - EPSG 32651 (the default) is appropriate for the Mt. Awu region in Indonesia.\n    \n    Raises:\n    -------\n    FileNotFoundError\n        If the shapefile does not exist at the specified path.\n    fiona.errors.DriverError\n        If the file exists but cannot be read as a shapefile.\n    \"\"\"\n    print(f\"Reading shapefile: {os.path.basename(path)}\")\n    with fiona.open(path) as src:\n        gdf = gpd.GeoDataFrame.from_features(src)\n        if gdf.crs is None:\n            gdf.set_crs(epsg=crs_epsg, inplace=True)\n    return gdf\n</pre> def read_shapefile(path, crs_epsg=32651):     \"\"\"     Read a shapefile into a GeoDataFrame with proper coordinate reference system handling.          This function reads a shapefile from the specified path using fiona and GeoPandas,     and ensures that a coordinate reference system (CRS) is assigned. If the shapefile     does not have a defined CRS, it will assign the specified EPSG code.          Parameters:     -----------     path : str         The file path to the shapefile to be read.          crs_epsg : int, optional         The EPSG code to assign if the shapefile lacks a CRS. Default is 32651         (WGS 84 / UTM zone 51N), which is appropriate for Indonesia/Sumatra region.          Returns:     --------     geopandas.GeoDataFrame         A GeoDataFrame containing the features from the shapefile with the proper CRS.          Notes:     ------     - Progress and timing information is printed to standard output.     - The function uses fiona directly to open the file and then creates a GeoDataFrame       from the features.     - If the shapefile has a defined CRS, it will be preserved; otherwise, the specified       EPSG code will be assigned.     - EPSG 32651 (the default) is appropriate for the Mt. Awu region in Indonesia.          Raises:     -------     FileNotFoundError         If the shapefile does not exist at the specified path.     fiona.errors.DriverError         If the file exists but cannot be read as a shapefile.     \"\"\"     print(f\"Reading shapefile: {os.path.basename(path)}\")     with fiona.open(path) as src:         gdf = gpd.GeoDataFrame.from_features(src)         if gdf.crs is None:             gdf.set_crs(epsg=crs_epsg, inplace=True)     return gdf In\u00a0[\u00a0]: Copied! <pre>def read_raster(path):\n    \"\"\"\n    Read a raster file and return its data and associated metadata.\n    \n    This function opens a raster file using rasterio and extracts the raster data,\n    metadata, spatial reference information, and other key properties needed for\n    geospatial analysis.\n    \n    Parameters:\n    -----------\n    path : str\n        The file path to the raster file to be read.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - data (numpy.ndarray): The raster data with shape (bands, rows, cols)\n        - meta (dict): A copy of the raster metadata\n        - transform (affine.Affine): The affine transform defining the relationship\n          between pixel coordinates and geographic coordinates\n        - nodata (float or int): The value used to represent no data or null values\n        - bounds (rasterio.coords.BoundingBox): The geographic bounds of the raster\n          (left, bottom, right, top)\n        - resolution (tuple): A tuple (x_res, y_res) containing the pixel size in\n          the x and y directions\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The function ensures the raster file is properly closed after reading by using\n      a context manager.\n    - The metadata dictionary returned includes information such as driver, CRS,\n      dimensions, data type, etc.\n    \n    Raises:\n    -------\n    FileNotFoundError\n        If the raster file does not exist at the specified path.\n    rasterio.errors.RasterioIOError\n        If the file exists but cannot be read as a raster.\n    \"\"\"\n    print(f\"Reading raster: {os.path.basename(path)}\")\n    with rasterio.open(path) as src:\n        data = src.read()\n        meta = src.meta.copy()\n        transform = src.transform\n        nodata = src.nodata\n        bounds = src.bounds\n        resolution = src.res\n    return data, meta, transform, nodata, bounds, resolution\n</pre> def read_raster(path):     \"\"\"     Read a raster file and return its data and associated metadata.          This function opens a raster file using rasterio and extracts the raster data,     metadata, spatial reference information, and other key properties needed for     geospatial analysis.          Parameters:     -----------     path : str         The file path to the raster file to be read.          Returns:     --------     tuple         A tuple containing:         - data (numpy.ndarray): The raster data with shape (bands, rows, cols)         - meta (dict): A copy of the raster metadata         - transform (affine.Affine): The affine transform defining the relationship           between pixel coordinates and geographic coordinates         - nodata (float or int): The value used to represent no data or null values         - bounds (rasterio.coords.BoundingBox): The geographic bounds of the raster           (left, bottom, right, top)         - resolution (tuple): A tuple (x_res, y_res) containing the pixel size in           the x and y directions          Notes:     ------     - Progress and timing information is printed to standard output.     - The function ensures the raster file is properly closed after reading by using       a context manager.     - The metadata dictionary returned includes information such as driver, CRS,       dimensions, data type, etc.          Raises:     -------     FileNotFoundError         If the raster file does not exist at the specified path.     rasterio.errors.RasterioIOError         If the file exists but cannot be read as a raster.     \"\"\"     print(f\"Reading raster: {os.path.basename(path)}\")     with rasterio.open(path) as src:         data = src.read()         meta = src.meta.copy()         transform = src.transform         nodata = src.nodata         bounds = src.bounds         resolution = src.res     return data, meta, transform, nodata, bounds, resolution In\u00a0[\u00a0]: Copied! <pre>def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):\n    \"\"\"\n    Save a numpy array as a raster file with specified metadata.\n    \n    This function writes a numpy array to disk as a geospatial raster file with\n    appropriate metadata, including coordinate reference system, transformation,\n    and no-data value.\n    \n    Parameters:\n    -----------\n    output_path : str\n        The file path where the raster will be saved.\n    \n    data : numpy.ndarray\n        The raster data to be saved. If 3D, the first dimension should be bands.\n        If 2D, it will be treated as a single-band raster.\n    \n    meta : dict\n        Metadata dictionary containing information such as width, height, transform,\n        and CRS. This is typically obtained from another raster.\n    \n    dtype : rasterio data type, optional\n        The data type to use for the output raster. Default is rasterio.float32.\n    \n    nodata : int or float, optional\n        The value to use for no-data or missing values in the output raster.\n        Default is -1.\n    \n    Returns:\n    --------\n    None\n        The function does not return a value, but writes the raster to disk.\n    \n    Notes:\n    ------\n    - The function updates the provided metadata with the specified dtype, band count,\n      compression method, and nodata value.\n    - LZW compression is applied to the output raster for efficient storage.\n    - The function assumes that if the input data is 2D, it represents a single band.\n    - Existing files at the output path will be overwritten without warning.\n    \n    Raises:\n    -------\n    rasterio.errors.RasterioIOError\n        If the raster cannot be written to the specified location.\n    \"\"\"\n    meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)\n    with rasterio.open(output_path, 'w', **meta) as dst:\n        dst.write(data, 1)\n</pre> def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):     \"\"\"     Save a numpy array as a raster file with specified metadata.          This function writes a numpy array to disk as a geospatial raster file with     appropriate metadata, including coordinate reference system, transformation,     and no-data value.          Parameters:     -----------     output_path : str         The file path where the raster will be saved.          data : numpy.ndarray         The raster data to be saved. If 3D, the first dimension should be bands.         If 2D, it will be treated as a single-band raster.          meta : dict         Metadata dictionary containing information such as width, height, transform,         and CRS. This is typically obtained from another raster.          dtype : rasterio data type, optional         The data type to use for the output raster. Default is rasterio.float32.          nodata : int or float, optional         The value to use for no-data or missing values in the output raster.         Default is -1.          Returns:     --------     None         The function does not return a value, but writes the raster to disk.          Notes:     ------     - The function updates the provided metadata with the specified dtype, band count,       compression method, and nodata value.     - LZW compression is applied to the output raster for efficient storage.     - The function assumes that if the input data is 2D, it represents a single band.     - Existing files at the output path will be overwritten without warning.          Raises:     -------     rasterio.errors.RasterioIOError         If the raster cannot be written to the specified location.     \"\"\"     meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)     with rasterio.open(output_path, 'w', **meta) as dst:         dst.write(data, 1) In\u00a0[\u00a0]: Copied! <pre>def load_raster(file_path):\n    \"\"\"\n    Load a single-band raster file and return its data and metadata.\n    \n    This function opens a raster file using rasterio and extracts the first band\n    of data along with its metadata. Unlike the read_raster function which reads\n    all bands, this function is optimized for single-band rasters.\n    \n    Parameters:\n    -----------\n    file_path : str\n        The file path to the raster file to be loaded.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - data (numpy.ndarray): The raster data from the first band with shape (rows, cols)\n        - meta (dict): A copy of the raster metadata\n    \n    Notes:\n    ------\n    - The function reads only the first band (band index 1) of the raster.\n    - The metadata dictionary returned includes information such as driver, CRS,\n      dimensions, data type, transform, etc.\n    - The function ensures the raster file is properly closed after reading by using\n      a context manager.\n    \n    Raises:\n    -------\n    FileNotFoundError\n        If the raster file does not exist at the specified path.\n    rasterio.errors.RasterioIOError\n        If the file exists but cannot be read as a raster.\n    \"\"\"\n    with rasterio.open(file_path) as src:\n        data = src.read(1)\n        meta = src.meta.copy()\n    return data, meta\n</pre> def load_raster(file_path):     \"\"\"     Load a single-band raster file and return its data and metadata.          This function opens a raster file using rasterio and extracts the first band     of data along with its metadata. Unlike the read_raster function which reads     all bands, this function is optimized for single-band rasters.          Parameters:     -----------     file_path : str         The file path to the raster file to be loaded.          Returns:     --------     tuple         A tuple containing:         - data (numpy.ndarray): The raster data from the first band with shape (rows, cols)         - meta (dict): A copy of the raster metadata          Notes:     ------     - The function reads only the first band (band index 1) of the raster.     - The metadata dictionary returned includes information such as driver, CRS,       dimensions, data type, transform, etc.     - The function ensures the raster file is properly closed after reading by using       a context manager.          Raises:     -------     FileNotFoundError         If the raster file does not exist at the specified path.     rasterio.errors.RasterioIOError         If the file exists but cannot be read as a raster.     \"\"\"     with rasterio.open(file_path) as src:         data = src.read(1)         meta = src.meta.copy()     return data, meta In\u00a0[\u00a0]: Copied! <pre>def save_analysis_report(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):\n    \"\"\"\n    Save evacuation analysis results to a formatted text report file.\n    \n    This function generates a human-readable text report summarizing the minimum \n    travel times from various sources to safe zones for different walking speeds.\n    \n    Parameters:\n    -----------\n    output_path : str\n        The file path where the report will be saved.\n    \n    results : dict\n        A nested dictionary containing travel time results:\n        {speed_name: {safe_zone_distance: [times_per_source]}}\n        where times_per_source is a list of travel times (in hours) for each source.\n    \n    min_coords : dict\n        A nested dictionary containing the coordinates of minimum travel times:\n        {speed_name: {safe_zone_distance: [coords_per_source]}}\n        where coords_per_source is a list of (row, col) tuples for each source.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n    \n    walking_speeds : dict\n        Dictionary mapping speed names (str) to speed values (float) in meters per second.\n    \n    safe_zone_distances : list\n        List of safe zone distances used in the analysis.\n    \n    Returns:\n    --------\n    None\n        The function does not return a value, but writes results to a text file.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The report is organized hierarchically by walking speed, safe zone distance, \n      and source location.\n    - Travel times are formatted to 2 decimal places.\n    \"\"\"\n    with open(output_path, 'w') as f:\n        f.write(\"Safe Zone Travel Time Analysis Report\\n\")\n        f.write(\"======================================\\n\\n\")\n        for speed_name in walking_speeds.keys():\n            f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")\n            f.write(\"-\" * 40 + \"\\n\")\n            for safe_zone in safe_zone_distances:\n                f.write(f\"\\nSafe Zone: {safe_zone} m\\n\")\n                for idx, source_name in enumerate(source_names):\n                    tt = results[speed_name][safe_zone][idx]\n                    coords = min_coords[speed_name][safe_zone][idx]\n                    f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\")\n</pre> def save_analysis_report(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):     \"\"\"     Save evacuation analysis results to a formatted text report file.          This function generates a human-readable text report summarizing the minimum      travel times from various sources to safe zones for different walking speeds.          Parameters:     -----------     output_path : str         The file path where the report will be saved.          results : dict         A nested dictionary containing travel time results:         {speed_name: {safe_zone_distance: [times_per_source]}}         where times_per_source is a list of travel times (in hours) for each source.          min_coords : dict         A nested dictionary containing the coordinates of minimum travel times:         {speed_name: {safe_zone_distance: [coords_per_source]}}         where coords_per_source is a list of (row, col) tuples for each source.          source_names : list         A list of source location names corresponding to indices in the results arrays.          walking_speeds : dict         Dictionary mapping speed names (str) to speed values (float) in meters per second.          safe_zone_distances : list         List of safe zone distances used in the analysis.          Returns:     --------     None         The function does not return a value, but writes results to a text file.          Notes:     ------     - Progress and timing information is printed to standard output.     - The report is organized hierarchically by walking speed, safe zone distance,        and source location.     - Travel times are formatted to 2 decimal places.     \"\"\"     with open(output_path, 'w') as f:         f.write(\"Safe Zone Travel Time Analysis Report\\n\")         f.write(\"======================================\\n\\n\")         for speed_name in walking_speeds.keys():             f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")             f.write(\"-\" * 40 + \"\\n\")             for safe_zone in safe_zone_distances:                 f.write(f\"\\nSafe Zone: {safe_zone} m\\n\")                 for idx, source_name in enumerate(source_names):                     tt = results[speed_name][safe_zone][idx]                     coords = min_coords[speed_name][safe_zone][idx]                     f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\") In\u00a0[\u00a0]: Copied! <pre>def save_metrics_csv(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):\n    \"\"\"\n    Save evacuation analysis metrics to a CSV file for further processing.\n    \n    This function exports the analysis results in a tabular format suitable for\n    import into spreadsheet software or data analysis tools. Each row represents\n    a unique combination of walking speed, safe zone distance, and source location.\n    \n    Parameters:\n    -----------\n    output_path : str\n        The file path where the CSV will be saved.\n    \n    results : dict\n        A nested dictionary containing travel time results:\n        {speed_name: {safe_zone_distance: [times_per_source]}}\n        where times_per_source is a list of travel times (in hours) for each source.\n    \n    min_coords : dict\n        A nested dictionary containing the coordinates of minimum travel times:\n        {speed_name: {safe_zone_distance: [coords_per_source]}}\n        where coords_per_source is a list of (row, col) tuples for each source.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n    \n    walking_speeds : dict\n        Dictionary mapping speed names (str) to speed values (float) in meters per second.\n    \n    safe_zone_distances : list\n        List of safe zone distances used in the analysis.\n    \n    Returns:\n    --------\n    None\n        The function does not return a value, but writes results to a CSV file.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The CSV includes columns for Walking_Speed, Safe_Zone, Source, \n      Min_Travel_Time (hrs), and Min_Coords (row,col).\n    - All combinations of walking speed, safe zone, and source are included.\n    \"\"\"\n    with open(output_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Walking_Speed', 'Safe_Zone', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])\n        for speed_name in walking_speeds.keys():\n            for safe_zone in safe_zone_distances:\n                for idx, source_name in enumerate(source_names):\n                    writer.writerow([\n                        speed_name,\n                        safe_zone,\n                        source_name,\n                        results[speed_name][safe_zone][idx],\n                        min_coords[speed_name][safe_zone][idx]\n                    ])\n</pre> def save_metrics_csv(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):     \"\"\"     Save evacuation analysis metrics to a CSV file for further processing.          This function exports the analysis results in a tabular format suitable for     import into spreadsheet software or data analysis tools. Each row represents     a unique combination of walking speed, safe zone distance, and source location.          Parameters:     -----------     output_path : str         The file path where the CSV will be saved.          results : dict         A nested dictionary containing travel time results:         {speed_name: {safe_zone_distance: [times_per_source]}}         where times_per_source is a list of travel times (in hours) for each source.          min_coords : dict         A nested dictionary containing the coordinates of minimum travel times:         {speed_name: {safe_zone_distance: [coords_per_source]}}         where coords_per_source is a list of (row, col) tuples for each source.          source_names : list         A list of source location names corresponding to indices in the results arrays.          walking_speeds : dict         Dictionary mapping speed names (str) to speed values (float) in meters per second.          safe_zone_distances : list         List of safe zone distances used in the analysis.          Returns:     --------     None         The function does not return a value, but writes results to a CSV file.          Notes:     ------     - Progress and timing information is printed to standard output.     - The CSV includes columns for Walking_Speed, Safe_Zone, Source,        Min_Travel_Time (hrs), and Min_Coords (row,col).     - All combinations of walking speed, safe zone, and source are included.     \"\"\"     with open(output_path, 'w', newline='') as f:         writer = csv.writer(f)         writer.writerow(['Walking_Speed', 'Safe_Zone', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])         for speed_name in walking_speeds.keys():             for safe_zone in safe_zone_distances:                 for idx, source_name in enumerate(source_names):                     writer.writerow([                         speed_name,                         safe_zone,                         source_name,                         results[speed_name][safe_zone][idx],                         min_coords[speed_name][safe_zone][idx]                     ])"},{"location":"api/evacuation-analysis/New%20folder/path-utils.html","title":"Path utils","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport time\nfrom numpy import sqrt\nfrom scipy.sparse import lil_matrix\nfrom grid_utils import to_1d\nfrom tqdm import tqdm\n</pre> import numpy as np import time from numpy import sqrt from scipy.sparse import lil_matrix from grid_utils import to_1d from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):\n    \"\"\"\n    Calculate metrics for the shortest path between source and target nodes.\n    \n    This function reconstructs the shortest path using the predecessor array and\n    computes various metrics including the number of pixels in the path, the cost\n    of each step, and the total path cost.\n    \n    Parameters:\n    -----------\n    predecessors : numpy.ndarray\n        A 1D array where predecessors[i] contains the predecessor node of node i\n        on the shortest path from the source.\n    \n    source_node : int\n        The source node index (starting point) of the path.\n    \n    target_node : int or float\n        The target node index (destination) of the path. Can be -9999 or NaN to\n        indicate an invalid target.\n    \n    graph_csr : scipy.sparse.csr_matrix\n        The graph in CSR format where each entry (i,j) represents the cost/weight\n        of the edge from node i to node j.\n    \n    rows : int\n        The number of rows in the original raster grid.\n    \n    cols : int\n        The number of columns in the original raster grid.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - pixel_count (int): The number of pixels in the path.\n        - cell_costs (list): A list of the costs for each step in the path.\n        - total_cost (float): The sum of all cell costs along the path.\n    \n    Notes:\n    ------\n    - If target_node is -9999 or NaN, the function returns (0, [], 0) indicating\n      no valid path.\n    - The path is reconstructed from the target to the source and then reversed.\n    - A value of -9999 in the predecessors array indicates \"no predecessor\".\n    - Progress and timing information is printed to standard output.\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return 0, [], 0\n    \n    path = []\n    current = target_node\n    while current != source_node and current != -9999:\n        path.append(current)\n        current = predecessors[current]\n    if current != -9999:\n        path.append(source_node)\n    path.reverse()\n    \n    pixel_count = len(path)\n    cell_costs = []\n    total_cost = 0\n    for i in range(len(path)-1):\n        cell_cost = graph_csr[path[i], path[i+1]]\n        cell_costs.append(cell_cost)\n        total_cost += cell_cost\n    \n    return pixel_count, cell_costs, total_cost\n</pre> def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):     \"\"\"     Calculate metrics for the shortest path between source and target nodes.          This function reconstructs the shortest path using the predecessor array and     computes various metrics including the number of pixels in the path, the cost     of each step, and the total path cost.          Parameters:     -----------     predecessors : numpy.ndarray         A 1D array where predecessors[i] contains the predecessor node of node i         on the shortest path from the source.          source_node : int         The source node index (starting point) of the path.          target_node : int or float         The target node index (destination) of the path. Can be -9999 or NaN to         indicate an invalid target.          graph_csr : scipy.sparse.csr_matrix         The graph in CSR format where each entry (i,j) represents the cost/weight         of the edge from node i to node j.          rows : int         The number of rows in the original raster grid.          cols : int         The number of columns in the original raster grid.          Returns:     --------     tuple         A tuple containing:         - pixel_count (int): The number of pixels in the path.         - cell_costs (list): A list of the costs for each step in the path.         - total_cost (float): The sum of all cell costs along the path.          Notes:     ------     - If target_node is -9999 or NaN, the function returns (0, [], 0) indicating       no valid path.     - The path is reconstructed from the target to the source and then reversed.     - A value of -9999 in the predecessors array indicates \"no predecessor\".     - Progress and timing information is printed to standard output.     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return 0, [], 0          path = []     current = target_node     while current != source_node and current != -9999:         path.append(current)         current = predecessors[current]     if current != -9999:         path.append(source_node)     path.reverse()          pixel_count = len(path)     cell_costs = []     total_cost = 0     for i in range(len(path)-1):         cell_cost = graph_csr[path[i], path[i+1]]         cell_costs.append(cell_cost)         total_cost += cell_cost          return pixel_count, cell_costs, total_cost In\u00a0[\u00a0]: Copied! <pre>def reconstruct_path(pred, source_node, target_node, cols):\n    \"\"\"\n    Reconstruct the shortest path from source to target and convert to 2D coordinates.\n    \n    This function traces back through a predecessor array to reconstruct the path\n    from source to target, then converts the 1D node indices to 2D grid coordinates.\n    \n    Parameters:\n    -----------\n    pred : numpy.ndarray\n        A 1D array where pred[i] contains the predecessor node of node i\n        on the shortest path from the source.\n    \n    source_node : int\n        The source node index (starting point) of the path.\n    \n    target_node : int or float\n        The target node index (destination) of the path. Can be -9999 or NaN to\n        indicate an invalid target.\n    \n    cols : int\n        The number of columns in the grid, used to convert 1D indices to 2D coordinates.\n    \n    Returns:\n    --------\n    list\n        A list of tuples (row, col) representing the 2D coordinates of each point\n        along the path from source to target. Returns an empty list if no valid path exists.\n    \n    Notes:\n    ------\n    - If target_node is -9999 or NaN, the function returns an empty list.\n    - The path is reconstructed from target to source and then reversed.\n    - A value of -9999 in the predecessor array indicates \"no predecessor\".\n    - 1D indices are converted to 2D coordinates using integer division and modulo operations.\n    - Progress and timing information is printed to standard output.\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return []\n        \n    path_nodes = []\n    current = target_node\n    while current != source_node and current != -9999:\n        path_nodes.append(current)\n        current = pred[current]\n    path_nodes.append(source_node)\n    path_nodes.reverse()\n    \n    # Convert each 1D index to (row, col)\n    path_coords = [(node // cols, node % cols) for node in path_nodes]\n    \n    return path_coords\n</pre> def reconstruct_path(pred, source_node, target_node, cols):     \"\"\"     Reconstruct the shortest path from source to target and convert to 2D coordinates.          This function traces back through a predecessor array to reconstruct the path     from source to target, then converts the 1D node indices to 2D grid coordinates.          Parameters:     -----------     pred : numpy.ndarray         A 1D array where pred[i] contains the predecessor node of node i         on the shortest path from the source.          source_node : int         The source node index (starting point) of the path.          target_node : int or float         The target node index (destination) of the path. Can be -9999 or NaN to         indicate an invalid target.          cols : int         The number of columns in the grid, used to convert 1D indices to 2D coordinates.          Returns:     --------     list         A list of tuples (row, col) representing the 2D coordinates of each point         along the path from source to target. Returns an empty list if no valid path exists.          Notes:     ------     - If target_node is -9999 or NaN, the function returns an empty list.     - The path is reconstructed from target to source and then reversed.     - A value of -9999 in the predecessor array indicates \"no predecessor\".     - 1D indices are converted to 2D coordinates using integer division and modulo operations.     - Progress and timing information is printed to standard output.     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return []              path_nodes = []     current = target_node     while current != source_node and current != -9999:         path_nodes.append(current)         current = pred[current]     path_nodes.append(source_node)     path_nodes.reverse()          # Convert each 1D index to (row, col)     path_coords = [(node // cols, node % cols) for node in path_nodes]          return path_coords In\u00a0[\u00a0]: Copied! <pre>def build_adjacency_matrix(cost_array, rows, cols, directions):\n    \"\"\"\n    Build a sparse adjacency matrix representing the graph for path finding.\n    \n    This function creates a graph representation of the raster grid where nodes\n    are grid cells and edges represent possible movements between adjacent cells.\n    Edge weights are derived from the cost array, with diagonal movements adjusted\n    by a factor of sqrt(2) to account for the increased distance.\n    \n    Parameters:\n    -----------\n    cost_array : numpy.ndarray\n        A 3D array of shape (n_directions, rows, cols) containing the cost values\n        for moving in each direction from each cell. The first dimension corresponds\n        to the directions defined in the directions parameter.\n    \n    rows : int\n        The number of rows in the grid.\n    \n    cols : int\n        The number of columns in the grid.\n    \n    directions : list of tuples\n        A list of (dr, dc) tuples defining the possible movement directions.\n        Typically includes the 8 neighboring directions (cardinal and diagonal).\n    \n    Returns:\n    --------\n    scipy.sparse.csr_matrix\n        A sparse CSR matrix representation of the graph, where each entry (i, j)\n        represents the cost of moving from node i to node j. The matrix has\n        dimensions (rows*cols, rows*cols).\n    \n    Notes:\n    ------\n    - Progress information is displayed using tqdm and timing information is printed.\n    - Invalid costs (NaN, infinite, negative, or zero) are skipped.\n    - Diagonal movements have their costs multiplied by sqrt(2) to account for the\n      increased distance.\n    - The function first builds the matrix in LIL format for efficient construction,\n      then converts to CSR format for efficient operations.\n    - Nodes are indexed in row-major order using the to_1d function.\n    \"\"\"\n    print(\"\\nBuilding adjacency matrix...\")\n    start_time = time.time()\n    \n    graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)\n    direction_indices = {dir: idx for idx, dir in enumerate(directions)}\n    \n    # Use tqdm for progress bar\n    for r in tqdm(range(rows), desc=\"Processing rows\"):\n        for c in range(cols):\n            current_node = to_1d(r, c, cols)\n            for dr, dc in directions:\n                nr, nc = r + dr, c + dc\n                if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:\n                    neighbor_node = to_1d(nr, nc, cols)\n                    idx = direction_indices[(dr, dc)]\n                    cost_val = cost_array[idx, r, c]\n                    if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:\n                        continue\n                    if abs(dr) == 1 and abs(dc) == 1:\n                        cost_val *= sqrt(2)\n                    graph[current_node, neighbor_node] = cost_val\n    \n    graph_csr = graph.tocsr()  # Convert to CSR format for efficient operations\n    end_time = time.time()\n    print(f\"Adjacency matrix built in {end_time - start_time:.2f} seconds\")\n    print(f\"Matrix shape: {graph_csr.shape}\")\n    print(f\"Number of non-zero elements: {graph_csr.nnz}\")\n    return graph_csr\n</pre> def build_adjacency_matrix(cost_array, rows, cols, directions):     \"\"\"     Build a sparse adjacency matrix representing the graph for path finding.          This function creates a graph representation of the raster grid where nodes     are grid cells and edges represent possible movements between adjacent cells.     Edge weights are derived from the cost array, with diagonal movements adjusted     by a factor of sqrt(2) to account for the increased distance.          Parameters:     -----------     cost_array : numpy.ndarray         A 3D array of shape (n_directions, rows, cols) containing the cost values         for moving in each direction from each cell. The first dimension corresponds         to the directions defined in the directions parameter.          rows : int         The number of rows in the grid.          cols : int         The number of columns in the grid.          directions : list of tuples         A list of (dr, dc) tuples defining the possible movement directions.         Typically includes the 8 neighboring directions (cardinal and diagonal).          Returns:     --------     scipy.sparse.csr_matrix         A sparse CSR matrix representation of the graph, where each entry (i, j)         represents the cost of moving from node i to node j. The matrix has         dimensions (rows*cols, rows*cols).          Notes:     ------     - Progress information is displayed using tqdm and timing information is printed.     - Invalid costs (NaN, infinite, negative, or zero) are skipped.     - Diagonal movements have their costs multiplied by sqrt(2) to account for the       increased distance.     - The function first builds the matrix in LIL format for efficient construction,       then converts to CSR format for efficient operations.     - Nodes are indexed in row-major order using the to_1d function.     \"\"\"     print(\"\\nBuilding adjacency matrix...\")     start_time = time.time()          graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)     direction_indices = {dir: idx for idx, dir in enumerate(directions)}          # Use tqdm for progress bar     for r in tqdm(range(rows), desc=\"Processing rows\"):         for c in range(cols):             current_node = to_1d(r, c, cols)             for dr, dc in directions:                 nr, nc = r + dr, c + dc                 if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:                     neighbor_node = to_1d(nr, nc, cols)                     idx = direction_indices[(dr, dc)]                     cost_val = cost_array[idx, r, c]                     if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:                         continue                     if abs(dr) == 1 and abs(dc) == 1:                         cost_val *= sqrt(2)                     graph[current_node, neighbor_node] = cost_val          graph_csr = graph.tocsr()  # Convert to CSR format for efficient operations     end_time = time.time()     print(f\"Adjacency matrix built in {end_time - start_time:.2f} seconds\")     print(f\"Matrix shape: {graph_csr.shape}\")     print(f\"Number of non-zero elements: {graph_csr.nnz}\")     return graph_csr"},{"location":"api/evacuation-analysis/New%20folder/visualization.html","title":"Visualization","text":"In\u00a0[\u00a0]: Copied! <pre>import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom matplotlib.patheffects import withStroke\nimport os\nimport rasterio\nimport pandas as pd\n</pre> import time import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as mcolors from matplotlib.patheffects import withStroke import os import rasterio import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def load_raster(path):\n    \"\"\"\n    Load a single-band raster file and return its data and metadata.\n    \n    This helper function opens a raster file using rasterio and extracts\n    the first band of data along with essential metadata for visualization.\n    \n    Parameters:\n    -----------\n    path : str\n        The file path to the raster file to be loaded.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - array (numpy.ndarray): The raster data from the first band with shape (rows, cols)\n        - metadata (dict): A dictionary containing:\n            - 'transform' (affine.Affine): The affine transform\n            - 'crs' (CRS): The coordinate reference system\n            - 'nodata' (float or int): The no-data value\n    \n    Notes:\n    ------\n    - This function is streamlined for visualization purposes, reading only the\n      first band and a subset of metadata.\n    - The raster file is properly closed after reading using a context manager.\n    \"\"\"\n    with rasterio.open(path) as src:\n        array = src.read(1)  # Read the first band\n        metadata = {\n            'transform': src.transform,\n            'crs': src.crs,\n            'nodata': src.nodata\n        }\n    return array, metadata\n</pre> def load_raster(path):     \"\"\"     Load a single-band raster file and return its data and metadata.          This helper function opens a raster file using rasterio and extracts     the first band of data along with essential metadata for visualization.          Parameters:     -----------     path : str         The file path to the raster file to be loaded.          Returns:     --------     tuple         A tuple containing:         - array (numpy.ndarray): The raster data from the first band with shape (rows, cols)         - metadata (dict): A dictionary containing:             - 'transform' (affine.Affine): The affine transform             - 'crs' (CRS): The coordinate reference system             - 'nodata' (float or int): The no-data value          Notes:     ------     - This function is streamlined for visualization purposes, reading only the       first band and a subset of metadata.     - The raster file is properly closed after reading using a context manager.     \"\"\"     with rasterio.open(path) as src:         array = src.read(1)  # Read the first band         metadata = {             'transform': src.transform,             'crs': src.crs,             'nodata': src.nodata         }     return array, metadata In\u00a0[\u00a0]: Copied! <pre>def raster_coord_to_map_coords(row, col, transform):\n    \"\"\"\n    Convert raster coordinates (row, column) to map coordinates (x, y).\n    \n    This function transforms pixel coordinates in a raster to their corresponding\n    geographic coordinates using the raster's affine transform.\n    \n    Parameters:\n    -----------\n    row : int\n        The row index in the raster (zero-based).\n    \n    col : int\n        The column index in the raster (zero-based).\n    \n    transform : affine.Affine or list/tuple\n        The affine transform of the raster, defining the relationship between\n        pixel coordinates and geographic coordinates. Can be provided as an\n        Affine object or as a 6-element tuple/list in the form \n        [a, b, c, d, e, f] where:\n        - a: width of a pixel\n        - b: row rotation (typically 0)\n        - c: x-coordinate of the upper-left corner\n        - d: column rotation (typically 0)\n        - e: height of a pixel (typically negative)\n        - f: y-coordinate of the upper-left corner\n    \n    Returns:\n    --------\n    tuple\n        A tuple (x, y) containing the geographic coordinates corresponding to\n        the specified raster cell.\n    \n    Notes:\n    ------\n    - The function calculates coordinates based on the transform components\n      rather than using the rasterio.transform.xy function, enabling use with\n      transform arrays as well as Affine objects.\n    \"\"\"\n    x = transform[2] + col * transform[0]\n    y = transform[5] + row * transform[4]\n    return x, y\n</pre> def raster_coord_to_map_coords(row, col, transform):     \"\"\"     Convert raster coordinates (row, column) to map coordinates (x, y).          This function transforms pixel coordinates in a raster to their corresponding     geographic coordinates using the raster's affine transform.          Parameters:     -----------     row : int         The row index in the raster (zero-based).          col : int         The column index in the raster (zero-based).          transform : affine.Affine or list/tuple         The affine transform of the raster, defining the relationship between         pixel coordinates and geographic coordinates. Can be provided as an         Affine object or as a 6-element tuple/list in the form          [a, b, c, d, e, f] where:         - a: width of a pixel         - b: row rotation (typically 0)         - c: x-coordinate of the upper-left corner         - d: column rotation (typically 0)         - e: height of a pixel (typically negative)         - f: y-coordinate of the upper-left corner          Returns:     --------     tuple         A tuple (x, y) containing the geographic coordinates corresponding to         the specified raster cell.          Notes:     ------     - The function calculates coordinates based on the transform components       rather than using the rasterio.transform.xy function, enabling use with       transform arrays as well as Affine objects.     \"\"\"     x = transform[2] + col * transform[0]     y = transform[5] + row * transform[4]     return x, y In\u00a0[\u00a0]: Copied! <pre>def plot_travel_time_comparison(all_results, safe_zone_distances, source_names, speed_colors):\n    \"\"\"\n    Create a comparison plot of minimum travel times for different evacuation scenarios.\n    \n    This function generates a figure with three subplots showing travel time curves\n    for summit, camp1, and camp2 sources. Each subplot compares travel times for\n    different walking speeds (slow, medium, fast) and terrain scenarios (original \n    and penalized landcover).\n    \n    Parameters:\n    -----------\n    all_results : dict\n        A nested dictionary containing travel time results for different datasets:\n        {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}\n        where times_per_source is a list of travel times (in hours) for each source.\n        Must include 'final' and 'modify_landcover' datasets.\n    \n    safe_zone_distances : list\n        List of safe zone distances used in the analysis.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n        Must include 'summit', 'camp1', and 'camp2'.\n    \n    speed_colors : dict\n        Dictionary mapping speed names (str) to color values for plotting.\n    \n    Returns:\n    --------\n    matplotlib.figure.Figure\n        The figure object containing the comparison plots, designed for A4 landscape format.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - Plots use solid lines for original landcover and dashed lines for penalized landcover.\n    - The figure includes a shared legend in the rightmost subplot.\n    - Each subplot is labeled with a letter (A, B, C) in the top-left corner.\n    \"\"\"\n    print(\"\\nCreating travel time comparison plot...\")\n    start_time = time.time()\n    \n    # Define specific sources to plot\n    selected_sources = ['summit', 'camp1']\n    n_sources = len(selected_sources)\n    \n    # Create figure in A4 landscape format (11.69 x 8.27 inches)\n    fig, axes = plt.subplots(1, 2, figsize=(11.69, 4), sharey=True)\n    \n    # Store lines and labels for the legend\n    lines = []\n    labels = []\n    first_plot = True\n    \n    # For each source\n    for src_idx, source_name in enumerate(selected_sources):\n        ax = axes[src_idx]\n        \n        # Plot all walking speeds for both original and penalized\n        for speed_name, speed_color in speed_colors.items():\n            # Get data for original and penalized\n            original = [\n                all_results['final'][speed_name][sz][source_names.index(source_name)]\n                for sz in safe_zone_distances\n            ]\n            penalized = [\n                all_results['modify_landcover'][speed_name][sz][source_names.index(source_name)]\n                for sz in safe_zone_distances\n            ]\n            \n            # Plot with consistent line styles\n            line1 = ax.plot(safe_zone_distances, original, '-', color=speed_color)\n            line2 = ax.plot(safe_zone_distances, penalized, '--', color=speed_color)\n            \n            # Only store legend entries once\n            if first_plot:\n                lines.extend([line1[0], line2[0]])\n                labels.extend([\n                    f'{speed_name.capitalize()} - Original', \n                    f'{speed_name.capitalize()} - Penalized'\n                ])\n        \n        # Add letter label in top left corner\n        ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes, \n                fontsize=12, fontweight='bold', va='top')\n        \n        ax.set_xlabel(\"Safe Zone Radius (m)\", fontsize=10)\n        if src_idx == 0:  # Add ylabel only for leftmost plot\n            ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)\n        ax.grid(True, alpha=0.3)\n        \n        # Add legend to the right side of the third subplot\n        if src_idx == 2:  # For the last subplot\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n            ax.legend(lines, labels, fontsize=9, \n                      loc='center left', \n                      bbox_to_anchor=(1.05, 0.5))\n        \n        first_plot = False\n    \n    plt.tight_layout()\n    \n    # Comment out plt.show() to avoid opening a new window:\n    plt.show()\n    \n    end_time = time.time()\n    print(f\"Travel time comparison plot created in {end_time - start_time:.2f} seconds\")\n    return fig\n</pre> def plot_travel_time_comparison(all_results, safe_zone_distances, source_names, speed_colors):     \"\"\"     Create a comparison plot of minimum travel times for different evacuation scenarios.          This function generates a figure with three subplots showing travel time curves     for summit, camp1, and camp2 sources. Each subplot compares travel times for     different walking speeds (slow, medium, fast) and terrain scenarios (original      and penalized landcover).          Parameters:     -----------     all_results : dict         A nested dictionary containing travel time results for different datasets:         {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}         where times_per_source is a list of travel times (in hours) for each source.         Must include 'final' and 'modify_landcover' datasets.          safe_zone_distances : list         List of safe zone distances used in the analysis.          source_names : list         A list of source location names corresponding to indices in the results arrays.         Must include 'summit', 'camp1', and 'camp2'.          speed_colors : dict         Dictionary mapping speed names (str) to color values for plotting.          Returns:     --------     matplotlib.figure.Figure         The figure object containing the comparison plots, designed for A4 landscape format.          Notes:     ------     - Progress and timing information is printed to standard output.     - Plots use solid lines for original landcover and dashed lines for penalized landcover.     - The figure includes a shared legend in the rightmost subplot.     - Each subplot is labeled with a letter (A, B, C) in the top-left corner.     \"\"\"     print(\"\\nCreating travel time comparison plot...\")     start_time = time.time()          # Define specific sources to plot     selected_sources = ['summit', 'camp1']     n_sources = len(selected_sources)          # Create figure in A4 landscape format (11.69 x 8.27 inches)     fig, axes = plt.subplots(1, 2, figsize=(11.69, 4), sharey=True)          # Store lines and labels for the legend     lines = []     labels = []     first_plot = True          # For each source     for src_idx, source_name in enumerate(selected_sources):         ax = axes[src_idx]                  # Plot all walking speeds for both original and penalized         for speed_name, speed_color in speed_colors.items():             # Get data for original and penalized             original = [                 all_results['final'][speed_name][sz][source_names.index(source_name)]                 for sz in safe_zone_distances             ]             penalized = [                 all_results['modify_landcover'][speed_name][sz][source_names.index(source_name)]                 for sz in safe_zone_distances             ]                          # Plot with consistent line styles             line1 = ax.plot(safe_zone_distances, original, '-', color=speed_color)             line2 = ax.plot(safe_zone_distances, penalized, '--', color=speed_color)                          # Only store legend entries once             if first_plot:                 lines.extend([line1[0], line2[0]])                 labels.extend([                     f'{speed_name.capitalize()} - Original',                      f'{speed_name.capitalize()} - Penalized'                 ])                  # Add letter label in top left corner         ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes,                  fontsize=12, fontweight='bold', va='top')                  ax.set_xlabel(\"Safe Zone Radius (m)\", fontsize=10)         if src_idx == 0:  # Add ylabel only for leftmost plot             ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)         ax.grid(True, alpha=0.3)                  # Add legend to the right side of the third subplot         if src_idx == 2:  # For the last subplot             box = ax.get_position()             ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])             ax.legend(lines, labels, fontsize=9,                        loc='center left',                        bbox_to_anchor=(1.05, 0.5))                  first_plot = False          plt.tight_layout()          # Comment out plt.show() to avoid opening a new window:     plt.show()          end_time = time.time()     print(f\"Travel time comparison plot created in {end_time - start_time:.2f} seconds\")     return fig In\u00a0[\u00a0]: Copied! <pre>def create_cost_surface_subplots(dataset_info, cost_arrays, transforms, evacuation_paths, \n                               summit_coords, safe_zone_distances, hiking_gdf, output_path):\n    \"\"\"\n    Create visualizations of cost surfaces with evacuation paths for two terrain scenarios.\n    \n    This function generates a figure with two cost surface maps showing optimal evacuation\n    paths from the summit to different safe zone distances. The maps include distance contours,\n    hiking paths, and the summit location. Cost surfaces use a logarithmic color scale to\n    better visualize travel time differences.\n    \n    Parameters:\n    -----------\n    dataset_info : dict\n        Dictionary containing information about each dataset, including:\n        - keys: Dataset names (e.g., 'final', 'modify_landcover')\n        - values: Dictionaries with dataset metadata\n    \n    cost_arrays : list\n        List of numpy arrays containing the cost surfaces for each dataset.\n    \n    transforms : list\n        List of affine transforms corresponding to each cost array.\n    \n    evacuation_paths : dict\n        Dictionary of evacuation paths for each dataset and safe zone:\n        {dataset_key: {safe_zone_distance: [path_coordinates]}}\n        where path_coordinates is a list of (row, col) tuples.\n    \n    summit_coords : dict\n        Dictionary mapping dataset keys to summit coordinates as (row, col) tuples.\n    \n    safe_zone_distances : list\n        List of safe zone distances to plot paths for.\n    \n    hiking_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing hiking trail geometries to overlay on the maps.\n    \n    output_path : str\n        File path where the figure will be saved.\n    \n    Returns:\n    --------\n    None\n        The function saves the figure to the specified output path but does not return a value.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - A logarithmic transformation is applied to the cost surfaces to enhance visualization.\n    - The figure includes a color bar indicating the travel time.\n    - Evacuation paths are color-coded by safe zone distance.\n    - Contour lines show distance from the summit.\n    - Each subplot is labeled with a letter (A, B) in the top-left corner.\n    \"\"\"\n    print(\"\\nCreating cost surface subplots...\")\n    start_time = time.time()\n    \n    # Create figure with larger size \n    fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4\n    \n    # Create subplot grid with space for legend and colorbar\n    gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    \n    # Create a common colormap and normalization\n    cmap = plt.cm.RdYlBu_r.copy()\n    cmap.set_bad('white', alpha=0)\n    \n    epsilon = 0.1\n    max_log_cost = np.log1p(10 + epsilon)\n    \n    # Find global vmin and vmax for consistent color scaling\n    vmin = float('inf')\n    vmax = float('-inf')\n    for cost_array in cost_arrays:\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))\n        vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))\n    vcenter = vmin + (vmax - vmin) / 2\n    \n    norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n    \n    # Colors for paths\n    colors_paths = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange', 'red', 'blue', 'purple']\n    \n    # Plot both subplots\n    for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):\n        cost_array = cost_arrays[idx]\n        transform = transforms[idx]\n        \n        # Process cost array\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        \n        # Plot cost surface\n        im = ax.imshow(\n            cost_array_masked, cmap=cmap, norm=norm,\n            extent=[\n                transform[2], \n                transform[2] + transform[0] * cost_array.shape[1],\n                transform[5] + transform[4] * cost_array.shape[0],\n                transform[5]\n            ]\n        )\n        \n        # Plot evacuation paths\n        for i, distance in enumerate(safe_zone_distances):\n            if distance in evacuation_paths[ds_key]:\n                path = evacuation_paths[ds_key][distance]\n                if path:\n                    path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]\n                    xs, ys = zip(*path_coords)\n                    ax.plot(xs, ys, '-', color=colors_paths[i], \n                            linewidth=3, label=f'{distance}m safe zone',\n                            alpha=1.0, zorder=5)\n        \n        # Plot hiking trail with outline effect\n        hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)\n        ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')\n        plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])\n        \n        # Plot summit\n        summit_x, summit_y = raster_coord_to_map_coords(\n            summit_coords[ds_key][0], summit_coords[ds_key][1], transform\n        )\n        ax.plot(summit_x, summit_y, '*', color='yellow', markersize=15, label='Summit', zorder=6)\n        \n        # Add distance contours\n        x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])\n        y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])\n        X, Y = np.meshgrid(x, y)\n        distances = np.sqrt((X - summit_x)**2 + (Y - summit_y)**2)\n        \n        contours = ax.contour(X, Y, distances, levels=safe_zone_distances,\n                              colors='black', linestyles='--', alpha=0.4,\n                              linewidths=1.5, zorder=3)\n        ax.clabel(contours, inline=True, fmt='%1.0fm', fontsize=8)\n        \n        # Set plot limits\n        ax.set_xlim(summit_x - 5000, summit_x + 5000)\n        ax.set_ylim(summit_y - 5000, summit_y + 5000)\n        \n        # Add simple A/B labels\n        ax.text(0.02, 0.98, f'{chr(65+idx)}',\n                transform=ax.transAxes, fontsize=10, fontweight='bold',\n                verticalalignment='top')\n        \n        # Add axis labels\n        ax.set_xlabel('Easting (m)')\n        ax.set_ylabel('Northing (m)')\n    \n    # Add legend\n    handles, labels = ax1.get_legend_handles_labels()\n    unique_labels = []\n    unique_handles = []\n    seen_labels = set()\n    for handle, label in zip(handles, labels):\n        if label not in seen_labels:\n            seen_labels.add(label)\n            unique_labels.append(label)\n            unique_handles.append(handle)\n    \n    ax2.legend(unique_handles, unique_labels, \n               loc='center left',\n               bbox_to_anchor=(1.0, 0.5))\n    \n    # Add horizontal colorbar\n    cbar_ax = fig.add_subplot(gs[1, :])\n    cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n    cbar.set_label('Accumulated Cost', labelpad=10)\n    \n    cbar.set_ticks(np.linspace(vmin, vmax, 6))\n    cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])\n    \n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.2)\n    \n    # Comment out the plt.show() call:\n    plt.show()\n    \n    # Save the plot\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    end_time = time.time()\n    print(f\"Cost surface subplots created in {end_time - start_time:.2f} seconds\")\n</pre> def create_cost_surface_subplots(dataset_info, cost_arrays, transforms, evacuation_paths,                                 summit_coords, safe_zone_distances, hiking_gdf, output_path):     \"\"\"     Create visualizations of cost surfaces with evacuation paths for two terrain scenarios.          This function generates a figure with two cost surface maps showing optimal evacuation     paths from the summit to different safe zone distances. The maps include distance contours,     hiking paths, and the summit location. Cost surfaces use a logarithmic color scale to     better visualize travel time differences.          Parameters:     -----------     dataset_info : dict         Dictionary containing information about each dataset, including:         - keys: Dataset names (e.g., 'final', 'modify_landcover')         - values: Dictionaries with dataset metadata          cost_arrays : list         List of numpy arrays containing the cost surfaces for each dataset.          transforms : list         List of affine transforms corresponding to each cost array.          evacuation_paths : dict         Dictionary of evacuation paths for each dataset and safe zone:         {dataset_key: {safe_zone_distance: [path_coordinates]}}         where path_coordinates is a list of (row, col) tuples.          summit_coords : dict         Dictionary mapping dataset keys to summit coordinates as (row, col) tuples.          safe_zone_distances : list         List of safe zone distances to plot paths for.          hiking_gdf : geopandas.GeoDataFrame         GeoDataFrame containing hiking trail geometries to overlay on the maps.          output_path : str         File path where the figure will be saved.          Returns:     --------     None         The function saves the figure to the specified output path but does not return a value.          Notes:     ------     - Progress and timing information is printed to standard output.     - A logarithmic transformation is applied to the cost surfaces to enhance visualization.     - The figure includes a color bar indicating the travel time.     - Evacuation paths are color-coded by safe zone distance.     - Contour lines show distance from the summit.     - Each subplot is labeled with a letter (A, B) in the top-left corner.     \"\"\"     print(\"\\nCreating cost surface subplots...\")     start_time = time.time()          # Create figure with larger size      fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4          # Create subplot grid with space for legend and colorbar     gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)     ax1 = fig.add_subplot(gs[0, 0])     ax2 = fig.add_subplot(gs[0, 1])          # Create a common colormap and normalization     cmap = plt.cm.RdYlBu_r.copy()     cmap.set_bad('white', alpha=0)          epsilon = 0.1     max_log_cost = np.log1p(10 + epsilon)          # Find global vmin and vmax for consistent color scaling     vmin = float('inf')     vmax = float('-inf')     for cost_array in cost_arrays:         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)         vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))         vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))     vcenter = vmin + (vmax - vmin) / 2          norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)          # Colors for paths     colors_paths = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange', 'red', 'blue', 'purple']          # Plot both subplots     for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):         cost_array = cost_arrays[idx]         transform = transforms[idx]                  # Process cost array         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)                  # Plot cost surface         im = ax.imshow(             cost_array_masked, cmap=cmap, norm=norm,             extent=[                 transform[2],                  transform[2] + transform[0] * cost_array.shape[1],                 transform[5] + transform[4] * cost_array.shape[0],                 transform[5]             ]         )                  # Plot evacuation paths         for i, distance in enumerate(safe_zone_distances):             if distance in evacuation_paths[ds_key]:                 path = evacuation_paths[ds_key][distance]                 if path:                     path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]                     xs, ys = zip(*path_coords)                     ax.plot(xs, ys, '-', color=colors_paths[i],                              linewidth=3, label=f'{distance}m safe zone',                             alpha=1.0, zorder=5)                  # Plot hiking trail with outline effect         hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)         ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')         plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])                  # Plot summit         summit_x, summit_y = raster_coord_to_map_coords(             summit_coords[ds_key][0], summit_coords[ds_key][1], transform         )         ax.plot(summit_x, summit_y, '*', color='yellow', markersize=15, label='Summit', zorder=6)                  # Add distance contours         x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])         y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])         X, Y = np.meshgrid(x, y)         distances = np.sqrt((X - summit_x)**2 + (Y - summit_y)**2)                  contours = ax.contour(X, Y, distances, levels=safe_zone_distances,                               colors='black', linestyles='--', alpha=0.4,                               linewidths=1.5, zorder=3)         ax.clabel(contours, inline=True, fmt='%1.0fm', fontsize=8)                  # Set plot limits         ax.set_xlim(summit_x - 5000, summit_x + 5000)         ax.set_ylim(summit_y - 5000, summit_y + 5000)                  # Add simple A/B labels         ax.text(0.02, 0.98, f'{chr(65+idx)}',                 transform=ax.transAxes, fontsize=10, fontweight='bold',                 verticalalignment='top')                  # Add axis labels         ax.set_xlabel('Easting (m)')         ax.set_ylabel('Northing (m)')          # Add legend     handles, labels = ax1.get_legend_handles_labels()     unique_labels = []     unique_handles = []     seen_labels = set()     for handle, label in zip(handles, labels):         if label not in seen_labels:             seen_labels.add(label)             unique_labels.append(label)             unique_handles.append(handle)          ax2.legend(unique_handles, unique_labels,                 loc='center left',                bbox_to_anchor=(1.0, 0.5))          # Add horizontal colorbar     cbar_ax = fig.add_subplot(gs[1, :])     cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')     cbar.set_label('Accumulated Cost', labelpad=10)          cbar.set_ticks(np.linspace(vmin, vmax, 6))     cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])          plt.tight_layout()     plt.subplots_adjust(hspace=0.2)          # Comment out the plt.show() call:     plt.show()          # Save the plot     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.close()          end_time = time.time()     print(f\"Cost surface subplots created in {end_time - start_time:.2f} seconds\") In\u00a0[\u00a0]: Copied! <pre>def create_decomposition_table(decomp_data, output_path):\n    \"\"\"\n    Create a visualization table showing the relative contributions of slope and landcover factors.\n    \n    This function takes decomposition analysis results and creates a formatted table\n    showing the percentage contribution of slope and landcover factors to the\n    evacuation path costs for different safe zone thresholds.\n    \n    Parameters:\n    -----------\n    decomp_data : list of dict\n        A list of dictionaries, each containing decomposition results for a safe zone:\n        {\n            \"Safe Zone Threshold (m)\": int,\n            \"Slope Contribution (%)\": float,\n            \"Landcover Contribution (%)\": float\n        }\n    \n    output_path : str\n        File path where the table visualization will be saved as an image.\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        A DataFrame containing the decomposition data, useful for further analysis.\n    \n    Notes:\n    ------\n    - The function creates a visualization of the table using matplotlib.\n    - The table has a formatted header row and is styled for readability.\n    - The table is saved as an image at the specified output path.\n    \"\"\"\n    df = pd.DataFrame(decomp_data)\n    \n    # Create a figure for the table\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.axis('off')\n    \n    # Create the table from the DataFrame\n    table = ax.table(\n        cellText=df.values,\n        colLabels=df.columns,\n        cellLoc='center',\n        loc='center'\n    )\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.4)\n    \n    # Optionally style header row\n    for key, cell in table.get_celld().items():\n        row, col = key\n        if row == 0:\n            cell.set_text_props(weight='bold')\n            cell.set_facecolor('#e6e6e6')\n    \n    plt.tight_layout()\n    plt.show()\n    # Save the figure\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close(fig)\n    \n    return df\n</pre> def create_decomposition_table(decomp_data, output_path):     \"\"\"     Create a visualization table showing the relative contributions of slope and landcover factors.          This function takes decomposition analysis results and creates a formatted table     showing the percentage contribution of slope and landcover factors to the     evacuation path costs for different safe zone thresholds.          Parameters:     -----------     decomp_data : list of dict         A list of dictionaries, each containing decomposition results for a safe zone:         {             \"Safe Zone Threshold (m)\": int,             \"Slope Contribution (%)\": float,             \"Landcover Contribution (%)\": float         }          output_path : str         File path where the table visualization will be saved as an image.          Returns:     --------     pandas.DataFrame         A DataFrame containing the decomposition data, useful for further analysis.          Notes:     ------     - The function creates a visualization of the table using matplotlib.     - The table has a formatted header row and is styled for readability.     - The table is saved as an image at the specified output path.     \"\"\"     df = pd.DataFrame(decomp_data)          # Create a figure for the table     fig, ax = plt.subplots(figsize=(6, 4))     ax.axis('off')          # Create the table from the DataFrame     table = ax.table(         cellText=df.values,         colLabels=df.columns,         cellLoc='center',         loc='center'     )     table.auto_set_font_size(False)     table.set_fontsize(9)     table.scale(1.2, 1.4)          # Optionally style header row     for key, cell in table.get_celld().items():         row, col = key         if row == 0:             cell.set_text_props(weight='bold')             cell.set_facecolor('#e6e6e6')          plt.tight_layout()     plt.show()     # Save the figure     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.close(fig)          return df In\u00a0[\u00a0]: Copied! <pre>def create_final_evacuation_table(all_results, source_names, output_path):\n    \"\"\"\n    Create a comprehensive evacuation time table for different sources and walking speeds.\n    \n    This function generates a formatted table visualization showing evacuation times\n    for different combinations of safe zone distances, source locations, and walking speeds.\n    The table uses a multi-level column organization for clear presentation.\n    \n    Parameters:\n    -----------\n    all_results : dict\n        A nested dictionary containing travel time results:\n        {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}\n        The function uses only the 'final' dataset from this dictionary.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n        Must include 'summit', 'camp1', and 'camp2'.\n    \n    output_path : str\n        File path where the table visualization will be saved as an image.\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        A DataFrame with multi-level columns containing the evacuation time data.\n    \n    Notes:\n    ------\n    - The function selects only specific safe zone distances: 500, 1500, 2500, 3500, 4500m.\n    - The table uses a three-level header structure:\n      1. \"Safe Zone (m)\" / \"Evacuation Time (hours)\"\n      2. Source names (Summit, Camp1, Camp2)\n      3. Walking speeds (Slow, Moderate, Fast)\n    - NaN values are handled appropriately in the output.\n    - The table is styled with a bold header row and saved as an image.\n    \"\"\"\n    \n    # We only want to display these safe zones and these sources/speeds\n    safe_zones = [500, 1500, 2500, 3500, 4500]\n    sources_to_show = ['summit', 'camp1']\n    speeds = ['slow', 'medium', 'fast']\n    \n    # Prepare the table rows\n    table_data = []\n    for sz in safe_zones:\n        # First column is the safe zone distance\n        row = [sz]\n        for src in sources_to_show:\n            for spd in speeds:\n                # Find the index of this source in source_names\n                src_idx = source_names.index(src)\n                # Get the travel time from all_results['final']\n                val = all_results['final'][spd][sz][src_idx]\n                if np.isnan(val):\n                    row.append(\"Nan\")\n                else:\n                    row.append(f\"{val:.2f}\")\n        table_data.append(row)\n    \n    # We will have 1 column for safe zone + 3 sources \u00d7 3 speeds = 1 + 9 = 10 columns total.\n    # Build multi-level columns:\n    #  Top row: \"Safe Zone (m)\" repeated once, then \"Evacuation Time (hours)\" repeated 9 times\n    #  Middle row: \"\", \"Summit\" \u00d7 3, \"Camp1\" \u00d7 3, \"Camp2\" \u00d7 3\n    #  Bottom row: \"\", \"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\" repeated 3 times\n    \n    arrays = [\n        [\"Safe Zone (m)\"] + [\"Evacuation Time (hours)\"] * 9,\n        [\"\"]\n        + [\"Summit\"]*3\n        + [\"Camp1\"]*3,\n        [\"\"]\n        + [\"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\"]*3\n    ]\n    # Convert these into a MultiIndex\n    tuples = list(zip(*arrays))  # Transpose\n    columns = pd.MultiIndex.from_tuples(tuples)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(table_data, columns=columns)\n    \n    # Now plot the DataFrame as a matplotlib table\n    fig, ax = plt.subplots(figsize=(12, 3))  # Wider figure for more columns\n    ax.axis('off')\n    \n    # Convert df to a 2D list (cellText) plus colLabels\n    # However, we have a MultiIndex. We'll create a table with two header rows:\n    # an approach is to flatten the columns for colLabels, but let's do a quick approach:\n    \n    # We can manually set up the table by using the DataFrame's .values for cellText\n    # and the multi-level columns as a header.\n    # For a nicely formatted multi-level header in matplotlib, it's a bit tricky,\n    # so a simpler approach is to just let DataFrame do the \"pretty printing\" into a single row.\n    \n    # Easiest is to do:\n    table = ax.table(\n        cellText=df.values,\n        colLabels=df.columns.to_flat_index(),  # Flatten the multiindex for a single row of headers\n        cellLoc='center',\n        loc='center'\n    )\n    \n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.4)\n    \n    # Optionally style the top row\n    for (row, col), cell in table.get_celld().items():\n        if row == 0:\n            cell.set_text_props(weight='bold')\n            cell.set_facecolor('#e6e6e6')\n    \n    # Show the table inline\n    plt.tight_layout()\n    plt.show()\n    \n    # Save the figure\n    fig.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close(fig)\n    \n    return df\n</pre> def create_final_evacuation_table(all_results, source_names, output_path):     \"\"\"     Create a comprehensive evacuation time table for different sources and walking speeds.          This function generates a formatted table visualization showing evacuation times     for different combinations of safe zone distances, source locations, and walking speeds.     The table uses a multi-level column organization for clear presentation.          Parameters:     -----------     all_results : dict         A nested dictionary containing travel time results:         {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}         The function uses only the 'final' dataset from this dictionary.          source_names : list         A list of source location names corresponding to indices in the results arrays.         Must include 'summit', 'camp1', and 'camp2'.          output_path : str         File path where the table visualization will be saved as an image.          Returns:     --------     pandas.DataFrame         A DataFrame with multi-level columns containing the evacuation time data.          Notes:     ------     - The function selects only specific safe zone distances: 500, 1500, 2500, 3500, 4500m.     - The table uses a three-level header structure:       1. \"Safe Zone (m)\" / \"Evacuation Time (hours)\"       2. Source names (Summit, Camp1, Camp2)       3. Walking speeds (Slow, Moderate, Fast)     - NaN values are handled appropriately in the output.     - The table is styled with a bold header row and saved as an image.     \"\"\"          # We only want to display these safe zones and these sources/speeds     safe_zones = [500, 1500, 2500, 3500, 4500]     sources_to_show = ['summit', 'camp1']     speeds = ['slow', 'medium', 'fast']          # Prepare the table rows     table_data = []     for sz in safe_zones:         # First column is the safe zone distance         row = [sz]         for src in sources_to_show:             for spd in speeds:                 # Find the index of this source in source_names                 src_idx = source_names.index(src)                 # Get the travel time from all_results['final']                 val = all_results['final'][spd][sz][src_idx]                 if np.isnan(val):                     row.append(\"Nan\")                 else:                     row.append(f\"{val:.2f}\")         table_data.append(row)          # We will have 1 column for safe zone + 3 sources \u00d7 3 speeds = 1 + 9 = 10 columns total.     # Build multi-level columns:     #  Top row: \"Safe Zone (m)\" repeated once, then \"Evacuation Time (hours)\" repeated 9 times     #  Middle row: \"\", \"Summit\" \u00d7 3, \"Camp1\" \u00d7 3, \"Camp2\" \u00d7 3     #  Bottom row: \"\", \"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\" repeated 3 times          arrays = [         [\"Safe Zone (m)\"] + [\"Evacuation Time (hours)\"] * 9,         [\"\"]         + [\"Summit\"]*3         + [\"Camp1\"]*3,         [\"\"]         + [\"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\"]*3     ]     # Convert these into a MultiIndex     tuples = list(zip(*arrays))  # Transpose     columns = pd.MultiIndex.from_tuples(tuples)          # Create a DataFrame     df = pd.DataFrame(table_data, columns=columns)          # Now plot the DataFrame as a matplotlib table     fig, ax = plt.subplots(figsize=(12, 3))  # Wider figure for more columns     ax.axis('off')          # Convert df to a 2D list (cellText) plus colLabels     # However, we have a MultiIndex. We'll create a table with two header rows:     # an approach is to flatten the columns for colLabels, but let's do a quick approach:          # We can manually set up the table by using the DataFrame's .values for cellText     # and the multi-level columns as a header.     # For a nicely formatted multi-level header in matplotlib, it's a bit tricky,     # so a simpler approach is to just let DataFrame do the \"pretty printing\" into a single row.          # Easiest is to do:     table = ax.table(         cellText=df.values,         colLabels=df.columns.to_flat_index(),  # Flatten the multiindex for a single row of headers         cellLoc='center',         loc='center'     )          table.auto_set_font_size(False)     table.set_fontsize(9)     table.scale(1.2, 1.4)          # Optionally style the top row     for (row, col), cell in table.get_celld().items():         if row == 0:             cell.set_text_props(weight='bold')             cell.set_facecolor('#e6e6e6')          # Show the table inline     plt.tight_layout()     plt.show()          # Save the figure     fig.savefig(output_path, dpi=300, bbox_inches='tight')     plt.close(fig)          return df"},{"location":"api/probability-analysis/analysis.html","title":"Analysis Module","text":"<p>The <code>analysis.py</code> module provides functions for conducting volcanic evacuation analysis based on eruption probability thresholds and calculating travel times to safe zones.</p>"},{"location":"api/probability-analysis/analysis.html#functions","title":"Functions","text":""},{"location":"api/probability-analysis/analysis.html#perform_evacuation_analysis","title":"<code>perform_evacuation_analysis</code>","text":"<pre><code>def perform_evacuation_analysis(cost_paths, source_coords, source_names, walking_speeds, output_dir)\n</code></pre> <p>Perform evacuation analysis for multiple cost datasets and walking speeds.</p> <p>This function processes cost rasters, builds adjacency matrices, computes shortest paths, and generates travel time rasters for various evacuation scenarios.</p> <p>Parameters:</p> <ul> <li><code>cost_paths (dict)</code>: Dictionary mapping dataset keys to file paths of cost rasters.</li> <li><code>source_coords (list)</code>: List of source (row, col) coordinates in the raster grid.</li> <li><code>source_names (list)</code>: List of source names corresponding to the coordinates.</li> <li><code>walking_speeds (dict)</code>: Dictionary mapping speed names to values in m/s.</li> <li><code>output_dir (str)</code>: Directory to save the output files.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>all_results (dict)</code>: Dictionary of evacuation analysis results.</li> <li><code>dataset_info (dict)</code>: Dictionary of dataset information including path predecessors and raster properties.</li> </ul>"},{"location":"api/probability-analysis/analysis.html#analyze_safe_zones","title":"<code>analyze_safe_zones</code>","text":"<pre><code>def analyze_safe_zones(probability_path, travel_time_data, thresholds, source_names, walking_speeds, dataset_key, output_dir)\n</code></pre> <p>Analyze safe zones based on eruption probability thresholds.</p> <p>This function identifies areas with eruption probability below specified thresholds and calculates minimum travel times to reach these safe zones from different source locations.</p> <p>Parameters:</p> <ul> <li><code>probability_path (str)</code>: Path to the eruption probability raster.</li> <li><code>travel_time_data (dict)</code>: Dictionary of travel time data structured as:   <pre><code>{\n    speed_name: {\n        source_name: {\n            'cost_array': 2D array,\n            'cost_array_flat': 1D array,\n            'meta': metadata,\n            'shape': tuple\n        }\n    }\n}\n</code></pre></li> <li><code>thresholds (list)</code>: List of probability thresholds defining safe zones.</li> <li><code>source_names (list)</code>: List of source names.</li> <li><code>walking_speeds (dict)</code>: Dictionary mapping speed names to values in m/s.</li> <li><code>dataset_key (str)</code>: Key identifying the current dataset.</li> <li><code>output_dir (str)</code>: Directory to save output files.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>results (dict)</code>: Dictionary of minimum travel times for each speed, threshold, and source.</li> <li><code>min_coords (dict)</code>: Dictionary of coordinates for minimum travel times.</li> </ul>"},{"location":"api/probability-analysis/analysis.html#load_travel_time_data","title":"<code>load_travel_time_data</code>","text":"<pre><code>def load_travel_time_data(dataset_key, source_names, walking_speeds, output_dir)\n</code></pre> <p>Load travel time rasters for further analysis.</p> <p>This function loads previously generated travel time rasters for a specific dataset and organizes them for safe zone analysis.</p> <p>Parameters:</p> <ul> <li><code>dataset_key (str)</code>: Key identifying the dataset.</li> <li><code>source_names (list)</code>: List of source names.</li> <li><code>walking_speeds (dict)</code>: Dictionary mapping speed names to values in m/s.</li> <li><code>output_dir (str)</code>: Directory where rasters are stored.</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary of travel time data organized by speed and source.</li> </ul>"},{"location":"api/probability-analysis/analysis.html#read_raster","title":"<code>read_raster</code>","text":"<pre><code>def read_raster(path)\n</code></pre> <p>Read a raster file and return its data, metadata, and properties.</p> <p>This function opens a raster file and extracts its data and spatial properties.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: Path to the raster file.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>data (numpy.ndarray)</code>: The raster data with shape (bands, rows, cols).</li> <li><code>meta (dict)</code>: A copy of the raster metadata.</li> <li><code>transform</code>: The affine transform of the raster.</li> <li><code>nodata</code>: The NoData value of the raster.</li> <li><code>bounds</code>: The spatial bounds of the raster.</li> <li><code>resolution</code>: The resolution of the raster.</li> </ul>"},{"location":"api/probability-analysis/data-utils.html","title":"Data Utilities","text":"<p>The <code>data_utils.py</code> module provides functions for loading, saving, and processing geospatial data files for volcanic evacuation analysis, including shapefiles, rasters, and analysis reports.</p>"},{"location":"api/probability-analysis/data-utils.html#functions","title":"Functions","text":""},{"location":"api/probability-analysis/data-utils.html#read_shapefile","title":"<code>read_shapefile</code>","text":"<pre><code>def read_shapefile(path)\n</code></pre> <p>Read a shapefile into a GeoDataFrame.</p> <p>This function opens a shapefile using Fiona and converts it to a GeoPandas GeoDataFrame for further spatial analysis.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: Path to the shapefile.</li> </ul> <p>Returns:</p> <ul> <li><code>gpd.GeoDataFrame</code>: GeoDataFrame containing the shapefile data.</li> </ul>"},{"location":"api/probability-analysis/data-utils.html#read_raster","title":"<code>read_raster</code>","text":"<pre><code>def read_raster(path)\n</code></pre> <p>Read a raster file and return its data, metadata, and properties.</p> <p>This function opens a raster file using Rasterio and extracts the raster data along with its metadata and spatial properties.</p> <p>Parameters:</p> <ul> <li><code>path (str)</code>: Path to the raster file.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>data (numpy.ndarray)</code>: The raster data with shape (bands, rows, cols).</li> <li><code>meta (dict)</code>: A copy of the raster metadata.</li> <li><code>transform</code>: The affine transform of the raster.</li> <li><code>nodata</code>: The NoData value of the raster.</li> <li><code>bounds</code>: The spatial bounds of the raster.</li> <li><code>resolution</code>: The resolution of the raster.</li> </ul>"},{"location":"api/probability-analysis/data-utils.html#load_raster","title":"<code>load_raster</code>","text":"<pre><code>def load_raster(file_path)\n</code></pre> <p>Load a raster file and return its data and metadata.</p> <p>A simplified version of <code>read_raster</code> that reads only the first band of a raster file and its metadata.</p> <p>Parameters:</p> <ul> <li><code>file_path (str)</code>: Path to the raster file.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>data (numpy.ndarray)</code>: The raster data from the first band.</li> <li><code>meta (dict)</code>: A copy of the raster metadata.</li> </ul>"},{"location":"api/probability-analysis/data-utils.html#save_raster","title":"<code>save_raster</code>","text":"<pre><code>def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1)\n</code></pre> <p>Save a numpy array as a raster file.</p> <p>This function saves a numpy array to disk as a geospatial raster file with the specified metadata.</p> <p>Parameters:</p> <ul> <li><code>output_path (str)</code>: Path to save the raster.</li> <li><code>data (numpy.ndarray)</code>: The raster data to save.</li> <li><code>meta (dict)</code>: The raster metadata.</li> <li><code>dtype</code>: The data type of the output raster (default: rasterio.float32).</li> <li><code>nodata</code>: The NoData value for the output raster (default: -1).</li> </ul> <p>Returns:</p> <ul> <li>None</li> </ul>"},{"location":"api/probability-analysis/data-utils.html#save_analysis_report","title":"<code>save_analysis_report</code>","text":"<pre><code>def save_analysis_report(results, min_coords, source_names, thresholds, walking_speeds, dataset_key, output_dir)\n</code></pre> <p>Save analysis results to a text report and CSV file.</p> <p>This function generates a human-readable text report and a CSV file summarizing the evacuation analysis results.</p> <p>Parameters:</p> <ul> <li><code>results (dict)</code>: Dictionary of results structured as:   <pre><code>{\n    speed_name: {\n        threshold: [times_per_source]\n    }\n}\n</code></pre></li> <li><code>min_coords (dict)</code>: Dictionary of coordinates structured as:   <pre><code>{\n    speed_name: {\n        threshold: [coords_per_source]\n    }\n}\n</code></pre></li> <li><code>source_names (list)</code>: List of source location names.</li> <li><code>thresholds (list)</code>: List of probability thresholds.</li> <li><code>walking_speeds (dict)</code>: Dictionary mapping speed names to values in m/s.</li> <li><code>dataset_key (str)</code>: Key identifying the dataset.</li> <li><code>output_dir (str)</code>: Directory to save outputs.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: Paths to the saved text report and CSV file.</li> </ul>"},{"location":"api/probability-analysis/data-utils.html#create_statistics_table","title":"<code>create_statistics_table</code>","text":"<pre><code>def create_statistics_table(results, source_names, walking_speeds, thresholds, dataset_key, output_dir)\n</code></pre> <p>Create a statistics table and save it as PNG and CSV.</p> <p>This function generates a summary table of travel time statistics (min, max, mean) for different sources and walking speeds, and saves it as both a PNG visualization and a CSV file.</p> <p>Parameters:</p> <ul> <li><code>results (dict)</code>: Dictionary of results.</li> <li><code>source_names (list)</code>: List of source names.</li> <li><code>walking_speeds (dict)</code>: Dictionary of walking speeds.</li> <li><code>thresholds (list)</code>: List of probability thresholds.</li> <li><code>dataset_key (str)</code>: Key for the dataset.</li> <li><code>output_dir (str)</code>: Directory to save outputs.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: Paths to the saved PNG and CSV files.</li> </ul>"},{"location":"api/probability-analysis/graph-utils.html","title":"Graph Utilities","text":"<p>The <code>graph_utils.py</code> module provides functions for constructing graphs from raster data, calculating shortest paths, and analyzing path metrics for evacuation analysis.</p>"},{"location":"api/probability-analysis/graph-utils.html#functions","title":"Functions","text":""},{"location":"api/probability-analysis/graph-utils.html#build_adjacency_matrix","title":"<code>build_adjacency_matrix</code>","text":"<pre><code>def build_adjacency_matrix(cost_array, rows, cols)\n</code></pre> <p>Build an adjacency matrix (graph) from a cost raster.</p> <p>This function creates a sparse graph representation where nodes are raster cells and edges represent possible movements between adjacent cells. The cost of movement is determined by the cost array values and adjusted for diagonal movement.</p> <p>Parameters:</p> <ul> <li><code>cost_array (numpy.ndarray)</code>: Cost array with shape (bands, rows, cols) where bands represent 8 directions of movement.</li> <li><code>rows (int)</code>: Number of rows in the raster.</li> <li><code>cols (int)</code>: Number of columns in the raster.</li> </ul> <p>Returns:</p> <ul> <li><code>scipy.sparse.csr_matrix</code>: CSR format adjacency matrix representing the graph for path analysis.</li> </ul>"},{"location":"api/probability-analysis/graph-utils.html#compute_shortest_paths","title":"<code>compute_shortest_paths</code>","text":"<pre><code>def compute_shortest_paths(graph_csr, source_nodes)\n</code></pre> <p>Compute shortest paths from source nodes using Dijkstra's algorithm.</p> <p>This function calculates the shortest paths from specified source nodes to all other nodes in the graph using Dijkstra's algorithm.</p> <p>Parameters:</p> <ul> <li><code>graph_csr (scipy.sparse.csr_matrix)</code>: CSR format adjacency matrix representing the graph.</li> <li><code>source_nodes (list)</code>: List of source node indices from which to compute shortest paths.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>distances (numpy.ndarray)</code>: 2D array of distances from each source to all nodes.</li> <li><code>predecessors (numpy.ndarray)</code>: 2D array of predecessor nodes in the shortest paths.</li> </ul>"},{"location":"api/probability-analysis/graph-utils.html#reconstruct_path","title":"<code>reconstruct_path</code>","text":"<pre><code>def reconstruct_path(predecessors, source_node, target_node, cols)\n</code></pre> <p>Reconstruct the path from source_node to target_node using the predecessor array.</p> <p>This function traces the shortest path from a source node to a target node using the predecessor information from Dijkstra's algorithm and converts the path to 2D raster coordinates.</p> <p>Parameters:</p> <ul> <li><code>predecessors (numpy.ndarray)</code>: Predecessor array from Dijkstra's algorithm.</li> <li><code>source_node (int)</code>: Source node index (starting point).</li> <li><code>target_node (int)</code>: Target node index (destination).</li> <li><code>cols (int)</code>: Number of columns in the raster.</li> </ul> <p>Returns:</p> <ul> <li><code>list</code>: List of (row, col) coordinates for the path. Empty list if no valid path exists.</li> </ul>"},{"location":"api/probability-analysis/graph-utils.html#calculate_path_metrics","title":"<code>calculate_path_metrics</code>","text":"<pre><code>def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols)\n</code></pre> <p>Calculate metrics for a path between source and target nodes.</p> <p>This function reconstructs the shortest path between a source and target node and calculates various metrics about the path, including its length and cost.</p> <p>Parameters:</p> <ul> <li><code>predecessors (numpy.ndarray)</code>: Predecessor array from Dijkstra's algorithm.</li> <li><code>source_node (int)</code>: Source node index (starting point).</li> <li><code>target_node (int)</code>: Target node index (destination).</li> <li><code>graph_csr (scipy.sparse.csr_matrix)</code>: CSR format adjacency matrix.</li> <li><code>rows (int)</code>: Number of rows in the raster.</li> <li><code>cols (int)</code>: Number of columns in the raster.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>pixel_count (int)</code>: Number of pixels in the path.</li> <li><code>cell_costs (list)</code>: List of costs for each step in the path.</li> <li><code>total_cost (float)</code>: Sum of all costs along the path.</li> </ul>"},{"location":"api/probability-analysis/raster-utils.html","title":"Raster Utilities","text":"<p>The <code>raster_utils.py</code> module provides functions for working with raster data, including coordinate transformations, resampling, and data processing for evacuation analysis.</p>"},{"location":"api/probability-analysis/raster-utils.html#functions","title":"Functions","text":""},{"location":"api/probability-analysis/raster-utils.html#resample_raster","title":"<code>resample_raster</code>","text":"<pre><code>def resample_raster(source_path, target_path, output_path, resampling_method=rasterio.warp.Resampling.bilinear)\n</code></pre> <p>Resample a source raster to match the grid of a target raster.</p> <p>This function resamples a source raster to match the spatial reference system, resolution, and extent of a target raster, which is useful for ensuring multiple raster datasets align correctly for analysis.</p> <p>Parameters:</p> <ul> <li><code>source_path (str)</code>: Path to the source raster to be resampled.</li> <li><code>target_path (str)</code>: Path to the target raster that provides the reference grid.</li> <li><code>output_path (str)</code>: Path where the resampled raster will be saved.</li> <li><code>resampling_method</code>: The resampling algorithm to use (default: bilinear interpolation).</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: A tuple containing:</li> <li><code>resampled_array (numpy.ndarray)</code>: The resampled data array.</li> <li><code>resampled_meta (dict)</code>: Metadata for the resampled raster.</li> </ul>"},{"location":"api/probability-analysis/raster-utils.html#coords_to_raster","title":"<code>coords_to_raster</code>","text":"<pre><code>def coords_to_raster(gdf, transform, bounds, res)\n</code></pre> <p>Convert geographic coordinates to raster row/column coordinates.</p> <p>This function takes a GeoDataFrame containing point geometries and converts their geographic coordinates to corresponding row and column indices in a raster.</p> <p>Parameters:</p> <ul> <li><code>gdf (GeoDataFrame)</code>: GeoDataFrame containing point geometries.</li> <li><code>transform</code>: The raster's affine transform defining the relationship between raster indices and geographic coordinates.</li> <li><code>bounds</code>: The raster's geographic bounds (left, bottom, right, top).</li> <li><code>res</code>: The raster's resolution as (x_resolution, y_resolution).</li> </ul> <p>Returns:</p> <ul> <li><code>list</code>: List of (row, col) coordinates corresponding to the input points. Points outside the raster bounds are excluded.</li> </ul>"},{"location":"api/probability-analysis/raster-utils.html#raster_coord_to_map_coords","title":"<code>raster_coord_to_map_coords</code>","text":"<pre><code>def raster_coord_to_map_coords(row, col, transform)\n</code></pre> <p>Convert raster (row, col) coordinates to map (x, y) coordinates.</p> <p>This function transforms raster grid indices to their corresponding geographic coordinates using the raster's affine transform.</p> <p>Parameters:</p> <ul> <li><code>row (int)</code>: Raster row index.</li> <li><code>col (int)</code>: Raster column index.</li> <li><code>transform</code>: Raster affine transform.</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: (x, y) map coordinates corresponding to the center of the specified raster cell.</li> </ul>"},{"location":"api/probability-analysis/raster-utils.html#to_1d","title":"<code>to_1d</code>","text":"<pre><code>def to_1d(r, c, cols)\n</code></pre> <p>Convert 2D (row, col) coordinates to 1D index.</p> <p>This function maps 2D raster coordinates to a 1D index, which is useful for working with graph algorithms that operate on linear arrays.</p> <p>Parameters:</p> <ul> <li><code>r (int)</code>: Row index.</li> <li><code>c (int)</code>: Column index.</li> <li><code>cols (int)</code>: Number of columns in the raster.</li> </ul> <p>Returns:</p> <ul> <li><code>int</code>: 1D index corresponding to the input row and column.</li> </ul>"},{"location":"api/probability-analysis/raster-utils.html#process_raster","title":"<code>process_raster</code>","text":"<pre><code>def process_raster(cost_array, walking_speed, cell_size=100)\n</code></pre> <p>Convert cost raster to travel time (in hours).</p> <p>This function transforms a cost raster into travel time by considering walking speed and cell size, which is essential for evacuation time analysis.</p> <p>Parameters:</p> <ul> <li><code>cost_array (numpy.ndarray)</code>: Cost array representing the difficulty or distance of traversing each cell.</li> <li><code>walking_speed (float)</code>: Walking speed in meters per second.</li> <li><code>cell_size (float)</code>: Cell size in meters (default: 100m).</li> </ul> <p>Returns:</p> <ul> <li><code>numpy.ndarray</code>: Travel time array in hours. Infinite values are replaced with -1.</li> </ul>"},{"location":"api/probability-analysis/visualization.html","title":"Visualization","text":"<p>The <code>visualization.py</code> module provides functions for creating visualizations of volcanic evacuation analysis results, including travel time comparisons, cost surfaces with evacuation paths, and VEI (Volcanic Explosivity Index) comparison plots.</p>"},{"location":"api/probability-analysis/visualization.html#functions","title":"Functions","text":""},{"location":"api/probability-analysis/visualization.html#plot_travel_time_comparison","title":"<code>plot_travel_time_comparison</code>","text":"<pre><code>def plot_travel_time_comparison(all_results, source_names, thresholds, walking_speeds, output_dir, filename=\"comparison_travel_time_by_source.png\")\n</code></pre> <p>Plot a comparison of travel times between datasets.</p> <p>This function creates a multi-panel plot comparing evacuation travel times for different probability thresholds, walking speeds, and terrain scenarios (original vs. penalized landcover).</p> <p>Parameters:</p> <ul> <li><code>all_results (dict)</code>: Dictionary of results from all datasets, structured as:   <pre><code>{\n    dataset_key: {\n        speed_name: {\n            threshold: [times_per_source]\n        }\n    }\n}\n</code></pre></li> <li><code>source_names (list)</code>: List of source names (e.g., 'summit', 'camp1').</li> <li><code>thresholds (list)</code>: List of probability thresholds for safe zones.</li> <li><code>walking_speeds (dict)</code>: Dictionary mapping speed names to values in m/s.</li> <li><code>output_dir (str)</code>: Directory to save the plot.</li> <li><code>filename (str)</code>: Filename for the output plot (default: \"comparison_travel_time_by_source.png\").</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Path to the saved plot.</li> </ul>"},{"location":"api/probability-analysis/visualization.html#plot_cost_surface_with_paths","title":"<code>plot_cost_surface_with_paths</code>","text":"<pre><code>def plot_cost_surface_with_paths(dataset_info, evacuation_paths, eruption_probability_path, hiking_path, selected_speed, thresholds, output_dir, vei_label=\"VEI4\")\n</code></pre> <p>Plot cost surface with evacuation paths.</p> <p>This function generates a visualization of evacuation paths overlaid on cost surfaces for different terrain scenarios, incorporating eruption probability contours and hiking trails.</p> <p>Parameters:</p> <ul> <li><code>dataset_info (dict)</code>: Dictionary containing information about each dataset.</li> <li><code>evacuation_paths (dict)</code>: Dictionary of evacuation paths structured as:    <pre><code>{\n    dataset_key: {\n        threshold: [(row, col), ...]\n    }\n}\n</code></pre></li> <li><code>eruption_probability_path (str)</code>: Path to the eruption probability raster.</li> <li><code>hiking_path (str)</code>: Path to the hiking trail shapefile.</li> <li><code>selected_speed (str)</code>: Selected walking speed for visualization (e.g., 'medium').</li> <li><code>thresholds (list)</code>: List of probability thresholds for evacuation paths.</li> <li><code>output_dir (str)</code>: Directory to save the plot.</li> <li><code>vei_label (str)</code>: VEI label for the title (default: \"VEI4\").</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Path to the saved plot.</li> </ul>"},{"location":"api/probability-analysis/visualization.html#create_vei_comparison_plot","title":"<code>create_vei_comparison_plot</code>","text":"<pre><code>def create_vei_comparison_plot(eruption_probability_paths, hiking_path, summit_path, output_dir)\n</code></pre> <p>Create a comparison plot of contour maps for different VEI levels side by side.</p> <p>This function creates a multi-panel visualization showing probability contours for different Volcanic Explosivity Index (VEI) levels, helping to compare the spatial extent of hazards across eruption scenarios.</p> <p>Parameters:</p> <ul> <li><code>eruption_probability_paths (dict)</code>: Dictionary mapping VEI levels to file paths of eruption probability rasters.</li> <li><code>hiking_path (str)</code>: Path to the hiking trail shapefile.</li> <li><code>summit_path (str)</code>: Path to the summit point shapefile.</li> <li><code>output_dir (str)</code>: Directory to save the plot.</li> </ul> <p>Returns:</p> <ul> <li><code>str</code>: Path to the saved plot.</li> </ul>"},{"location":"api/probability-analysis/New%20folder/data-utils.html","title":"Data utils","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nData loading and file operation utilities for the volcanic evacuation analysis.\n\"\"\"\n\nimport os\nimport numpy as np\nimport rasterio\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nimport csv\n\n\ndef read_shapefile(path):\n    \"\"\"\n    Read a shapefile into a GeoDataFrame.\n    \n    Args:\n        path (str): Path to the shapefile\n        \n    Returns:\n        gpd.GeoDataFrame: GeoDataFrame containing the shapefile data\n    \"\"\"\n    with fiona.open(path) as src:\n        return gpd.GeoDataFrame.from_features(src)\n\n\ndef read_raster(path):\n    \"\"\"\n    Read a raster file and return its data, metadata, and properties.\n    \n    Args:\n        path (str): Path to the raster file\n        \n    Returns:\n        tuple: (data, meta, transform, nodata, bounds, resolution)\n    \"\"\"\n    with rasterio.open(path) as src:\n        data = src.read()\n        meta = src.meta.copy()\n        transform = src.transform\n        nodata = src.nodata\n        bounds = src.bounds\n        resolution = src.res\n    return data, meta, transform, nodata, bounds, resolution\n\n\ndef load_raster(file_path):\n    \"\"\"\n    Load a raster file and return its data and metadata.\n    \n    Args:\n        file_path (str): Path to the raster file\n        \n    Returns:\n        tuple: (data, meta)\n    \"\"\"\n    with rasterio.open(file_path) as src:\n        data = src.read(1)  # Read the first band\n        meta = src.meta.copy()\n    return data, meta\n\n\ndef save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):\n    \"\"\"\n    Save a numpy array as a raster file.\n    \n    Args:\n        output_path (str): Path to save the raster\n        data (numpy.ndarray): The raster data\n        meta (dict): The raster metadata\n        dtype: The data type of the output raster\n        nodata: The NoData value\n    \"\"\"\n    meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)\n    with rasterio.open(output_path, 'w', **meta) as dst:\n        dst.write(data, 1)\n\n\ndef save_analysis_report(results, min_coords, source_names, thresholds, walking_speeds, dataset_key, output_dir):\n    \"\"\"\n    Save analysis results to a text report and CSV file.\n    \n    Args:\n        results (dict): Dictionary of results\n        min_coords (dict): Dictionary of coordinates\n        source_names (list): List of source names\n        thresholds (list): List of probability thresholds\n        walking_speeds (dict): Dictionary of walking speeds\n        dataset_key (str): Key for the dataset\n        output_dir (str): Directory to save outputs\n    \"\"\"\n    # Save text report\n    output_report_path = os.path.join(output_dir, f\"eruption_probability_travel_time_report_{dataset_key}.txt\")\n    with open(output_report_path, 'w') as f:\n        f.write(\"Eruption Probability Travel Time Analysis Report\\n\")\n        f.write(\"=================================================\\n\\n\")\n        for speed_name in walking_speeds.keys():\n            f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")\n            f.write(\"-\" * 40 + \"\\n\")\n            for thresh in thresholds:\n                f.write(f\"\\nEruption Probability Threshold: {thresh}\\n\")\n                for idx, source_name in enumerate(source_names):\n                    tt = results[speed_name][thresh][idx]\n                    coords = min_coords[speed_name][thresh][idx]\n                    f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\")\n    \n    # Save CSV file\n    output_csv_path = os.path.join(output_dir, f\"eruption_probability_travel_time_metrics_{dataset_key}.csv\")\n    with open(output_csv_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Walking_Speed', 'Probability_Threshold', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])\n        for speed_name in walking_speeds.keys():\n            for thresh in thresholds:\n                for idx, source_name in enumerate(source_names):\n                    writer.writerow([\n                        speed_name,\n                        thresh,\n                        source_name,\n                        results[speed_name][thresh][idx],\n                        min_coords[speed_name][thresh][idx]\n                    ])\n    \n    return output_report_path, output_csv_path\n\n\ndef create_statistics_table(results, source_names, walking_speeds, thresholds, dataset_key, output_dir):\n    \"\"\"\n    Create a statistics table and save it as PNG and CSV.\n    \n    Args:\n        results (dict): Dictionary of results\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        thresholds (list): List of probability thresholds\n        dataset_key (str): Key for the dataset\n        output_dir (str): Directory to save outputs\n        \n    Returns:\n        tuple: Paths to the saved PNG and CSV files\n    \"\"\"\n    import matplotlib.pyplot as plt\n    \n    stats_data = []\n    for source_name in source_names:\n        i = source_names.index(source_name)\n        for metric in ['Min', 'Max', 'Mean']:\n            row_data = {\n                'Source': source_name,\n                'Metric': f'{metric} (hours)'\n            }\n            for speed_name, speed_val in walking_speeds.items():\n                times = [results[speed_name][thresh][i] for thresh in thresholds]\n                times = [t for t in times if not np.isnan(t)]\n                if len(times) == 0:\n                    value = np.nan\n                else:\n                    if metric == 'Min':\n                        value = min(times)\n                    elif metric == 'Max':\n                        value = max(times)\n                    else:\n                        value = np.mean(times)\n                row_data[f'{speed_name.capitalize()} ({speed_val} m/s)'] = f\"{value:.2f}\" if not np.isnan(value) else \"NaN\"\n            stats_data.append(row_data)\n    \n    df_stats = pd.DataFrame(stats_data)\n    plt.figure(figsize=(12, 8))\n    plt.axis('tight')\n    plt.axis('off')\n    table = plt.table(\n        cellText=df_stats.values,\n        colLabels=df_stats.columns,\n        cellLoc='center',\n        loc='center',\n        colColours=['#e6e6e6']*len(df_stats.columns)\n    )\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.5)\n    plt.title(f'Travel Time Statistics by Source and Walking Speed\\n[Dataset: {dataset_key}]', pad=3, y=0.85)\n    \n    # Save as PNG\n    table_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.png\")\n    plt.savefig(table_output_path, dpi=300, bbox_inches='tight', pad_inches=0.2)\n    plt.close()\n    \n    # Save as CSV\n    csv_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.csv\")\n    df_stats.to_csv(csv_output_path, index=False)\n    \n    return table_output_path, csv_output_path\n</pre> \"\"\" Data loading and file operation utilities for the volcanic evacuation analysis. \"\"\"  import os import numpy as np import rasterio import geopandas as gpd import fiona import pandas as pd import csv   def read_shapefile(path):     \"\"\"     Read a shapefile into a GeoDataFrame.          Args:         path (str): Path to the shapefile              Returns:         gpd.GeoDataFrame: GeoDataFrame containing the shapefile data     \"\"\"     with fiona.open(path) as src:         return gpd.GeoDataFrame.from_features(src)   def read_raster(path):     \"\"\"     Read a raster file and return its data, metadata, and properties.          Args:         path (str): Path to the raster file              Returns:         tuple: (data, meta, transform, nodata, bounds, resolution)     \"\"\"     with rasterio.open(path) as src:         data = src.read()         meta = src.meta.copy()         transform = src.transform         nodata = src.nodata         bounds = src.bounds         resolution = src.res     return data, meta, transform, nodata, bounds, resolution   def load_raster(file_path):     \"\"\"     Load a raster file and return its data and metadata.          Args:         file_path (str): Path to the raster file              Returns:         tuple: (data, meta)     \"\"\"     with rasterio.open(file_path) as src:         data = src.read(1)  # Read the first band         meta = src.meta.copy()     return data, meta   def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):     \"\"\"     Save a numpy array as a raster file.          Args:         output_path (str): Path to save the raster         data (numpy.ndarray): The raster data         meta (dict): The raster metadata         dtype: The data type of the output raster         nodata: The NoData value     \"\"\"     meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)     with rasterio.open(output_path, 'w', **meta) as dst:         dst.write(data, 1)   def save_analysis_report(results, min_coords, source_names, thresholds, walking_speeds, dataset_key, output_dir):     \"\"\"     Save analysis results to a text report and CSV file.          Args:         results (dict): Dictionary of results         min_coords (dict): Dictionary of coordinates         source_names (list): List of source names         thresholds (list): List of probability thresholds         walking_speeds (dict): Dictionary of walking speeds         dataset_key (str): Key for the dataset         output_dir (str): Directory to save outputs     \"\"\"     # Save text report     output_report_path = os.path.join(output_dir, f\"eruption_probability_travel_time_report_{dataset_key}.txt\")     with open(output_report_path, 'w') as f:         f.write(\"Eruption Probability Travel Time Analysis Report\\n\")         f.write(\"=================================================\\n\\n\")         for speed_name in walking_speeds.keys():             f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")             f.write(\"-\" * 40 + \"\\n\")             for thresh in thresholds:                 f.write(f\"\\nEruption Probability Threshold: {thresh}\\n\")                 for idx, source_name in enumerate(source_names):                     tt = results[speed_name][thresh][idx]                     coords = min_coords[speed_name][thresh][idx]                     f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\")          # Save CSV file     output_csv_path = os.path.join(output_dir, f\"eruption_probability_travel_time_metrics_{dataset_key}.csv\")     with open(output_csv_path, 'w', newline='') as f:         writer = csv.writer(f)         writer.writerow(['Walking_Speed', 'Probability_Threshold', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])         for speed_name in walking_speeds.keys():             for thresh in thresholds:                 for idx, source_name in enumerate(source_names):                     writer.writerow([                         speed_name,                         thresh,                         source_name,                         results[speed_name][thresh][idx],                         min_coords[speed_name][thresh][idx]                     ])          return output_report_path, output_csv_path   def create_statistics_table(results, source_names, walking_speeds, thresholds, dataset_key, output_dir):     \"\"\"     Create a statistics table and save it as PNG and CSV.          Args:         results (dict): Dictionary of results         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         thresholds (list): List of probability thresholds         dataset_key (str): Key for the dataset         output_dir (str): Directory to save outputs              Returns:         tuple: Paths to the saved PNG and CSV files     \"\"\"     import matplotlib.pyplot as plt          stats_data = []     for source_name in source_names:         i = source_names.index(source_name)         for metric in ['Min', 'Max', 'Mean']:             row_data = {                 'Source': source_name,                 'Metric': f'{metric} (hours)'             }             for speed_name, speed_val in walking_speeds.items():                 times = [results[speed_name][thresh][i] for thresh in thresholds]                 times = [t for t in times if not np.isnan(t)]                 if len(times) == 0:                     value = np.nan                 else:                     if metric == 'Min':                         value = min(times)                     elif metric == 'Max':                         value = max(times)                     else:                         value = np.mean(times)                 row_data[f'{speed_name.capitalize()} ({speed_val} m/s)'] = f\"{value:.2f}\" if not np.isnan(value) else \"NaN\"             stats_data.append(row_data)          df_stats = pd.DataFrame(stats_data)     plt.figure(figsize=(12, 8))     plt.axis('tight')     plt.axis('off')     table = plt.table(         cellText=df_stats.values,         colLabels=df_stats.columns,         cellLoc='center',         loc='center',         colColours=['#e6e6e6']*len(df_stats.columns)     )     table.auto_set_font_size(False)     table.set_fontsize(9)     table.scale(1.2, 1.5)     plt.title(f'Travel Time Statistics by Source and Walking Speed\\n[Dataset: {dataset_key}]', pad=3, y=0.85)          # Save as PNG     table_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.png\")     plt.savefig(table_output_path, dpi=300, bbox_inches='tight', pad_inches=0.2)     plt.close()          # Save as CSV     csv_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.csv\")     df_stats.to_csv(csv_output_path, index=False)          return table_output_path, csv_output_path"},{"location":"api/probability-analysis/New%20folder/graph-utils.html","title":"Graph utils","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nGraph construction, shortest path calculation, and path analysis utilities.\n\"\"\"\n\nimport time\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csr_matrix\nfrom scipy.sparse.csgraph import dijkstra\nfrom tqdm import tqdm\nfrom numpy import sqrt\n\nfrom raster_utils import to_1d\n\n\ndef build_adjacency_matrix(cost_array, rows, cols):\n    \"\"\"\n    Build an adjacency matrix (graph) from a cost raster.\n    \n    Args:\n        cost_array (numpy.ndarray): Cost array (bands, rows, cols)\n        rows (int): Number of rows\n        cols (int): Number of columns\n        \n    Returns:\n        scipy.sparse.csr_matrix: CSR format adjacency matrix\n    \"\"\"\n    print(\"Building adjacency matrix...\")\n    graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)\n    \n    # Define 8 movement directions and map them to band indices\n    directions = [\n        (-1, 0),   # Up\n        (1, 0),    # Down\n        (0, 1),    # Right\n        (0, -1),   # Left\n        (-1, 1),   # Up-Right\n        (-1, -1),  # Up-Left\n        (1, 1),    # Down-Right\n        (1, -1)    # Down-Left\n    ]\n    direction_indices = {dir: idx for idx, dir in enumerate(directions)}\n    \n    for r in tqdm(range(rows), desc=\"Processing rows\"):\n        for c in range(cols):\n            current_node = to_1d(r, c, cols)\n            for dr, dc in directions:\n                nr, nc = r + dr, c + dc\n                if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:\n                    neighbor_node = to_1d(nr, nc, cols)\n                    idx = direction_indices[(dr, dc)]\n                    cost_val = cost_array[idx, r, c]\n                    \n                    # Skip invalid costs\n                    if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:\n                        continue\n                    \n                    # Apply sqrt(2) factor for diagonal movement\n                    if abs(dr) == 1 and abs(dc) == 1:\n                        cost_val *= sqrt(2)\n                    \n                    graph[current_node, neighbor_node] = cost_val\n    \n    # Convert to CSR format for faster computation\n    graph_csr = graph.tocsr()\n    print(\"Adjacency matrix created.\")\n    \n    return graph_csr\n\n\ndef compute_shortest_paths(graph_csr, source_nodes):\n    \"\"\"\n    Compute shortest paths from source nodes using Dijkstra's algorithm.\n    \n    Args:\n        graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix\n        source_nodes (list): List of source node indices\n        \n    Returns:\n        tuple: (distances, predecessors)\n    \"\"\"\n    print(\"Running Dijkstra's algorithm for all sources...\")\n    start_time = time.time()\n    distances, predecessors = dijkstra(\n        csgraph=graph_csr, \n        directed=True, \n        indices=source_nodes, \n        return_predecessors=True\n    )\n    end_time = time.time()\n    print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds.\")\n    \n    return distances, predecessors\n\n\ndef reconstruct_path(predecessors, source_node, target_node, cols):\n    \"\"\"\n    Reconstruct the path from source_node to target_node using the predecessor array.\n    \n    Args:\n        predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm\n        source_node (int): Source node index\n        target_node (int): Target node index\n        cols (int): Number of columns in the raster\n        \n    Returns:\n        list: List of (row, col) coordinates for the path\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return []\n    \n    path_nodes = []\n    current = target_node\n    \n    while current != source_node and current != -9999:\n        path_nodes.append(current)\n        current = predecessors[current]\n    \n    path_nodes.append(source_node)\n    path_nodes.reverse()\n    \n    # Convert each 1D index to (row, col)\n    return [(node // cols, node % cols) for node in path_nodes]\n\n\ndef calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):\n    \"\"\"\n    Calculate metrics for a path between source and target nodes.\n    \n    Args:\n        predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm\n        source_node (int): Source node index\n        target_node (int): Target node index\n        graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix\n        rows (int): Number of rows in the raster\n        cols (int): Number of columns in the raster\n        \n    Returns:\n        tuple: (pixel_count, cell_costs, total_cost)\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return 0, [], 0\n    \n    path = []\n    current = target_node\n    \n    while current != source_node and current != -9999:\n        path.append(current)\n        current = predecessors[current]\n    \n    if current != -9999:\n        path.append(source_node)\n    \n    path.reverse()\n    pixel_count = len(path)\n    \n    cell_costs = []\n    total_cost = 0\n    \n    for i in range(len(path)-1):\n        cell_cost = graph_csr[path[i], path[i+1]]\n        cell_costs.append(cell_cost)\n        total_cost += cell_cost\n    \n    return pixel_count, cell_costs, total_cost\n</pre> \"\"\" Graph construction, shortest path calculation, and path analysis utilities. \"\"\"  import time import numpy as np from scipy.sparse import lil_matrix, csr_matrix from scipy.sparse.csgraph import dijkstra from tqdm import tqdm from numpy import sqrt  from raster_utils import to_1d   def build_adjacency_matrix(cost_array, rows, cols):     \"\"\"     Build an adjacency matrix (graph) from a cost raster.          Args:         cost_array (numpy.ndarray): Cost array (bands, rows, cols)         rows (int): Number of rows         cols (int): Number of columns              Returns:         scipy.sparse.csr_matrix: CSR format adjacency matrix     \"\"\"     print(\"Building adjacency matrix...\")     graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)          # Define 8 movement directions and map them to band indices     directions = [         (-1, 0),   # Up         (1, 0),    # Down         (0, 1),    # Right         (0, -1),   # Left         (-1, 1),   # Up-Right         (-1, -1),  # Up-Left         (1, 1),    # Down-Right         (1, -1)    # Down-Left     ]     direction_indices = {dir: idx for idx, dir in enumerate(directions)}          for r in tqdm(range(rows), desc=\"Processing rows\"):         for c in range(cols):             current_node = to_1d(r, c, cols)             for dr, dc in directions:                 nr, nc = r + dr, c + dc                 if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:                     neighbor_node = to_1d(nr, nc, cols)                     idx = direction_indices[(dr, dc)]                     cost_val = cost_array[idx, r, c]                                          # Skip invalid costs                     if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:                         continue                                          # Apply sqrt(2) factor for diagonal movement                     if abs(dr) == 1 and abs(dc) == 1:                         cost_val *= sqrt(2)                                          graph[current_node, neighbor_node] = cost_val          # Convert to CSR format for faster computation     graph_csr = graph.tocsr()     print(\"Adjacency matrix created.\")          return graph_csr   def compute_shortest_paths(graph_csr, source_nodes):     \"\"\"     Compute shortest paths from source nodes using Dijkstra's algorithm.          Args:         graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix         source_nodes (list): List of source node indices              Returns:         tuple: (distances, predecessors)     \"\"\"     print(\"Running Dijkstra's algorithm for all sources...\")     start_time = time.time()     distances, predecessors = dijkstra(         csgraph=graph_csr,          directed=True,          indices=source_nodes,          return_predecessors=True     )     end_time = time.time()     print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds.\")          return distances, predecessors   def reconstruct_path(predecessors, source_node, target_node, cols):     \"\"\"     Reconstruct the path from source_node to target_node using the predecessor array.          Args:         predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm         source_node (int): Source node index         target_node (int): Target node index         cols (int): Number of columns in the raster              Returns:         list: List of (row, col) coordinates for the path     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return []          path_nodes = []     current = target_node          while current != source_node and current != -9999:         path_nodes.append(current)         current = predecessors[current]          path_nodes.append(source_node)     path_nodes.reverse()          # Convert each 1D index to (row, col)     return [(node // cols, node % cols) for node in path_nodes]   def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):     \"\"\"     Calculate metrics for a path between source and target nodes.          Args:         predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm         source_node (int): Source node index         target_node (int): Target node index         graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix         rows (int): Number of rows in the raster         cols (int): Number of columns in the raster              Returns:         tuple: (pixel_count, cell_costs, total_cost)     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return 0, [], 0          path = []     current = target_node          while current != source_node and current != -9999:         path.append(current)         current = predecessors[current]          if current != -9999:         path.append(source_node)          path.reverse()     pixel_count = len(path)          cell_costs = []     total_cost = 0          for i in range(len(path)-1):         cell_cost = graph_csr[path[i], path[i+1]]         cell_costs.append(cell_cost)         total_cost += cell_cost          return pixel_count, cell_costs, total_cost"},{"location":"api/probability-analysis/New%20folder/prob_visualization.html","title":"Prob visualization","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nModified plotting and visualization functions for the volcanic evacuation analysis.\nHandles multiple VEI levels and creates separate contour maps.\n\"\"\"\n</pre> \"\"\" Modified plotting and visualization functions for the volcanic evacuation analysis. Handles multiple VEI levels and creates separate contour maps. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors as mcolors\nfrom matplotlib.patheffects import withStroke\nimport rasterio\nfrom rasterio.warp import reproject, Resampling\nfrom pyproj import CRS, Transformer\nimport contextily as ctx\nfrom shapely.geometry import box\nimport geopandas as gpd\n</pre> import os import numpy as np import matplotlib.pyplot as plt from matplotlib import colors as mcolors from matplotlib.patheffects import withStroke import rasterio from rasterio.warp import reproject, Resampling from pyproj import CRS, Transformer import contextily as ctx from shapely.geometry import box import geopandas as gpd In\u00a0[\u00a0]: Copied! <pre>from raster_utils import raster_coord_to_map_coords\nfrom data_utils import load_raster, read_shapefile\n</pre> from raster_utils import raster_coord_to_map_coords from data_utils import load_raster, read_shapefile In\u00a0[\u00a0]: Copied! <pre>def plot_travel_time_comparison(all_results, source_names, thresholds, walking_speeds, output_dir, filename=\"comparison_travel_time_by_source.png\"):\n    \"\"\"\n    Plot a comparison of travel times between datasets.\n    \n    Args:\n        all_results (dict): Dictionary of results from all datasets\n        source_names (list): List of source names\n        thresholds (list): List of probability thresholds\n        walking_speeds (dict): Dictionary of walking speeds\n        output_dir (str): Directory to save the plot\n        filename (str): Filename for the plot\n        \n    Returns:\n        str: Path to the saved plot\n    \"\"\"\n    dataset_styles = {\n        'final': {'label': 'Original', 'linestyle': '-', 'alpha': 0.9},\n        'modify_landcover': {'label': 'Penalized', 'linestyle': '--', 'alpha': 0.9}\n    }\n    \n    speed_markers = {'slow': 'o', 'medium': 's', 'fast': '^'}\n    speed_colors = {'slow': 'red', 'medium': 'blue', 'fast': 'green'}\n    \n    # Create figure with one row, two columns\n    n_sources = min(len(source_names), 2)  # Limit to first two sources\n    fig, axes = plt.subplots(1, n_sources, figsize=(16, 6), sharey=True)\n    \n    # Handle case of single subplot\n    if n_sources == 1:\n        axes = [axes]\n    \n    # Store lines and labels for legend\n    lines = []\n    labels = []\n    first_plot = True\n    \n    # Plot for each source\n    for src_idx, source_name in enumerate(source_names[:n_sources]):\n        ax = axes[src_idx]\n        \n        # Plot all walking speeds for both original and modified landcover\n        for speed_name, speed_color in speed_colors.items():\n            # Get data for both datasets\n            original_times = [all_results['final'][speed_name][thresh][src_idx] for thresh in thresholds]\n            modified_times = [all_results['modify_landcover'][speed_name][thresh][src_idx] for thresh in thresholds]\n            \n            # Plot with consistent line styles\n            line1 = ax.plot(thresholds, original_times, '-', color=speed_color)\n            line2 = ax.plot(thresholds, modified_times, '--', color=speed_color)\n            \n            # Store legend entries only once\n            if first_plot:\n                lines.extend([line1[0], line2[0]])\n                labels.extend([f'{speed_name.capitalize()} - Original', \n                             f'{speed_name.capitalize()} - Penalized'])\n        \n        # Add label in the top left corner\n        ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes, \n                fontsize=14, fontweight='bold', va='top')\n        \n        # Invert the x-axis to show high probabilities first\n        ax.invert_xaxis()\n        \n        ax.set_xlabel(\"Eruption Probability Threshold\", fontsize=10)\n        if src_idx == 0:  # Add ylabel only for leftmost plot\n            ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)\n        ax.grid(True, alpha=0.3)\n        \n        # Set y-axis to go from 0 to 2.5\n        ax.set_ylim(bottom=0, top=2.5)\n        \n        first_plot = False\n    \n    # Add legend to the right of the second subplot\n    fig.legend(lines, labels, fontsize=9, loc='center left', \n              bbox_to_anchor=(1.02, 0.5))\n    \n    plt.tight_layout()\n    \n    # Save the plot\n    output_path = os.path.join(output_dir, filename)\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show() \n    plt.close()\n    return output_path\n</pre> def plot_travel_time_comparison(all_results, source_names, thresholds, walking_speeds, output_dir, filename=\"comparison_travel_time_by_source.png\"):     \"\"\"     Plot a comparison of travel times between datasets.          Args:         all_results (dict): Dictionary of results from all datasets         source_names (list): List of source names         thresholds (list): List of probability thresholds         walking_speeds (dict): Dictionary of walking speeds         output_dir (str): Directory to save the plot         filename (str): Filename for the plot              Returns:         str: Path to the saved plot     \"\"\"     dataset_styles = {         'final': {'label': 'Original', 'linestyle': '-', 'alpha': 0.9},         'modify_landcover': {'label': 'Penalized', 'linestyle': '--', 'alpha': 0.9}     }          speed_markers = {'slow': 'o', 'medium': 's', 'fast': '^'}     speed_colors = {'slow': 'red', 'medium': 'blue', 'fast': 'green'}          # Create figure with one row, two columns     n_sources = min(len(source_names), 2)  # Limit to first two sources     fig, axes = plt.subplots(1, n_sources, figsize=(16, 6), sharey=True)          # Handle case of single subplot     if n_sources == 1:         axes = [axes]          # Store lines and labels for legend     lines = []     labels = []     first_plot = True          # Plot for each source     for src_idx, source_name in enumerate(source_names[:n_sources]):         ax = axes[src_idx]                  # Plot all walking speeds for both original and modified landcover         for speed_name, speed_color in speed_colors.items():             # Get data for both datasets             original_times = [all_results['final'][speed_name][thresh][src_idx] for thresh in thresholds]             modified_times = [all_results['modify_landcover'][speed_name][thresh][src_idx] for thresh in thresholds]                          # Plot with consistent line styles             line1 = ax.plot(thresholds, original_times, '-', color=speed_color)             line2 = ax.plot(thresholds, modified_times, '--', color=speed_color)                          # Store legend entries only once             if first_plot:                 lines.extend([line1[0], line2[0]])                 labels.extend([f'{speed_name.capitalize()} - Original',                               f'{speed_name.capitalize()} - Penalized'])                  # Add label in the top left corner         ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes,                  fontsize=14, fontweight='bold', va='top')                  # Invert the x-axis to show high probabilities first         ax.invert_xaxis()                  ax.set_xlabel(\"Eruption Probability Threshold\", fontsize=10)         if src_idx == 0:  # Add ylabel only for leftmost plot             ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)         ax.grid(True, alpha=0.3)                  # Set y-axis to go from 0 to 2.5         ax.set_ylim(bottom=0, top=2.5)                  first_plot = False          # Add legend to the right of the second subplot     fig.legend(lines, labels, fontsize=9, loc='center left',                bbox_to_anchor=(1.02, 0.5))          plt.tight_layout()          # Save the plot     output_path = os.path.join(output_dir, filename)     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.show()      plt.close()     return output_path In\u00a0[\u00a0]: Copied! <pre>def plot_cost_surface_with_paths(dataset_info, evacuation_paths, eruption_probability_path, \n                                  hiking_path, selected_speed, thresholds, output_dir, vei_label=\"VEI4\"):\n    \"\"\"\n    Plot cost surface with evacuation paths.\n    \n    Args:\n        dataset_info (dict): Dictionary of dataset information\n        evacuation_paths (dict): Dictionary of evacuation paths\n        eruption_probability_path (str): Path to the eruption probability raster\n        hiking_path (str): Path to the hiking path shapefile\n        selected_speed (str): Selected walking speed\n        thresholds (list): List of probability thresholds\n        output_dir (str): Directory to save the plot\n        vei_label (str): VEI label for the title\n        \n    Returns:\n        str: Path to the saved plot\n    \"\"\"\n    print(f\"\\nCreating cost surface subplots with evacuation paths for {vei_label}...\")\n    \n    # Set up colors for each probability threshold\n    threshold_colors = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange']\n    \n    # Load hiking trail shapefile\n    hiking_gdf = read_shapefile(hiking_path)\n    \n    # Load cost rasters for visualization\n    cost_arrays = []\n    transforms = []\n    for ds_key in dataset_info.keys():\n        cost_raster_path = os.path.join(output_dir, f'cost_distance_summit_{ds_key}_{selected_speed}_hours.tif')\n        cost_array, cost_meta = load_raster(cost_raster_path)\n        cost_arrays.append(cost_array)\n        transforms.append(cost_meta['transform'])\n    \n    # Create figure with larger size \n    fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4\n    \n    # Create subplot grid with space for legend and colorbar\n    gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    \n    # Create a common colormap and normalization\n    cmap = plt.cm.RdYlBu_r.copy()\n    cmap.set_bad('white', alpha=0)\n    \n    epsilon = 0.1\n    max_log_cost = np.log1p(10 + epsilon)\n    \n    # Find global vmin and vmax for consistent color scaling\n    vmin = float('inf')\n    vmax = float('-inf')\n    for cost_array in cost_arrays:\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))\n        vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))\n    vcenter = vmin + (vmax - vmin) / 2\n    \n    norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n    \n    # Add title for the overall figure\n    fig.suptitle(f'Contour Map Showing Summit Location and Hiking Trail for {vei_label}', fontsize=12)\n    \n    # Plot both subplots\n    for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):\n        cost_array = cost_arrays[idx]\n        transform = transforms[idx]\n        \n        # Process cost array\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        \n        # Plot cost surface\n        im = ax.imshow(\n            cost_array_masked, cmap=cmap, norm=norm,\n            extent=[\n                transform[2], \n                transform[2] + transform[0] * cost_array.shape[1],\n                transform[5] + transform[4] * cost_array.shape[0],\n                transform[5]\n            ]\n        )\n        \n        # Plot evacuation paths\n        for i, thresh in enumerate(thresholds):\n            if thresh in evacuation_paths[ds_key]:\n                path = evacuation_paths[ds_key][thresh]\n                if path:\n                    path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]\n                    xs, ys = zip(*path_coords)\n                    ax.plot(xs, ys, '-', color=threshold_colors[i % len(threshold_colors)], \n                            linewidth=3, label=f'Prob \u2264 {thresh}',\n                            alpha=1.0, zorder=5)\n        \n        # Plot hiking trail with outline effect\n        hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)\n        ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')\n        plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])\n        \n        # Plot summit\n        summit_rc = dataset_info[ds_key][\"summit_raster_coords\"]\n        summit_x, summit_y = raster_coord_to_map_coords(\n            summit_rc[0], summit_rc[1], transform\n        )\n        ax.plot(summit_x, summit_y, '*', color='blue', markersize=15, label='Summit', zorder=6)\n        \n        # Add probability contours\n        x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])\n        y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])\n        X, Y = np.meshgrid(x, y)\n        \n        # Load eruption probability raster\n        with rasterio.open(eruption_probability_path) as src:\n            prob_array = src.read(1)\n            prob_transform = src.transform\n        \n        # Ensure probability array matches cost array dimensions\n        if prob_array.shape != cost_array.shape:\n            print(f\"Reshaping probability array for {vei_label} to match cost array dimensions...\")\n            new_prob_array = np.empty(cost_array.shape, dtype=np.float32)\n            reproject(\n                source=prob_array,\n                destination=new_prob_array,\n                src_transform=prob_transform,\n                dst_transform=transform,\n                src_crs=CRS.from_epsg(32751),\n                dst_crs=CRS.from_epsg(32751),\n                resampling=Resampling.bilinear\n            )\n            prob_array = new_prob_array\n        \n        # Define contour levels in ASCENDING order (critical)\n        contour_levels = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]  # Sorted in ascending order\n        \n        # Create contours for probability thresholds\n        contours = ax.contour(X, Y, prob_array, levels=contour_levels,\n                            colors='black', linestyles='--', alpha=0.7,\n                            linewidths=1.5, zorder=3)\n        ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)\n        \n        # Set plot limits based on summit location (adjust as needed for different VEI levels)\n        # For VEI3, use a smaller radius, for VEI5 use a larger radius\n        if vei_label == 'VEI3':\n            radius = 3500\n        elif vei_label == 'VEI5':\n            radius = 7000\n        else:  # VEI4 default\n            radius = 5000\n            \n        ax.set_xlim(summit_x - radius, summit_x + radius)\n        ax.set_ylim(summit_y - radius, summit_y + radius)\n        \n        # Add legend with blue Summit star and red Hiking Path\n        ax.legend(loc='upper right', fontsize=8)\n        \n        # Add axis labels\n        ax.set_xlabel('Easting (m)')\n        ax.set_ylabel('Northing (m)')\n    \n    # Add horizontal colorbar\n    cbar_ax = fig.add_subplot(gs[1, :])\n    cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n    cbar.set_label('Accumulated Cost', labelpad=10)\n    \n    cbar.set_ticks(np.linspace(vmin, vmax, 6))\n    cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])\n    \n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.2, top=0.92)  # Make room for suptitle\n    \n    # Save the plot\n    output_path = os.path.join(output_dir, f'contour_map_showing_summit_location_and_hiking_trail_for_{vei_label}.png')\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show() \n    plt.close()\n    print(f\"Cost surface visualization with evacuation paths for {vei_label} saved to: {output_path}\")\n    return output_path\n</pre> def plot_cost_surface_with_paths(dataset_info, evacuation_paths, eruption_probability_path,                                    hiking_path, selected_speed, thresholds, output_dir, vei_label=\"VEI4\"):     \"\"\"     Plot cost surface with evacuation paths.          Args:         dataset_info (dict): Dictionary of dataset information         evacuation_paths (dict): Dictionary of evacuation paths         eruption_probability_path (str): Path to the eruption probability raster         hiking_path (str): Path to the hiking path shapefile         selected_speed (str): Selected walking speed         thresholds (list): List of probability thresholds         output_dir (str): Directory to save the plot         vei_label (str): VEI label for the title              Returns:         str: Path to the saved plot     \"\"\"     print(f\"\\nCreating cost surface subplots with evacuation paths for {vei_label}...\")          # Set up colors for each probability threshold     threshold_colors = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange']          # Load hiking trail shapefile     hiking_gdf = read_shapefile(hiking_path)          # Load cost rasters for visualization     cost_arrays = []     transforms = []     for ds_key in dataset_info.keys():         cost_raster_path = os.path.join(output_dir, f'cost_distance_summit_{ds_key}_{selected_speed}_hours.tif')         cost_array, cost_meta = load_raster(cost_raster_path)         cost_arrays.append(cost_array)         transforms.append(cost_meta['transform'])          # Create figure with larger size      fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4          # Create subplot grid with space for legend and colorbar     gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)     ax1 = fig.add_subplot(gs[0, 0])     ax2 = fig.add_subplot(gs[0, 1])          # Create a common colormap and normalization     cmap = plt.cm.RdYlBu_r.copy()     cmap.set_bad('white', alpha=0)          epsilon = 0.1     max_log_cost = np.log1p(10 + epsilon)          # Find global vmin and vmax for consistent color scaling     vmin = float('inf')     vmax = float('-inf')     for cost_array in cost_arrays:         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)         vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))         vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))     vcenter = vmin + (vmax - vmin) / 2          norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)          # Add title for the overall figure     fig.suptitle(f'Contour Map Showing Summit Location and Hiking Trail for {vei_label}', fontsize=12)          # Plot both subplots     for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):         cost_array = cost_arrays[idx]         transform = transforms[idx]                  # Process cost array         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)                  # Plot cost surface         im = ax.imshow(             cost_array_masked, cmap=cmap, norm=norm,             extent=[                 transform[2],                  transform[2] + transform[0] * cost_array.shape[1],                 transform[5] + transform[4] * cost_array.shape[0],                 transform[5]             ]         )                  # Plot evacuation paths         for i, thresh in enumerate(thresholds):             if thresh in evacuation_paths[ds_key]:                 path = evacuation_paths[ds_key][thresh]                 if path:                     path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]                     xs, ys = zip(*path_coords)                     ax.plot(xs, ys, '-', color=threshold_colors[i % len(threshold_colors)],                              linewidth=3, label=f'Prob \u2264 {thresh}',                             alpha=1.0, zorder=5)                  # Plot hiking trail with outline effect         hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)         ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')         plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])                  # Plot summit         summit_rc = dataset_info[ds_key][\"summit_raster_coords\"]         summit_x, summit_y = raster_coord_to_map_coords(             summit_rc[0], summit_rc[1], transform         )         ax.plot(summit_x, summit_y, '*', color='blue', markersize=15, label='Summit', zorder=6)                  # Add probability contours         x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])         y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])         X, Y = np.meshgrid(x, y)                  # Load eruption probability raster         with rasterio.open(eruption_probability_path) as src:             prob_array = src.read(1)             prob_transform = src.transform                  # Ensure probability array matches cost array dimensions         if prob_array.shape != cost_array.shape:             print(f\"Reshaping probability array for {vei_label} to match cost array dimensions...\")             new_prob_array = np.empty(cost_array.shape, dtype=np.float32)             reproject(                 source=prob_array,                 destination=new_prob_array,                 src_transform=prob_transform,                 dst_transform=transform,                 src_crs=CRS.from_epsg(32751),                 dst_crs=CRS.from_epsg(32751),                 resampling=Resampling.bilinear             )             prob_array = new_prob_array                  # Define contour levels in ASCENDING order (critical)         contour_levels = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]  # Sorted in ascending order                  # Create contours for probability thresholds         contours = ax.contour(X, Y, prob_array, levels=contour_levels,                             colors='black', linestyles='--', alpha=0.7,                             linewidths=1.5, zorder=3)         ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)                  # Set plot limits based on summit location (adjust as needed for different VEI levels)         # For VEI3, use a smaller radius, for VEI5 use a larger radius         if vei_label == 'VEI3':             radius = 3500         elif vei_label == 'VEI5':             radius = 7000         else:  # VEI4 default             radius = 5000                      ax.set_xlim(summit_x - radius, summit_x + radius)         ax.set_ylim(summit_y - radius, summit_y + radius)                  # Add legend with blue Summit star and red Hiking Path         ax.legend(loc='upper right', fontsize=8)                  # Add axis labels         ax.set_xlabel('Easting (m)')         ax.set_ylabel('Northing (m)')          # Add horizontal colorbar     cbar_ax = fig.add_subplot(gs[1, :])     cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')     cbar.set_label('Accumulated Cost', labelpad=10)          cbar.set_ticks(np.linspace(vmin, vmax, 6))     cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])          plt.tight_layout()     plt.subplots_adjust(hspace=0.2, top=0.92)  # Make room for suptitle          # Save the plot     output_path = os.path.join(output_dir, f'contour_map_showing_summit_location_and_hiking_trail_for_{vei_label}.png')     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.show()      plt.close()     print(f\"Cost surface visualization with evacuation paths for {vei_label} saved to: {output_path}\")     return output_path In\u00a0[\u00a0]: Copied! <pre>def create_vei_comparison_plot(eruption_probability_paths, hiking_path, summit_path, output_dir):\n    \"\"\"\n    Create a comparison plot of contour maps for different VEI levels side by side.\n    \n    Args:\n        eruption_probability_paths (dict): Dictionary of VEI levels and their file paths\n        hiking_path (str): Path to the hiking path shapefile\n        summit_path (str): Path to the summit point shapefile\n        output_dir (str): Directory to save the plot\n        \n    Returns:\n        str: Path to the saved plot\n    \"\"\"\n    print(\"\\nCreating VEI comparison plot...\")\n    \n    # Load hiking trail shapefile\n    hiking_gdf = read_shapefile(hiking_path)\n    \n    # Load summit points\n    summit_gdf = read_shapefile(summit_path)\n    \n    # Create figure with subplots for each VEI level\n    fig, axes = plt.subplots(1, len(eruption_probability_paths), figsize=(18, 6), sharey=True)\n    \n    # If only one VEI level, convert axes to list\n    if len(eruption_probability_paths) == 1:\n        axes = [axes]\n    \n    # Define eruption probability thresholds - CRITICAL: Must be in ASCENDING order for matplotlib contour\n    thresholds = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\n    \n    # Process each VEI level\n    for i, (vei, path) in enumerate(eruption_probability_paths.items()):\n        ax = axes[i]\n        \n        # Load eruption probability raster\n        with rasterio.open(path) as src:\n            prob_array = src.read(1)\n            transform = src.transform\n            bounds = src.bounds\n            \n            # Get CRS information for the raster\n            raster_crs = src.crs\n            \n            # Create coordinate grid\n            x = np.linspace(bounds.left, bounds.right, prob_array.shape[1])\n            y = np.linspace(bounds.bottom, bounds.top, prob_array.shape[0])\n            X, Y = np.meshgrid(x, y)\n            \n            # Get summit coordinates\n            summit_x = summit_gdf.geometry.x.values[0]\n            summit_y = summit_gdf.geometry.y.values[0]\n            \n            # Set radius based on VEI level\n            if vei == 'VEI3':\n                radius = 3500\n            elif vei == 'VEI5':\n                radius = 7000\n            else:  # VEI4 default\n                radius = 5000\n            \n            # Create a bounding box for the area around the summit\n            bbox = box(summit_x - radius, summit_y - radius, \n                      summit_x + radius, summit_y + radius)\n            \n            # Create a GeoDataFrame from the bounding box\n            bbox_gdf = gpd.GeoDataFrame({'geometry': [bbox]}, crs=raster_crs)\n            \n            # Create a light blue background for the map\n            ax.imshow(np.ones(prob_array.shape), \n                     extent=[bounds.left, bounds.right, bounds.bottom, bounds.top],\n                     cmap='Blues', alpha=0.1, vmin=0, vmax=1)\n            \n            # Plot the OpenStreetMap tiles (wrapped in try/except in case of connection issues)\n            try:\n                ctx.add_basemap(ax, crs=raster_crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n                print(f\"Successfully added OpenStreetMap background for {vei}\")\n            except Exception as e:\n                print(f\"Warning: Could not add OpenStreetMap basemap for {vei}. Error: {e}\")\n                # Continue without the basemap\n            \n            # Create contours for probability thresholds (already in ascending order)\n            contours = ax.contour(X, Y, prob_array, levels=thresholds,\n                                colors='black', linestyles='--', alpha=0.7,\n                                linewidths=1.0)\n            ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)\n            \n            # Plot hiking trail\n            hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.0)\n            \n            # Plot summit\n            ax.plot(summit_x, summit_y, '*', color='blue', markersize=12)\n            \n            # Set title\n            ax.set_title(f'Contour Map Showing Summit Location and Hiking Trail for {vei}', fontsize=10)\n            \n            # Set plot limits\n            ax.set_xlim(summit_x - radius, summit_x + radius)\n            ax.set_ylim(summit_y - radius, summit_y + radius)\n            \n            # Add axis labels\n            ax.set_xlabel('Easting (m)')\n            if i == 0:  # Only add y-label for the first subplot\n                ax.set_ylabel('Northing (m)')\n            \n            # Add legend\n            ax.plot([], [], '*', color='blue', markersize=12, label='Summit')\n            ax.plot([], [], '-', color='red', linewidth=2.0, label='Hiking Path')\n            ax.legend(loc='upper right', fontsize=8)\n    \n    plt.tight_layout()\n    \n    # Save the plot\n    output_path = os.path.join(output_dir, 'vei_comparison_contour_maps.png')\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()\n    print(f\"VEI comparison plot saved to: {output_path}\")\n    return output_path\n</pre> def create_vei_comparison_plot(eruption_probability_paths, hiking_path, summit_path, output_dir):     \"\"\"     Create a comparison plot of contour maps for different VEI levels side by side.          Args:         eruption_probability_paths (dict): Dictionary of VEI levels and their file paths         hiking_path (str): Path to the hiking path shapefile         summit_path (str): Path to the summit point shapefile         output_dir (str): Directory to save the plot              Returns:         str: Path to the saved plot     \"\"\"     print(\"\\nCreating VEI comparison plot...\")          # Load hiking trail shapefile     hiking_gdf = read_shapefile(hiking_path)          # Load summit points     summit_gdf = read_shapefile(summit_path)          # Create figure with subplots for each VEI level     fig, axes = plt.subplots(1, len(eruption_probability_paths), figsize=(18, 6), sharey=True)          # If only one VEI level, convert axes to list     if len(eruption_probability_paths) == 1:         axes = [axes]          # Define eruption probability thresholds - CRITICAL: Must be in ASCENDING order for matplotlib contour     thresholds = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]          # Process each VEI level     for i, (vei, path) in enumerate(eruption_probability_paths.items()):         ax = axes[i]                  # Load eruption probability raster         with rasterio.open(path) as src:             prob_array = src.read(1)             transform = src.transform             bounds = src.bounds                          # Get CRS information for the raster             raster_crs = src.crs                          # Create coordinate grid             x = np.linspace(bounds.left, bounds.right, prob_array.shape[1])             y = np.linspace(bounds.bottom, bounds.top, prob_array.shape[0])             X, Y = np.meshgrid(x, y)                          # Get summit coordinates             summit_x = summit_gdf.geometry.x.values[0]             summit_y = summit_gdf.geometry.y.values[0]                          # Set radius based on VEI level             if vei == 'VEI3':                 radius = 3500             elif vei == 'VEI5':                 radius = 7000             else:  # VEI4 default                 radius = 5000                          # Create a bounding box for the area around the summit             bbox = box(summit_x - radius, summit_y - radius,                        summit_x + radius, summit_y + radius)                          # Create a GeoDataFrame from the bounding box             bbox_gdf = gpd.GeoDataFrame({'geometry': [bbox]}, crs=raster_crs)                          # Create a light blue background for the map             ax.imshow(np.ones(prob_array.shape),                       extent=[bounds.left, bounds.right, bounds.bottom, bounds.top],                      cmap='Blues', alpha=0.1, vmin=0, vmax=1)                          # Plot the OpenStreetMap tiles (wrapped in try/except in case of connection issues)             try:                 ctx.add_basemap(ax, crs=raster_crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)                 print(f\"Successfully added OpenStreetMap background for {vei}\")             except Exception as e:                 print(f\"Warning: Could not add OpenStreetMap basemap for {vei}. Error: {e}\")                 # Continue without the basemap                          # Create contours for probability thresholds (already in ascending order)             contours = ax.contour(X, Y, prob_array, levels=thresholds,                                 colors='black', linestyles='--', alpha=0.7,                                 linewidths=1.0)             ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)                          # Plot hiking trail             hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.0)                          # Plot summit             ax.plot(summit_x, summit_y, '*', color='blue', markersize=12)                          # Set title             ax.set_title(f'Contour Map Showing Summit Location and Hiking Trail for {vei}', fontsize=10)                          # Set plot limits             ax.set_xlim(summit_x - radius, summit_x + radius)             ax.set_ylim(summit_y - radius, summit_y + radius)                          # Add axis labels             ax.set_xlabel('Easting (m)')             if i == 0:  # Only add y-label for the first subplot                 ax.set_ylabel('Northing (m)')                          # Add legend             ax.plot([], [], '*', color='blue', markersize=12, label='Summit')             ax.plot([], [], '-', color='red', linewidth=2.0, label='Hiking Path')             ax.legend(loc='upper right', fontsize=8)          plt.tight_layout()          # Save the plot     output_path = os.path.join(output_dir, 'vei_comparison_contour_maps.png')     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.show()     plt.close()     print(f\"VEI comparison plot saved to: {output_path}\")     return output_path"},{"location":"api/probability-analysis/New%20folder/probability_analysis.html","title":"Probability analysis","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nEvacuation path analysis, safe zone calculations, and related functionality.\n\"\"\"\n\nimport os\nimport numpy as np\nimport rasterio\n\nfrom data_utils import load_raster, save_raster, save_analysis_report, create_statistics_table\nfrom raster_utils import process_raster, to_1d\nfrom graph_utils import build_adjacency_matrix, compute_shortest_paths\n\n\ndef perform_evacuation_analysis(cost_paths, source_coords, source_names, walking_speeds, output_dir):\n    \"\"\"\n    Perform evacuation analysis for multiple cost datasets and walking speeds.\n    \n    Args:\n        cost_paths (dict): Dictionary of cost raster paths\n        source_coords (list): List of source (row, col) coordinates\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        output_dir (str): Directory to save outputs\n        \n    Returns:\n        tuple: (all_results, dataset_info)\n    \"\"\"\n    # Dictionary to store results\n    all_results = {}\n    dataset_info = {}\n    \n    # Process each cost dataset\n    for dataset_key, current_cost_path in cost_paths.items():\n        print(f\"\\nProcessing cost dataset: {dataset_key}\")\n        \n        # Read the cost raster\n        cost_array, cost_meta, transform, cost_nodata, raster_bounds, resolution = read_raster(current_cost_path)\n        print(f\"Raster shape: {cost_array.shape}\")\n        \n        # Get the dimensions of the raster\n        bands, rows, cols = cost_array.shape\n        \n        # Convert 2D source coordinates to 1D node indices\n        source_nodes = [to_1d(r, c, cols) for (r, c) in source_coords]\n        print(\"Source Nodes (1D):\", source_nodes)\n        \n        # Build the adjacency matrix\n        graph_csr = build_adjacency_matrix(cost_array, rows, cols)\n        \n        # Compute shortest paths\n        distances, predecessors = compute_shortest_paths(graph_csr, source_nodes)\n        \n        # Save the base cost distance rasters\n        for i, source_name in enumerate(source_names):\n            source_distance = distances[i, :].reshape(rows, cols)\n            source_distance[np.isinf(source_distance)] = -1\n            out_filename = f'cost_distance_{source_name}_{dataset_key}.tif'\n            out_path = os.path.join(output_dir, out_filename)\n            save_raster(out_path, source_distance, cost_meta, dtype=rasterio.float32, nodata=-1)\n            print(f\"Saved base cost distance raster for {source_name}: {out_filename}\")\n        \n        # Convert to travel time for each walking speed\n        for speed_name, speed_value in walking_speeds.items():\n            print(f\"\\nProcessing travel time rasters for walking speed '{speed_name}' ({speed_value} m/s)...\")\n            \n            for source_name in source_names:\n                base_raster_filename = f'cost_distance_{source_name}_{dataset_key}.tif'\n                base_raster_path = os.path.join(output_dir, base_raster_filename)\n                \n                cost_array_base, cost_meta_base = load_raster(base_raster_path)\n                travel_time_array = process_raster(cost_array_base, speed_value)\n                \n                out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n                out_path = os.path.join(output_dir, out_filename)\n                \n                save_raster(out_path, travel_time_array, cost_meta_base, dtype=rasterio.float32, nodata=-1)\n                print(f\"Saved travel time raster for {source_name} at speed '{speed_name}': {out_filename}\")\n        \n        # Store info for this dataset\n        summit_idx = 0  # Assuming summit is always the first source\n        dataset_info[dataset_key] = {\n            \"pred_summit\": predecessors[summit_idx],\n            \"rows\": rows,\n            \"cols\": cols,\n            \"transform\": transform,\n            \"summit_raster_coords\": source_coords[summit_idx]\n        }\n        \n        print(f\"Processing for dataset '{dataset_key}' complete!\")\n    \n    return all_results, dataset_info\n\n\ndef analyze_safe_zones(probability_path, travel_time_data, thresholds, source_names, walking_speeds, dataset_key, output_dir):\n    \"\"\"\n    Analyze safe zones based on eruption probability thresholds.\n    \n    Args:\n        probability_path (str): Path to the eruption probability raster\n        travel_time_data (dict): Dictionary of travel time data\n        thresholds (list): List of probability thresholds\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        dataset_key (str): Key for the dataset\n        output_dir (str): Directory to save outputs\n        \n    Returns:\n        tuple: (results, min_coords)\n    \"\"\"\n    print(\"\\nPerforming safe zone analysis based on eruption probability thresholds...\")\n    \n    # Load the eruption probability raster\n    with rasterio.open(probability_path) as src:\n        probability_array = src.read(1)\n    \n    # Initialize result dictionaries\n    results = {speed_name: {} for speed_name in walking_speeds.keys()}\n    min_coords = {speed_name: {} for speed_name in walking_speeds.keys()}\n    \n    # Analyze each walking speed\n    for speed_name in walking_speeds.keys():\n        print(f\"\\n--- Walking speed: {speed_name} ---\")\n        results[speed_name] = {}\n        \n        # Analyze each probability threshold\n        for thresh in thresholds:\n            print(f\"\\nEruption Probability Threshold: {thresh}\")\n            \n            # Flatten probability array for easier masking\n            probability_flat = probability_array.ravel()\n            \n            # Create safe zone mask where probability &lt;= threshold\n            safe_zone_mask = (probability_flat &lt;= thresh)\n            \n            min_times_in_zone = []\n            coords_in_zone = []\n            \n            # Find minimum travel time for each source\n            for source_name in source_names:\n                cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']\n                \n                # Find valid times within the safe zone\n                valid_times = cost_array_flat[safe_zone_mask]\n                valid_times = valid_times[~np.isnan(valid_times)]\n                \n                if len(valid_times) &gt; 0:\n                    min_time = np.min(valid_times)\n                    cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']\n                    safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)\n                    valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)\n                    min_idx = np.nanargmin(valid_times_2d)\n                    min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)\n                else:\n                    min_time = np.nan\n                    min_r, min_c = (np.nan, np.nan)\n                \n                min_times_in_zone.append(min_time)\n                coords_in_zone.append((min_r, min_c))\n                print(f\"{source_name}: min travel time = {min_time:.2f} hrs at cell ({min_r}, {min_c})\")\n            \n            results[speed_name][thresh] = min_times_in_zone\n            min_coords[speed_name][thresh] = coords_in_zone\n    \n    # Save analysis reports\n    report_path, csv_path = save_analysis_report(\n        results, min_coords, source_names, thresholds, \n        walking_speeds, dataset_key, output_dir\n    )\n    \n    # Create and save statistics table\n    table_path, stats_csv_path = create_statistics_table(\n        results, source_names, walking_speeds, \n        thresholds, dataset_key, output_dir\n    )\n    \n    print(f\"Saved analysis report: {report_path}\")\n    print(f\"Saved metrics CSV: {csv_path}\")\n    print(f\"Saved statistics table: {table_path}\")\n    print(f\"Saved statistics CSV: {stats_csv_path}\")\n    \n    return results, min_coords\n\n\ndef load_travel_time_data(dataset_key, source_names, walking_speeds, output_dir):\n    \"\"\"\n    Load travel time rasters for further analysis.\n    \n    Args:\n        dataset_key (str): Key for the dataset\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        output_dir (str): Directory where rasters are stored\n        \n    Returns:\n        dict: Dictionary of travel time data\n    \"\"\"\n    travel_time_data = {speed_name: {} for speed_name in walking_speeds.keys()}\n    \n    for speed_name in walking_speeds.keys():\n        for source_name in source_names:\n            raster_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n            raster_path = os.path.join(output_dir, raster_filename)\n            \n            cost_array_hours, meta_hours = load_raster(raster_path)\n            rows_rt, cols_rt = cost_array_hours.shape\n            cost_array_hours = np.where(cost_array_hours == meta_hours['nodata'], np.nan, cost_array_hours)\n            \n            travel_time_data[speed_name][source_name] = {\n                'cost_array': cost_array_hours,\n                'meta': meta_hours,\n                'shape': (rows_rt, cols_rt),\n                'cost_array_flat': cost_array_hours.ravel()\n            }\n        \n        print(f\"Loaded travel time data for speed '{speed_name}'\")\n    \n    return travel_time_data\n\n\ndef read_raster(path):\n    \"\"\"\n    Read a raster file and return its data, metadata, and properties.\n    \n    Args:\n        path (str): Path to the raster file\n        \n    Returns:\n        tuple: (data, meta, transform, nodata, bounds, resolution)\n    \"\"\"\n    with rasterio.open(path) as src:\n        data = src.read()\n        meta = src.meta.copy()\n        transform = src.transform\n        nodata = src.nodata\n        bounds = src.bounds\n        resolution = src.res\n    return data, meta, transform, nodata, bounds, resolution\n</pre> \"\"\" Evacuation path analysis, safe zone calculations, and related functionality. \"\"\"  import os import numpy as np import rasterio  from data_utils import load_raster, save_raster, save_analysis_report, create_statistics_table from raster_utils import process_raster, to_1d from graph_utils import build_adjacency_matrix, compute_shortest_paths   def perform_evacuation_analysis(cost_paths, source_coords, source_names, walking_speeds, output_dir):     \"\"\"     Perform evacuation analysis for multiple cost datasets and walking speeds.          Args:         cost_paths (dict): Dictionary of cost raster paths         source_coords (list): List of source (row, col) coordinates         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         output_dir (str): Directory to save outputs              Returns:         tuple: (all_results, dataset_info)     \"\"\"     # Dictionary to store results     all_results = {}     dataset_info = {}          # Process each cost dataset     for dataset_key, current_cost_path in cost_paths.items():         print(f\"\\nProcessing cost dataset: {dataset_key}\")                  # Read the cost raster         cost_array, cost_meta, transform, cost_nodata, raster_bounds, resolution = read_raster(current_cost_path)         print(f\"Raster shape: {cost_array.shape}\")                  # Get the dimensions of the raster         bands, rows, cols = cost_array.shape                  # Convert 2D source coordinates to 1D node indices         source_nodes = [to_1d(r, c, cols) for (r, c) in source_coords]         print(\"Source Nodes (1D):\", source_nodes)                  # Build the adjacency matrix         graph_csr = build_adjacency_matrix(cost_array, rows, cols)                  # Compute shortest paths         distances, predecessors = compute_shortest_paths(graph_csr, source_nodes)                  # Save the base cost distance rasters         for i, source_name in enumerate(source_names):             source_distance = distances[i, :].reshape(rows, cols)             source_distance[np.isinf(source_distance)] = -1             out_filename = f'cost_distance_{source_name}_{dataset_key}.tif'             out_path = os.path.join(output_dir, out_filename)             save_raster(out_path, source_distance, cost_meta, dtype=rasterio.float32, nodata=-1)             print(f\"Saved base cost distance raster for {source_name}: {out_filename}\")                  # Convert to travel time for each walking speed         for speed_name, speed_value in walking_speeds.items():             print(f\"\\nProcessing travel time rasters for walking speed '{speed_name}' ({speed_value} m/s)...\")                          for source_name in source_names:                 base_raster_filename = f'cost_distance_{source_name}_{dataset_key}.tif'                 base_raster_path = os.path.join(output_dir, base_raster_filename)                                  cost_array_base, cost_meta_base = load_raster(base_raster_path)                 travel_time_array = process_raster(cost_array_base, speed_value)                                  out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'                 out_path = os.path.join(output_dir, out_filename)                                  save_raster(out_path, travel_time_array, cost_meta_base, dtype=rasterio.float32, nodata=-1)                 print(f\"Saved travel time raster for {source_name} at speed '{speed_name}': {out_filename}\")                  # Store info for this dataset         summit_idx = 0  # Assuming summit is always the first source         dataset_info[dataset_key] = {             \"pred_summit\": predecessors[summit_idx],             \"rows\": rows,             \"cols\": cols,             \"transform\": transform,             \"summit_raster_coords\": source_coords[summit_idx]         }                  print(f\"Processing for dataset '{dataset_key}' complete!\")          return all_results, dataset_info   def analyze_safe_zones(probability_path, travel_time_data, thresholds, source_names, walking_speeds, dataset_key, output_dir):     \"\"\"     Analyze safe zones based on eruption probability thresholds.          Args:         probability_path (str): Path to the eruption probability raster         travel_time_data (dict): Dictionary of travel time data         thresholds (list): List of probability thresholds         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         dataset_key (str): Key for the dataset         output_dir (str): Directory to save outputs              Returns:         tuple: (results, min_coords)     \"\"\"     print(\"\\nPerforming safe zone analysis based on eruption probability thresholds...\")          # Load the eruption probability raster     with rasterio.open(probability_path) as src:         probability_array = src.read(1)          # Initialize result dictionaries     results = {speed_name: {} for speed_name in walking_speeds.keys()}     min_coords = {speed_name: {} for speed_name in walking_speeds.keys()}          # Analyze each walking speed     for speed_name in walking_speeds.keys():         print(f\"\\n--- Walking speed: {speed_name} ---\")         results[speed_name] = {}                  # Analyze each probability threshold         for thresh in thresholds:             print(f\"\\nEruption Probability Threshold: {thresh}\")                          # Flatten probability array for easier masking             probability_flat = probability_array.ravel()                          # Create safe zone mask where probability &lt;= threshold             safe_zone_mask = (probability_flat &lt;= thresh)                          min_times_in_zone = []             coords_in_zone = []                          # Find minimum travel time for each source             for source_name in source_names:                 cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']                                  # Find valid times within the safe zone                 valid_times = cost_array_flat[safe_zone_mask]                 valid_times = valid_times[~np.isnan(valid_times)]                                  if len(valid_times) &gt; 0:                     min_time = np.min(valid_times)                     cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']                     safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)                     valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)                     min_idx = np.nanargmin(valid_times_2d)                     min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)                 else:                     min_time = np.nan                     min_r, min_c = (np.nan, np.nan)                                  min_times_in_zone.append(min_time)                 coords_in_zone.append((min_r, min_c))                 print(f\"{source_name}: min travel time = {min_time:.2f} hrs at cell ({min_r}, {min_c})\")                          results[speed_name][thresh] = min_times_in_zone             min_coords[speed_name][thresh] = coords_in_zone          # Save analysis reports     report_path, csv_path = save_analysis_report(         results, min_coords, source_names, thresholds,          walking_speeds, dataset_key, output_dir     )          # Create and save statistics table     table_path, stats_csv_path = create_statistics_table(         results, source_names, walking_speeds,          thresholds, dataset_key, output_dir     )          print(f\"Saved analysis report: {report_path}\")     print(f\"Saved metrics CSV: {csv_path}\")     print(f\"Saved statistics table: {table_path}\")     print(f\"Saved statistics CSV: {stats_csv_path}\")          return results, min_coords   def load_travel_time_data(dataset_key, source_names, walking_speeds, output_dir):     \"\"\"     Load travel time rasters for further analysis.          Args:         dataset_key (str): Key for the dataset         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         output_dir (str): Directory where rasters are stored              Returns:         dict: Dictionary of travel time data     \"\"\"     travel_time_data = {speed_name: {} for speed_name in walking_speeds.keys()}          for speed_name in walking_speeds.keys():         for source_name in source_names:             raster_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'             raster_path = os.path.join(output_dir, raster_filename)                          cost_array_hours, meta_hours = load_raster(raster_path)             rows_rt, cols_rt = cost_array_hours.shape             cost_array_hours = np.where(cost_array_hours == meta_hours['nodata'], np.nan, cost_array_hours)                          travel_time_data[speed_name][source_name] = {                 'cost_array': cost_array_hours,                 'meta': meta_hours,                 'shape': (rows_rt, cols_rt),                 'cost_array_flat': cost_array_hours.ravel()             }                  print(f\"Loaded travel time data for speed '{speed_name}'\")          return travel_time_data   def read_raster(path):     \"\"\"     Read a raster file and return its data, metadata, and properties.          Args:         path (str): Path to the raster file              Returns:         tuple: (data, meta, transform, nodata, bounds, resolution)     \"\"\"     with rasterio.open(path) as src:         data = src.read()         meta = src.meta.copy()         transform = src.transform         nodata = src.nodata         bounds = src.bounds         resolution = src.res     return data, meta, transform, nodata, bounds, resolution"},{"location":"api/probability-analysis/New%20folder/raster-utils.html","title":"Raster utils","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nRaster operations and coordinate conversion utilities.\n\"\"\"\n\nimport numpy as np\nimport rasterio\nimport rasterio.warp\nfrom rasterio.transform import xy\n\n\ndef resample_raster(source_path, target_path, output_path, resampling_method=rasterio.warp.Resampling.bilinear):\n    \"\"\"\n    Resample a source raster to match the grid of a target raster.\n    \n    Args:\n        source_path (str): Path to the source raster\n        target_path (str): Path to the target raster (for grid reference)\n        output_path (str): Path to save the resampled raster\n        resampling_method: Resampling method to use\n        \n    Returns:\n        tuple: (resampled_array, resampled_meta)\n    \"\"\"\n    # Load source raster\n    with rasterio.open(source_path) as src:\n        source_array = src.read(1)  # Read the first band\n        source_meta = src.meta.copy()\n        source_nodata = src.nodata\n        source_transform = src.transform\n        source_crs = src.crs\n    \n    # Replace NoData values with np.nan\n    source_array = np.where(source_array == source_nodata, np.nan, source_array)\n    \n    # Get target grid properties\n    with rasterio.open(target_path) as src:\n        target_meta = src.meta.copy()\n        target_transform = src.transform\n        target_crs = src.crs\n        target_shape = src.shape\n    \n    # Set up the output metadata\n    resampled_meta = target_meta.copy()\n    resampled_meta.update({\n        'dtype': 'float32',\n        'count': 1,\n        'nodata': np.nan,\n        'driver': 'GTiff'\n    })\n    \n    # Create an empty array for the resampled data\n    resampled_array = np.empty(target_shape, dtype=np.float32)\n    \n    # Perform the resampling\n    rasterio.warp.reproject(\n        source=source_array,\n        destination=resampled_array,\n        src_transform=source_transform,\n        src_crs=source_crs,\n        dst_transform=target_transform,\n        dst_crs=target_crs,\n        resampling=resampling_method\n    )\n    \n    # Replace any remaining NoData values with np.nan\n    resampled_array = np.where(np.isnan(resampled_array), np.nan, resampled_array)\n    \n    # Save the resampled raster\n    with rasterio.open(output_path, 'w', **resampled_meta) as dst:\n        dst.write(resampled_array, 1)\n    \n    return resampled_array, resampled_meta\n\n\ndef coords_to_raster(gdf, transform, bounds, res):\n    \"\"\"\n    Convert geographic coordinates to raster row/column coordinates.\n    \n    Args:\n        gdf (GeoDataFrame): GeoDataFrame containing point geometries\n        transform: The raster's affine transform\n        bounds: The raster's bounds\n        res: The raster's resolution\n        \n    Returns:\n        list: List of (row, col) coordinates\n    \"\"\"\n    raster_coords = []\n    for point in gdf.geometry:\n        x, y = point.x, point.y\n        if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):\n            print(f\"Point {x}, {y} is out of raster bounds.\")\n            continue\n        col = int((x - bounds.left) / res[0])\n        row = int((bounds.top - y) / res[1])\n        raster_coords.append((row, col))\n    return raster_coords\n\n\ndef raster_coord_to_map_coords(row, col, transform):\n    \"\"\"\n    Convert raster (row, col) coordinates to map (x, y) coordinates.\n    \n    Args:\n        row (int): Raster row index\n        col (int): Raster column index\n        transform: Raster affine transform\n        \n    Returns:\n        tuple: (x, y) map coordinates\n    \"\"\"\n    x, y = xy(transform, row, col, offset='center')\n    return x, y\n\n\ndef to_1d(r, c, cols):\n    \"\"\"\n    Convert 2D (row, col) coordinates to 1D index.\n    \n    Args:\n        r (int): Row index\n        c (int): Column index\n        cols (int): Number of columns in the raster\n        \n    Returns:\n        int: 1D index\n    \"\"\"\n    return r * cols + c\n\n\ndef process_raster(cost_array, walking_speed, cell_size=100):\n    \"\"\"\n    Convert cost raster to travel time (in hours).\n    \n    Args:\n        cost_array (numpy.ndarray): Cost array\n        walking_speed (float): Walking speed in m/s\n        cell_size (float): Cell size in meters\n        \n    Returns:\n        numpy.ndarray: Travel time array in hours\n    \"\"\"\n    # Multiply by cell size to get distance\n    cost_array = cost_array * cell_size  \n    # Convert to seconds (m / (m/s) = s)\n    cost_array = cost_array / walking_speed  \n    # Convert seconds to hours\n    cost_array = cost_array / 3600  \n    # Replace infinite values with -1\n    cost_array[np.isinf(cost_array)] = -1\n    \n    return cost_array\n</pre> \"\"\" Raster operations and coordinate conversion utilities. \"\"\"  import numpy as np import rasterio import rasterio.warp from rasterio.transform import xy   def resample_raster(source_path, target_path, output_path, resampling_method=rasterio.warp.Resampling.bilinear):     \"\"\"     Resample a source raster to match the grid of a target raster.          Args:         source_path (str): Path to the source raster         target_path (str): Path to the target raster (for grid reference)         output_path (str): Path to save the resampled raster         resampling_method: Resampling method to use              Returns:         tuple: (resampled_array, resampled_meta)     \"\"\"     # Load source raster     with rasterio.open(source_path) as src:         source_array = src.read(1)  # Read the first band         source_meta = src.meta.copy()         source_nodata = src.nodata         source_transform = src.transform         source_crs = src.crs          # Replace NoData values with np.nan     source_array = np.where(source_array == source_nodata, np.nan, source_array)          # Get target grid properties     with rasterio.open(target_path) as src:         target_meta = src.meta.copy()         target_transform = src.transform         target_crs = src.crs         target_shape = src.shape          # Set up the output metadata     resampled_meta = target_meta.copy()     resampled_meta.update({         'dtype': 'float32',         'count': 1,         'nodata': np.nan,         'driver': 'GTiff'     })          # Create an empty array for the resampled data     resampled_array = np.empty(target_shape, dtype=np.float32)          # Perform the resampling     rasterio.warp.reproject(         source=source_array,         destination=resampled_array,         src_transform=source_transform,         src_crs=source_crs,         dst_transform=target_transform,         dst_crs=target_crs,         resampling=resampling_method     )          # Replace any remaining NoData values with np.nan     resampled_array = np.where(np.isnan(resampled_array), np.nan, resampled_array)          # Save the resampled raster     with rasterio.open(output_path, 'w', **resampled_meta) as dst:         dst.write(resampled_array, 1)          return resampled_array, resampled_meta   def coords_to_raster(gdf, transform, bounds, res):     \"\"\"     Convert geographic coordinates to raster row/column coordinates.          Args:         gdf (GeoDataFrame): GeoDataFrame containing point geometries         transform: The raster's affine transform         bounds: The raster's bounds         res: The raster's resolution              Returns:         list: List of (row, col) coordinates     \"\"\"     raster_coords = []     for point in gdf.geometry:         x, y = point.x, point.y         if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):             print(f\"Point {x}, {y} is out of raster bounds.\")             continue         col = int((x - bounds.left) / res[0])         row = int((bounds.top - y) / res[1])         raster_coords.append((row, col))     return raster_coords   def raster_coord_to_map_coords(row, col, transform):     \"\"\"     Convert raster (row, col) coordinates to map (x, y) coordinates.          Args:         row (int): Raster row index         col (int): Raster column index         transform: Raster affine transform              Returns:         tuple: (x, y) map coordinates     \"\"\"     x, y = xy(transform, row, col, offset='center')     return x, y   def to_1d(r, c, cols):     \"\"\"     Convert 2D (row, col) coordinates to 1D index.          Args:         r (int): Row index         c (int): Column index         cols (int): Number of columns in the raster              Returns:         int: 1D index     \"\"\"     return r * cols + c   def process_raster(cost_array, walking_speed, cell_size=100):     \"\"\"     Convert cost raster to travel time (in hours).          Args:         cost_array (numpy.ndarray): Cost array         walking_speed (float): Walking speed in m/s         cell_size (float): Cell size in meters              Returns:         numpy.ndarray: Travel time array in hours     \"\"\"     # Multiply by cell size to get distance     cost_array = cost_array * cell_size       # Convert to seconds (m / (m/s) = s)     cost_array = cost_array / walking_speed       # Convert seconds to hours     cost_array = cost_array / 3600       # Replace infinite values with -1     cost_array[np.isinf(cost_array)] = -1          return cost_array"},{"location":"demo/cost-calculation.html","title":"Cost Surface Generation for Volcano Evacuation","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport rasterio\nimport geopandas as gpd\nfrom data_loading import read_shapefile, read_raster\nfrom dem_processing import calculate_slope, calculate_walking_speed, normalize_walking_speed\nfrom cost_calculations import (\n    map_landcover_to_cost,\n    rasterize_layer,\n    update_cost_raster,\n    adjust_cost_with_walking_speed,\n    invert_cost_array,\n    invert_walking_speed,\n    invert_cost_raster\n)\nfrom plotting_utils import (\n    plot_continuous_raster_with_points,\n    plot_normalized_walking_speed,\n    plot_adjusted_cost_raster,\n    plot_inverted_cost_raster,\n    plot_walking_speed_vs_slope,\n    plot_north_east_speed_conservation,\n)\nfrom rasterio.plot import plotting_extent\n</pre> import os import numpy as np import rasterio import geopandas as gpd from data_loading import read_shapefile, read_raster from dem_processing import calculate_slope, calculate_walking_speed, normalize_walking_speed from cost_calculations import (     map_landcover_to_cost,     rasterize_layer,     update_cost_raster,     adjust_cost_with_walking_speed,     invert_cost_array,     invert_walking_speed,     invert_cost_raster ) from plotting_utils import (     plot_continuous_raster_with_points,     plot_normalized_walking_speed,     plot_adjusted_cost_raster,     plot_inverted_cost_raster,     plot_walking_speed_vs_slope,     plot_north_east_speed_conservation, ) from rasterio.plot import plotting_extent In\u00a0[16]: Copied! <pre># Define input data for Marapi and Awu volcanoes\nvolcanoes = [\n    {\n        \"name\": \"Marapi\",\n        \"landcover_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Marapi_LandCover_2019_100m_Buffer_UTM.tif\",\n        \"DEM_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Marapi_DEM_100m_Buffer_UTM.tif\",\n        \"hikingpath_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Marapi_original_hikinpath_final.gpkg\",\n        \"stream_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\stream_hydroriver_buffer10m1.gpkg\",\n        \"summit_path\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_marapi.gpkg\",\n    },\n    {\n        \"name\": \"Awu\",\n        \"landcover_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_LandCover_2019_100m_Buffer_UTM.tif\",\n        \"DEM_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_DEM_100m_Buffer_UTM.tif\",\n        \"hikingpath_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\awu_hikingpath_final.shp\",\n        \"stream_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\stream_awu_final.shp\",\n        \"summit_path\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\summit_awu_final.gpkg\",\n    }\n]\n</pre> # Define input data for Marapi and Awu volcanoes volcanoes = [     {         \"name\": \"Marapi\",         \"landcover_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Marapi_LandCover_2019_100m_Buffer_UTM.tif\",         \"DEM_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Marapi_DEM_100m_Buffer_UTM.tif\",         \"hikingpath_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Marapi_original_hikinpath_final.gpkg\",         \"stream_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\stream_hydroriver_buffer10m1.gpkg\",         \"summit_path\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_marapi.gpkg\",     },     {         \"name\": \"Awu\",         \"landcover_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_LandCover_2019_100m_Buffer_UTM.tif\",         \"DEM_100m\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_DEM_100m_Buffer_UTM.tif\",         \"hikingpath_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\awu_hikingpath_final.shp\",         \"stream_shapefile\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\stream_awu_final.shp\",         \"summit_path\": r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\summit_awu_final.gpkg\",     } ] In\u00a0[18]: Copied! <pre># Define cost mappings\noriginal_cost_mapping = {\n    0: np.nan, 30: 0.8333, 40: 0.8333, 50: 0.9091,\n    60: 0.5556, 90: 0.5556, 112: 0.8333,\n    116: 0.8333, 122: 0.8333, 126: 0.8333, 80: 0\n}\n\nmodified_cost_mapping = {\n    0: np.nan, 30: 0.01, 40: 0.01, 50: 0.01,\n    60: 0.01, 90: 0.01, 112: 0.01,\n    116: 0.01, 122: 0.01, 126: 0.01, 80: 0\n}\n\ncost_mappings = [\n    (\"original\", original_cost_mapping),\n    (\"modified\", modified_cost_mapping)\n]\n</pre> # Define cost mappings original_cost_mapping = {     0: np.nan, 30: 0.8333, 40: 0.8333, 50: 0.9091,     60: 0.5556, 90: 0.5556, 112: 0.8333,     116: 0.8333, 122: 0.8333, 126: 0.8333, 80: 0 }  modified_cost_mapping = {     0: np.nan, 30: 0.01, 40: 0.01, 50: 0.01,     60: 0.01, 90: 0.01, 112: 0.01,     116: 0.01, 122: 0.01, 126: 0.01, 80: 0 }  cost_mappings = [     (\"original\", original_cost_mapping),     (\"modified\", modified_cost_mapping) ] In\u00a0[19]: Copied! <pre>def load_volcano_data(volcano):\n    \"\"\"\n    Load all data for a specific volcano.\n    \n    Args:\n        volcano (dict): Dictionary containing file paths for the volcano\n        \n    Returns:\n        dict: Dictionary containing all loaded data and metadata\n    \"\"\"\n    volcano_name = volcano[\"name\"]\n    print(f\"Loading data for {volcano_name}...\")\n    \n    # Load volcano-specific data\n    landcover_data, landcover_profile, landcover_transform, landcover_crs, landcover_nodata = read_raster(volcano[\"landcover_100m\"])\n    dem_data, dem_profile, dem_transform, dem_crs, dem_nodata = read_raster(volcano[\"DEM_100m\"])\n    hiking_path_gdf = read_shapefile(volcano[\"hikingpath_shapefile\"])\n    stream_gdf = read_shapefile(volcano[\"stream_shapefile\"])\n    summit_gdf = read_shapefile(volcano[\"summit_path\"])\n\n    resolution_x = dem_transform.a\n    resolution_y = -dem_transform.e\n    extent = plotting_extent(dem_data, dem_transform)\n    \n    print(f\"Successfully loaded data for {volcano_name}\")\n    \n    return {\n        \"name\": volcano_name,\n        \"landcover_data\": landcover_data,\n        \"landcover_profile\": landcover_profile,\n        \"landcover_transform\": landcover_transform,\n        \"landcover_crs\": landcover_crs,\n        \"landcover_nodata\": landcover_nodata,\n        \"dem_data\": dem_data,\n        \"dem_profile\": dem_profile,\n        \"dem_transform\": dem_transform,\n        \"dem_crs\": dem_crs,\n        \"dem_nodata\": dem_nodata,\n        \"hiking_path_gdf\": hiking_path_gdf,\n        \"stream_gdf\": stream_gdf,\n        \"summit_gdf\": summit_gdf,\n        \"resolution_x\": resolution_x,\n        \"resolution_y\": resolution_y,\n        \"extent\": extent\n    }\n</pre> def load_volcano_data(volcano):     \"\"\"     Load all data for a specific volcano.          Args:         volcano (dict): Dictionary containing file paths for the volcano              Returns:         dict: Dictionary containing all loaded data and metadata     \"\"\"     volcano_name = volcano[\"name\"]     print(f\"Loading data for {volcano_name}...\")          # Load volcano-specific data     landcover_data, landcover_profile, landcover_transform, landcover_crs, landcover_nodata = read_raster(volcano[\"landcover_100m\"])     dem_data, dem_profile, dem_transform, dem_crs, dem_nodata = read_raster(volcano[\"DEM_100m\"])     hiking_path_gdf = read_shapefile(volcano[\"hikingpath_shapefile\"])     stream_gdf = read_shapefile(volcano[\"stream_shapefile\"])     summit_gdf = read_shapefile(volcano[\"summit_path\"])      resolution_x = dem_transform.a     resolution_y = -dem_transform.e     extent = plotting_extent(dem_data, dem_transform)          print(f\"Successfully loaded data for {volcano_name}\")          return {         \"name\": volcano_name,         \"landcover_data\": landcover_data,         \"landcover_profile\": landcover_profile,         \"landcover_transform\": landcover_transform,         \"landcover_crs\": landcover_crs,         \"landcover_nodata\": landcover_nodata,         \"dem_data\": dem_data,         \"dem_profile\": dem_profile,         \"dem_transform\": dem_transform,         \"dem_crs\": dem_crs,         \"dem_nodata\": dem_nodata,         \"hiking_path_gdf\": hiking_path_gdf,         \"stream_gdf\": stream_gdf,         \"summit_gdf\": summit_gdf,         \"resolution_x\": resolution_x,         \"resolution_y\": resolution_y,         \"extent\": extent     } In\u00a0[20]: Copied! <pre># Load data for Mount Marapi\nmarapi_data = load_volcano_data(volcanoes[0])\n\n# Display some basic information\nprint(f\"DEM dimensions: {marapi_data['dem_data'].shape}\")\nprint(f\"Land cover dimensions: {marapi_data['landcover_data'].shape}\")\nprint(f\"Resolution: {marapi_data['resolution_x']} meters\")\nprint(f\"Number of hiking paths: {len(marapi_data['hiking_path_gdf'])}\")\nprint(f\"Number of streams: {len(marapi_data['stream_gdf'])}\")\n</pre> # Load data for Mount Marapi marapi_data = load_volcano_data(volcanoes[0])  # Display some basic information print(f\"DEM dimensions: {marapi_data['dem_data'].shape}\") print(f\"Land cover dimensions: {marapi_data['landcover_data'].shape}\") print(f\"Resolution: {marapi_data['resolution_x']} meters\") print(f\"Number of hiking paths: {len(marapi_data['hiking_path_gdf'])}\") print(f\"Number of streams: {len(marapi_data['stream_gdf'])}\") <pre>Loading data for Marapi...\nSuccessfully loaded data for Marapi\nDEM dimensions: (399, 399)\nLand cover dimensions: (399, 399)\nResolution: 100.0 meters\nNumber of hiking paths: 1\nNumber of streams: 325\n</pre> In\u00a0[21]: Copied! <pre>def rasterize_vector_data(data):\n    \"\"\"\n    Rasterize stream and hiking path vector data to match the land cover raster dimensions.\n    \n    Args:\n        data (dict): Dictionary containing volcano data\n        \n    Returns:\n        tuple: (stream_raster, hiking_path_raster)\n    \"\"\"\n    landcover_data = data[\"landcover_data\"]\n    landcover_transform = data[\"landcover_transform\"]\n    stream_gdf = data[\"stream_gdf\"]\n    hiking_path_gdf = data[\"hiking_path_gdf\"]\n    \n    # Rasterize streams (as barriers - value 0)\n    stream_raster = rasterize_layer(\n        stream_gdf.geometry, \n        out_shape=landcover_data.shape,\n        transform=landcover_transform, \n        burn_value=0\n    )\n    \n    # Rasterize hiking paths (as preferred routes - value 1)\n    hiking_path_raster = rasterize_layer(\n        hiking_path_gdf.geometry, \n        out_shape=landcover_data.shape,\n        transform=landcover_transform, \n        burn_value=1\n    )\n    \n    return stream_raster, hiking_path_raster\n\n# Rasterize vector data for Marapi\nmarapi_stream_raster, marapi_hiking_path_raster = rasterize_vector_data(marapi_data)\n\nprint(\"Vector data rasterized successfully\")\nprint(f\"Stream raster shape: {marapi_stream_raster.shape}\")\nprint(f\"Hiking path raster shape: {marapi_hiking_path_raster.shape}\")\n</pre> def rasterize_vector_data(data):     \"\"\"     Rasterize stream and hiking path vector data to match the land cover raster dimensions.          Args:         data (dict): Dictionary containing volcano data              Returns:         tuple: (stream_raster, hiking_path_raster)     \"\"\"     landcover_data = data[\"landcover_data\"]     landcover_transform = data[\"landcover_transform\"]     stream_gdf = data[\"stream_gdf\"]     hiking_path_gdf = data[\"hiking_path_gdf\"]          # Rasterize streams (as barriers - value 0)     stream_raster = rasterize_layer(         stream_gdf.geometry,          out_shape=landcover_data.shape,         transform=landcover_transform,          burn_value=0     )          # Rasterize hiking paths (as preferred routes - value 1)     hiking_path_raster = rasterize_layer(         hiking_path_gdf.geometry,          out_shape=landcover_data.shape,         transform=landcover_transform,          burn_value=1     )          return stream_raster, hiking_path_raster  # Rasterize vector data for Marapi marapi_stream_raster, marapi_hiking_path_raster = rasterize_vector_data(marapi_data)  print(\"Vector data rasterized successfully\") print(f\"Stream raster shape: {marapi_stream_raster.shape}\") print(f\"Hiking path raster shape: {marapi_hiking_path_raster.shape}\") <pre>Vector data rasterized successfully\nStream raster shape: (399, 399)\nHiking path raster shape: (399, 399)\n</pre> In\u00a0[31]: Copied! <pre>import rasterio\nimport numpy as np\n\ndef calculate_terrain_effects(data, volcano_name):\n    \"\"\"\n    Calculate slopes, walking speeds, and save inverted cost raster for a given volcano.\n    \n    Args:\n        data (dict): Dictionary containing volcano data\n        volcano_name (str): Name of the volcano for automatic file naming\n    \n    Returns:\n        tuple: (slope_array, walking_speed_array, normalized_walking_speed_array, inverted_walking_speed_array)\n    \"\"\"\n    dem_data = data[\"dem_data\"]\n    resolution_x = data[\"resolution_x\"]\n    resolution_y = data[\"resolution_y\"]\n    dem_nodata = data[\"dem_nodata\"]\n    dem_crs = data.get(\"dem_crs\", None)\n    dem_transform = data.get(\"dem_transform\", None)\n    \n    # Calculate slopes in 8 directions\n    slope_array = calculate_slope(dem_data, resolution_x, resolution_y, dem_nodata)\n    \n    # Calculate walking speeds using Tobler's hiking function\n    walking_speed_array = calculate_walking_speed(slope_array)\n    \n    # Normalize walking speeds\n    normalized_walking_speed_array = normalize_walking_speed(walking_speed_array)\n    \n    # Invert walking speeds\n    from cost_calculations import invert_walking_speed\n    inverted_walking_speed_array = invert_walking_speed(normalized_walking_speed_array)\n    \n    # Save inverted cost raster\n    inverted_cost_raster_path = rf\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Python Code\\contribution rate\\inverted_Slopecost_8_directions_{volcano_name}_OriginalLandcover.tif\"\n    \n    # Ensure the array has the correct dimensions\n    if len(inverted_walking_speed_array.shape) == 2:\n        inverted_walking_speed_array = np.expand_dims(inverted_walking_speed_array, axis=0)\n    \n    with rasterio.open(\n        inverted_cost_raster_path,\n        'w',\n        driver='GTiff',\n        height=inverted_walking_speed_array.shape[1],\n        width=inverted_walking_speed_array.shape[2],\n        count=8,\n        dtype=np.float32,\n        crs=dem_crs,\n        transform=dem_transform,\n        nodata=np.nan\n    ) as dst:\n        for band in range(8):\n            dst.write(inverted_walking_speed_array[band], band + 1)\n    \n    return slope_array, walking_speed_array, normalized_walking_speed_array, inverted_walking_speed_array\n\n# Calculate terrain effects for Marapi\nmarapi_slope_array, marapi_walking_speed_array, marapi_normalized_walking_speed_array, marapi_inverted_walking_speed_array = calculate_terrain_effects(marapi_data, \"Marapi\")\n\nprint(\"Terrain effects calculated successfully\")\nprint(f\"Slope array shape: {marapi_slope_array.shape}\")\nprint(f\"Walking speed array shape: {marapi_walking_speed_array.shape}\")\nprint(f\"Normalized walking speed array shape: {marapi_normalized_walking_speed_array.shape}\")\n</pre> import rasterio import numpy as np  def calculate_terrain_effects(data, volcano_name):     \"\"\"     Calculate slopes, walking speeds, and save inverted cost raster for a given volcano.          Args:         data (dict): Dictionary containing volcano data         volcano_name (str): Name of the volcano for automatic file naming          Returns:         tuple: (slope_array, walking_speed_array, normalized_walking_speed_array, inverted_walking_speed_array)     \"\"\"     dem_data = data[\"dem_data\"]     resolution_x = data[\"resolution_x\"]     resolution_y = data[\"resolution_y\"]     dem_nodata = data[\"dem_nodata\"]     dem_crs = data.get(\"dem_crs\", None)     dem_transform = data.get(\"dem_transform\", None)          # Calculate slopes in 8 directions     slope_array = calculate_slope(dem_data, resolution_x, resolution_y, dem_nodata)          # Calculate walking speeds using Tobler's hiking function     walking_speed_array = calculate_walking_speed(slope_array)          # Normalize walking speeds     normalized_walking_speed_array = normalize_walking_speed(walking_speed_array)          # Invert walking speeds     from cost_calculations import invert_walking_speed     inverted_walking_speed_array = invert_walking_speed(normalized_walking_speed_array)          # Save inverted cost raster     inverted_cost_raster_path = rf\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Python Code\\contribution rate\\inverted_Slopecost_8_directions_{volcano_name}_OriginalLandcover.tif\"          # Ensure the array has the correct dimensions     if len(inverted_walking_speed_array.shape) == 2:         inverted_walking_speed_array = np.expand_dims(inverted_walking_speed_array, axis=0)          with rasterio.open(         inverted_cost_raster_path,         'w',         driver='GTiff',         height=inverted_walking_speed_array.shape[1],         width=inverted_walking_speed_array.shape[2],         count=8,         dtype=np.float32,         crs=dem_crs,         transform=dem_transform,         nodata=np.nan     ) as dst:         for band in range(8):             dst.write(inverted_walking_speed_array[band], band + 1)          return slope_array, walking_speed_array, normalized_walking_speed_array, inverted_walking_speed_array  # Calculate terrain effects for Marapi marapi_slope_array, marapi_walking_speed_array, marapi_normalized_walking_speed_array, marapi_inverted_walking_speed_array = calculate_terrain_effects(marapi_data, \"Marapi\")  print(\"Terrain effects calculated successfully\") print(f\"Slope array shape: {marapi_slope_array.shape}\") print(f\"Walking speed array shape: {marapi_walking_speed_array.shape}\") print(f\"Normalized walking speed array shape: {marapi_normalized_walking_speed_array.shape}\") <pre>Terrain effects calculated successfully\nSlope array shape: (8, 399, 399)\nWalking speed array shape: (8, 399, 399)\nNormalized walking speed array shape: (8, 399, 399)\n</pre> In\u00a0[9]: Copied! <pre>def visualize_terrain_effects(data, normalized_walking_speed_array):\n    \"\"\"\n    Create visualizations of slope and walking speed effects.\n    \n    Args:\n        data (dict): Dictionary containing volcano data\n        slope_array (ndarray): Array of slopes in 8 directions\n        walking_speed_array (ndarray): Array of walking speeds in 8 directions\n        normalized_walking_speed_array (ndarray): Array of normalized walking speeds\n    \"\"\"\n    volcano_name = data[\"name\"]\n    extent = data[\"extent\"]\n    summit_gdf = data[\"summit_gdf\"]\n    \n    # Plot North-East speed conservation comparison\n    plot_north_east_speed_conservation(\n        normalized_walking_speed_array=normalized_walking_speed_array, \n        extent=extent, \n        points_gdf=summit_gdf,\n        title=f\"Speed Conservation Values for Mount {volcano_name}\",\n        output_file_path=f\"{volcano_name}_north_east_speed_conservation.jpg\"\n    )\n    \n    print(f\"Created terrain effect visualizations for {volcano_name}\")\n\n# Visualize terrain effects for Marapi\nvisualize_terrain_effects(\n    marapi_data, \n    marapi_normalized_walking_speed_array\n)\n</pre> def visualize_terrain_effects(data, normalized_walking_speed_array):     \"\"\"     Create visualizations of slope and walking speed effects.          Args:         data (dict): Dictionary containing volcano data         slope_array (ndarray): Array of slopes in 8 directions         walking_speed_array (ndarray): Array of walking speeds in 8 directions         normalized_walking_speed_array (ndarray): Array of normalized walking speeds     \"\"\"     volcano_name = data[\"name\"]     extent = data[\"extent\"]     summit_gdf = data[\"summit_gdf\"]          # Plot North-East speed conservation comparison     plot_north_east_speed_conservation(         normalized_walking_speed_array=normalized_walking_speed_array,          extent=extent,          points_gdf=summit_gdf,         title=f\"Speed Conservation Values for Mount {volcano_name}\",         output_file_path=f\"{volcano_name}_north_east_speed_conservation.jpg\"     )          print(f\"Created terrain effect visualizations for {volcano_name}\")  # Visualize terrain effects for Marapi visualize_terrain_effects(     marapi_data,      marapi_normalized_walking_speed_array ) <pre>C:\\Users\\Mojan\\Desktop\\RA Volcano\\Python Code\\contribution rate\\plotting_utils.py:344: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Adjust layout to give space for colorbar\n</pre> <pre>Created terrain effect visualizations for Marapi\n</pre> In\u00a0[10]: Copied! <pre># Create land cover cost rasters for Marapi with different cost mappings\ndef create_land_cover_cost_raster(data, stream_raster, hiking_path_raster, cost_mapping, name):\n    \"\"\"\n    Create a cost raster based on land cover, streams, and hiking paths.\n    \n    Args:\n        data (dict): Dictionary containing volcano data\n        stream_raster (ndarray): Rasterized stream data\n        hiking_path_raster (ndarray): Rasterized hiking path data\n        cost_mapping (dict): Dictionary mapping land cover classes to costs\n        name (str): Name of the cost mapping (e.g., \"original\", \"modified\")\n        \n    Returns:\n        ndarray: Updated cost raster\n    \"\"\"\n    volcano_name = data[\"name\"]\n    landcover_data = data[\"landcover_data\"]\n    landcover_transform = data[\"landcover_transform\"]\n    landcover_crs = data[\"landcover_crs\"]\n    \n    # Map land cover to cost\n    cost_raster = map_landcover_to_cost(landcover_data, cost_mapping)\n    \n    # Update cost raster with streams and hiking paths\n    updated_cost_raster = update_cost_raster(cost_raster, stream_raster, hiking_path_raster)\n    \n    # Save updated cost raster\n    with rasterio.open(\n        f\"{volcano_name}_LandCover_Cost_Stream_HikingPath_100m_{name}.tif\", 'w', driver='GTiff',\n        height=updated_cost_raster.shape[0], width=updated_cost_raster.shape[1],\n        count=1, dtype=updated_cost_raster.dtype, crs=landcover_crs, transform=landcover_transform\n    ) as dst:\n        dst.write(updated_cost_raster, 1)\n    \n    print(f\"Created and saved land cover cost raster for {volcano_name} with {name} mapping\")\n    \n    return updated_cost_raster\n\n# First, create all cost rasters and store them\nmarapi_cost_rasters = {}\nfor name, cost_mapping in cost_mappings:\n    marapi_cost_rasters[name] = create_land_cover_cost_raster(\n        marapi_data, \n        marapi_stream_raster, \n        marapi_hiking_path_raster, \n        cost_mapping, \n        name\n    )\n\n# Then, plot the comparison after all rasters have been created\nif \"original\" in marapi_cost_rasters and \"modified\" in marapi_cost_rasters:\n    plot_continuous_raster_with_points(\n        raster_data=[marapi_cost_rasters[\"original\"], marapi_cost_rasters[\"modified\"]], \n        extent=marapi_data[\"extent\"],\n        points_gdf=marapi_data[\"summit_gdf\"],\n        title=[\"Original Cost Mapping Marapi\", \"Modified Cost Mapping Marapi\"],\n        colorbar_label=\"Speed Conservation Values of Land Cover\",\n        output_file_path=f\"{marapi_data['name']}_cost_mapping_comparison.jpg\"\n    )\n    print(f\"Created comparison plot for {marapi_data['name']}\")\n</pre> # Create land cover cost rasters for Marapi with different cost mappings def create_land_cover_cost_raster(data, stream_raster, hiking_path_raster, cost_mapping, name):     \"\"\"     Create a cost raster based on land cover, streams, and hiking paths.          Args:         data (dict): Dictionary containing volcano data         stream_raster (ndarray): Rasterized stream data         hiking_path_raster (ndarray): Rasterized hiking path data         cost_mapping (dict): Dictionary mapping land cover classes to costs         name (str): Name of the cost mapping (e.g., \"original\", \"modified\")              Returns:         ndarray: Updated cost raster     \"\"\"     volcano_name = data[\"name\"]     landcover_data = data[\"landcover_data\"]     landcover_transform = data[\"landcover_transform\"]     landcover_crs = data[\"landcover_crs\"]          # Map land cover to cost     cost_raster = map_landcover_to_cost(landcover_data, cost_mapping)          # Update cost raster with streams and hiking paths     updated_cost_raster = update_cost_raster(cost_raster, stream_raster, hiking_path_raster)          # Save updated cost raster     with rasterio.open(         f\"{volcano_name}_LandCover_Cost_Stream_HikingPath_100m_{name}.tif\", 'w', driver='GTiff',         height=updated_cost_raster.shape[0], width=updated_cost_raster.shape[1],         count=1, dtype=updated_cost_raster.dtype, crs=landcover_crs, transform=landcover_transform     ) as dst:         dst.write(updated_cost_raster, 1)          print(f\"Created and saved land cover cost raster for {volcano_name} with {name} mapping\")          return updated_cost_raster  # First, create all cost rasters and store them marapi_cost_rasters = {} for name, cost_mapping in cost_mappings:     marapi_cost_rasters[name] = create_land_cover_cost_raster(         marapi_data,          marapi_stream_raster,          marapi_hiking_path_raster,          cost_mapping,          name     )  # Then, plot the comparison after all rasters have been created if \"original\" in marapi_cost_rasters and \"modified\" in marapi_cost_rasters:     plot_continuous_raster_with_points(         raster_data=[marapi_cost_rasters[\"original\"], marapi_cost_rasters[\"modified\"]],          extent=marapi_data[\"extent\"],         points_gdf=marapi_data[\"summit_gdf\"],         title=[\"Original Cost Mapping Marapi\", \"Modified Cost Mapping Marapi\"],         colorbar_label=\"Speed Conservation Values of Land Cover\",         output_file_path=f\"{marapi_data['name']}_cost_mapping_comparison.jpg\"     )     print(f\"Created comparison plot for {marapi_data['name']}\") <pre>Created and saved land cover cost raster for Marapi with original mapping\nCreated and saved land cover cost raster for Marapi with modified mapping\n</pre> <pre>Created comparison plot for Marapi\n</pre> In\u00a0[33]: Copied! <pre>import rasterio\nimport numpy as np\n\n# Create land cover cost rasters for a given volcano with different cost mappings (without plotting)\ndef create_land_cover_cost_raster(data, stream_raster, hiking_path_raster, cost_mapping, name):\n    \"\"\"\n    Create a cost raster based on land cover, streams, and hiking paths, and also save the inverted version.\n\n    Args:\n        data (dict): Dictionary containing volcano data\n        stream_raster (ndarray): Rasterized stream data\n        hiking_path_raster (ndarray): Rasterized hiking path data\n        cost_mapping (dict): Dictionary mapping land cover classes to costs\n        name (str): Name of the cost mapping (\"original\" or \"modified\")\n        \n    Returns:\n        tuple: (Updated cost raster, Inverted cost raster)\n    \"\"\"\n    volcano_name = data[\"name\"]\n    landcover_data = data[\"landcover_data\"]\n    landcover_transform = data[\"landcover_transform\"]\n    landcover_crs = data[\"landcover_crs\"]\n    \n    # Map land cover to cost\n    cost_raster = map_landcover_to_cost(landcover_data, cost_mapping)\n    \n    # Update cost raster with streams and hiking paths\n    updated_cost_raster = update_cost_raster(cost_raster, stream_raster, hiking_path_raster)\n    \n    # Save updated cost raster\n    cost_raster_filename = f\"{volcano_name}_LandCover_Cost_Stream_HikingPath_100m_{name}.tif\"\n    with rasterio.open(\n        cost_raster_filename, 'w', driver='GTiff',\n        height=updated_cost_raster.shape[0], width=updated_cost_raster.shape[1],\n        count=1, dtype=updated_cost_raster.dtype, crs=landcover_crs, transform=landcover_transform\n    ) as dst:\n        dst.write(updated_cost_raster, 1)\n    \n    print(f\"Created and saved land cover cost raster: {cost_raster_filename}\")\n\n    # Generate and save the inverted cost raster\n    inverted_cost_raster = invert_cost_raster(updated_cost_raster)\n    \n    # Define unique names for the inverted raster\n    inverted_raster_filename = f\"invert_landcovercost_{name}_{volcano_name}.tif\"\n    \n    with rasterio.open(\n        inverted_raster_filename, 'w', driver='GTiff',\n        height=inverted_cost_raster.shape[0], width=inverted_cost_raster.shape[1],\n        count=1, dtype=inverted_cost_raster.dtype, crs=landcover_crs, transform=landcover_transform\n    ) as dst:\n        dst.write(inverted_cost_raster, 1)\n\n    print(f\"Created and saved inverted land cover cost raster: {inverted_raster_filename}\")\n    \n    return updated_cost_raster, inverted_cost_raster\n\n\n# Process and save cost rasters for Marapi\nmarapi_cost_rasters = {}\nmarapi_inverted_cost_rasters = {}\n\nfor name, cost_mapping in cost_mappings:\n    updated_raster, inverted_raster = create_land_cover_cost_raster(\n        marapi_data, \n        marapi_stream_raster, \n        marapi_hiking_path_raster, \n        cost_mapping, \n        name\n    )\n    marapi_cost_rasters[name] = updated_raster\n    marapi_inverted_cost_rasters[name] = inverted_raster\n\n\n\n\n# Then, plot the comparison after all rasters have been created\nif \"original\" in marapi_cost_rasters and \"modified\" in marapi_cost_rasters:\n    plot_continuous_raster_with_points(\n        raster_data=[marapi_cost_rasters[\"original\"], marapi_cost_rasters[\"modified\"]], \n        extent=marapi_data[\"extent\"],\n        points_gdf=marapi_data[\"summit_gdf\"],\n        title=[\"Original Cost Mapping Marapi\", \"Modified Cost Mapping Marapi\"],\n        colorbar_label=\"Speed Conservation Values of Land Cover\",\n        output_file_path=f\"{marapi_data['name']}_cost_mapping_comparison.jpg\"\n    )\n    print(f\"Created comparison plot for {marapi_data['name']}\")\n</pre> import rasterio import numpy as np  # Create land cover cost rasters for a given volcano with different cost mappings (without plotting) def create_land_cover_cost_raster(data, stream_raster, hiking_path_raster, cost_mapping, name):     \"\"\"     Create a cost raster based on land cover, streams, and hiking paths, and also save the inverted version.      Args:         data (dict): Dictionary containing volcano data         stream_raster (ndarray): Rasterized stream data         hiking_path_raster (ndarray): Rasterized hiking path data         cost_mapping (dict): Dictionary mapping land cover classes to costs         name (str): Name of the cost mapping (\"original\" or \"modified\")              Returns:         tuple: (Updated cost raster, Inverted cost raster)     \"\"\"     volcano_name = data[\"name\"]     landcover_data = data[\"landcover_data\"]     landcover_transform = data[\"landcover_transform\"]     landcover_crs = data[\"landcover_crs\"]          # Map land cover to cost     cost_raster = map_landcover_to_cost(landcover_data, cost_mapping)          # Update cost raster with streams and hiking paths     updated_cost_raster = update_cost_raster(cost_raster, stream_raster, hiking_path_raster)          # Save updated cost raster     cost_raster_filename = f\"{volcano_name}_LandCover_Cost_Stream_HikingPath_100m_{name}.tif\"     with rasterio.open(         cost_raster_filename, 'w', driver='GTiff',         height=updated_cost_raster.shape[0], width=updated_cost_raster.shape[1],         count=1, dtype=updated_cost_raster.dtype, crs=landcover_crs, transform=landcover_transform     ) as dst:         dst.write(updated_cost_raster, 1)          print(f\"Created and saved land cover cost raster: {cost_raster_filename}\")      # Generate and save the inverted cost raster     inverted_cost_raster = invert_cost_raster(updated_cost_raster)          # Define unique names for the inverted raster     inverted_raster_filename = f\"invert_landcovercost_{name}_{volcano_name}.tif\"          with rasterio.open(         inverted_raster_filename, 'w', driver='GTiff',         height=inverted_cost_raster.shape[0], width=inverted_cost_raster.shape[1],         count=1, dtype=inverted_cost_raster.dtype, crs=landcover_crs, transform=landcover_transform     ) as dst:         dst.write(inverted_cost_raster, 1)      print(f\"Created and saved inverted land cover cost raster: {inverted_raster_filename}\")          return updated_cost_raster, inverted_cost_raster   # Process and save cost rasters for Marapi marapi_cost_rasters = {} marapi_inverted_cost_rasters = {}  for name, cost_mapping in cost_mappings:     updated_raster, inverted_raster = create_land_cover_cost_raster(         marapi_data,          marapi_stream_raster,          marapi_hiking_path_raster,          cost_mapping,          name     )     marapi_cost_rasters[name] = updated_raster     marapi_inverted_cost_rasters[name] = inverted_raster     # Then, plot the comparison after all rasters have been created if \"original\" in marapi_cost_rasters and \"modified\" in marapi_cost_rasters:     plot_continuous_raster_with_points(         raster_data=[marapi_cost_rasters[\"original\"], marapi_cost_rasters[\"modified\"]],          extent=marapi_data[\"extent\"],         points_gdf=marapi_data[\"summit_gdf\"],         title=[\"Original Cost Mapping Marapi\", \"Modified Cost Mapping Marapi\"],         colorbar_label=\"Speed Conservation Values of Land Cover\",         output_file_path=f\"{marapi_data['name']}_cost_mapping_comparison.jpg\"     )     print(f\"Created comparison plot for {marapi_data['name']}\")  <pre>Created and saved land cover cost raster: Marapi_LandCover_Cost_Stream_HikingPath_100m_original.tif\nCreated and saved inverted land cover cost raster: invert_landcovercost_original_Marapi.tif\nCreated and saved land cover cost raster: Marapi_LandCover_Cost_Stream_HikingPath_100m_modified.tif\nCreated and saved inverted land cover cost raster: invert_landcovercost_modified_Marapi.tif\n</pre> <pre>Created comparison plot for Marapi\n</pre> In\u00a0[34]: Copied! <pre>def combine_costs(data, normalized_walking_speed_array, updated_cost_raster, name):\n    \"\"\"\n    Combine land cover and slope effects on walking speed.\n    \n    Args:\n        data (dict): Dictionary containing volcano data\n        normalized_walking_speed_array (ndarray): Array of normalized walking speeds\n        updated_cost_raster (ndarray): Updated cost raster based on land cover\n        name (str): Name of the cost mapping (e.g., \"original\", \"modified\")\n        \n    Returns:\n        tuple: (adjusted_cost_array, scaled_adjusted_cost_array, inverted_cost_array)\n    \"\"\"\n    volcano_name = data[\"name\"]\n    landcover_transform = data[\"landcover_transform\"]\n    landcover_crs = data[\"landcover_crs\"]\n    extent = data[\"extent\"]\n    summit_gdf = data[\"summit_gdf\"]\n    \n    # Adjust cost with walking speed\n    adjusted_cost_array = adjust_cost_with_walking_speed(normalized_walking_speed_array, updated_cost_raster)\n    \n    # Scale adjusted cost array for visualization (1.22 m/s is a reference walking speed)\n    scaled_adjusted_cost_array = adjusted_cost_array * 1.22\n    \n    # Save adjusted cost array\n    with rasterio.open(\n        f\"{volcano_name}_adjusted_cost_8_directions_{name}.tif\", 'w', driver='GTiff',\n        height=adjusted_cost_array.shape[1], width=adjusted_cost_array.shape[2],\n        count=8, dtype=np.float32, crs=landcover_crs, transform=landcover_transform,\n        nodata=np.nan\n    ) as dst:\n        for band in range(8):\n            dst.write(adjusted_cost_array[band], band + 1)\n    \n    # Save scaled adjusted cost array\n    with rasterio.open(\n        f\"{volcano_name}_adjusted_cost_8_directions_scaled_{name}.tif\", 'w', driver='GTiff',\n        height=scaled_adjusted_cost_array.shape[1], width=scaled_adjusted_cost_array.shape[2],\n        count=8, dtype=np.float32, crs=landcover_crs, transform=landcover_transform,\n        nodata=np.nan\n    ) as dst:\n        for band in range(8):\n            dst.write(scaled_adjusted_cost_array[band], band + 1)\n    \n    # Visualize adjusted cost (north direction)\n    plot_adjusted_cost_raster(\n        adjusted_cost_raster=scaled_adjusted_cost_array[0], \n        extent=extent, \n        points_gdf=summit_gdf,\n        title=f\"Speed Reduction by Land Cover and Slope, North Direction (Mount {volcano_name}) - {name.capitalize()}\",\n        output_file_path=f\"{volcano_name}_Speed_reduction_land_cover_slope_{name}.jpg\"\n    )\n    \n    # Invert cost array for path finding\n    inverted_cost_array = invert_cost_array(adjusted_cost_array)\n    \n    # Save inverted cost array\n    with rasterio.open(\n        f\"{volcano_name}_inverted_cost_8_directions_{name}.tif\", 'w', driver='GTiff',\n        height=inverted_cost_array.shape[1], width=inverted_cost_array.shape[2],\n        count=8, dtype=np.float32, crs=landcover_crs, transform=landcover_transform,\n        nodata=np.nan\n    ) as dst:\n        for band in range(8):\n            dst.write(inverted_cost_array[band], band + 1)\n    \n    # Visualize inverted cost (north direction)\n    plot_inverted_cost_raster(\n        inverted_cost_raster=inverted_cost_array[0], \n        extent=extent, \n        points_gdf=summit_gdf,\n        title=f\"Cost Raster, North Direction (Mount {volcano_name}) - {name.capitalize()}\",\n        output_file_path=f\"{volcano_name}_InvertedCost_{name}.jpg\"\n    )\n    \n    print(f\"Combined costs for {volcano_name} with {name} mapping\")\n    \n    return adjusted_cost_array, scaled_adjusted_cost_array, inverted_cost_array\n\n# Combine costs for Marapi with different cost mappings\nmarapi_combined_costs = {}\nfor name, cost_mapping in cost_mappings:\n    marapi_combined_costs[name] = combine_costs(\n        marapi_data, \n        marapi_normalized_walking_speed_array, \n        marapi_cost_rasters[name], \n        name\n    )\n</pre> def combine_costs(data, normalized_walking_speed_array, updated_cost_raster, name):     \"\"\"     Combine land cover and slope effects on walking speed.          Args:         data (dict): Dictionary containing volcano data         normalized_walking_speed_array (ndarray): Array of normalized walking speeds         updated_cost_raster (ndarray): Updated cost raster based on land cover         name (str): Name of the cost mapping (e.g., \"original\", \"modified\")              Returns:         tuple: (adjusted_cost_array, scaled_adjusted_cost_array, inverted_cost_array)     \"\"\"     volcano_name = data[\"name\"]     landcover_transform = data[\"landcover_transform\"]     landcover_crs = data[\"landcover_crs\"]     extent = data[\"extent\"]     summit_gdf = data[\"summit_gdf\"]          # Adjust cost with walking speed     adjusted_cost_array = adjust_cost_with_walking_speed(normalized_walking_speed_array, updated_cost_raster)          # Scale adjusted cost array for visualization (1.22 m/s is a reference walking speed)     scaled_adjusted_cost_array = adjusted_cost_array * 1.22          # Save adjusted cost array     with rasterio.open(         f\"{volcano_name}_adjusted_cost_8_directions_{name}.tif\", 'w', driver='GTiff',         height=adjusted_cost_array.shape[1], width=adjusted_cost_array.shape[2],         count=8, dtype=np.float32, crs=landcover_crs, transform=landcover_transform,         nodata=np.nan     ) as dst:         for band in range(8):             dst.write(adjusted_cost_array[band], band + 1)          # Save scaled adjusted cost array     with rasterio.open(         f\"{volcano_name}_adjusted_cost_8_directions_scaled_{name}.tif\", 'w', driver='GTiff',         height=scaled_adjusted_cost_array.shape[1], width=scaled_adjusted_cost_array.shape[2],         count=8, dtype=np.float32, crs=landcover_crs, transform=landcover_transform,         nodata=np.nan     ) as dst:         for band in range(8):             dst.write(scaled_adjusted_cost_array[band], band + 1)          # Visualize adjusted cost (north direction)     plot_adjusted_cost_raster(         adjusted_cost_raster=scaled_adjusted_cost_array[0],          extent=extent,          points_gdf=summit_gdf,         title=f\"Speed Reduction by Land Cover and Slope, North Direction (Mount {volcano_name}) - {name.capitalize()}\",         output_file_path=f\"{volcano_name}_Speed_reduction_land_cover_slope_{name}.jpg\"     )          # Invert cost array for path finding     inverted_cost_array = invert_cost_array(adjusted_cost_array)          # Save inverted cost array     with rasterio.open(         f\"{volcano_name}_inverted_cost_8_directions_{name}.tif\", 'w', driver='GTiff',         height=inverted_cost_array.shape[1], width=inverted_cost_array.shape[2],         count=8, dtype=np.float32, crs=landcover_crs, transform=landcover_transform,         nodata=np.nan     ) as dst:         for band in range(8):             dst.write(inverted_cost_array[band], band + 1)          # Visualize inverted cost (north direction)     plot_inverted_cost_raster(         inverted_cost_raster=inverted_cost_array[0],          extent=extent,          points_gdf=summit_gdf,         title=f\"Cost Raster, North Direction (Mount {volcano_name}) - {name.capitalize()}\",         output_file_path=f\"{volcano_name}_InvertedCost_{name}.jpg\"     )          print(f\"Combined costs for {volcano_name} with {name} mapping\")          return adjusted_cost_array, scaled_adjusted_cost_array, inverted_cost_array  # Combine costs for Marapi with different cost mappings marapi_combined_costs = {} for name, cost_mapping in cost_mappings:     marapi_combined_costs[name] = combine_costs(         marapi_data,          marapi_normalized_walking_speed_array,          marapi_cost_rasters[name],          name     )  <pre>C:\\Users\\Mojan\\miniconda3\\envs\\py38\\lib\\site-packages\\numpy\\lib\\function_base.py:4737: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n</pre> <pre>Combined costs for Marapi with original mapping\n</pre> <pre>C:\\Users\\Mojan\\miniconda3\\envs\\py38\\lib\\site-packages\\numpy\\lib\\function_base.py:4737: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n</pre> <pre>Combined costs for Marapi with modified mapping\n</pre> In\u00a0[35]: Copied! <pre># Load data for Mount Awu\nawu_data = load_volcano_data(volcanoes[1])\n\n# Display some basic information\nprint(f\"DEM dimensions: {awu_data['dem_data'].shape}\")\nprint(f\"Land cover dimensions: {awu_data['landcover_data'].shape}\")\nprint(f\"Resolution: {awu_data['resolution_x']} meters\")\nprint(f\"Number of hiking paths: {len(awu_data['hiking_path_gdf'])}\")\nprint(f\"Number of streams: {len(awu_data['stream_gdf'])}\")\n</pre> # Load data for Mount Awu awu_data = load_volcano_data(volcanoes[1])  # Display some basic information print(f\"DEM dimensions: {awu_data['dem_data'].shape}\") print(f\"Land cover dimensions: {awu_data['landcover_data'].shape}\") print(f\"Resolution: {awu_data['resolution_x']} meters\") print(f\"Number of hiking paths: {len(awu_data['hiking_path_gdf'])}\") print(f\"Number of streams: {len(awu_data['stream_gdf'])}\") <pre>Loading data for Awu...\nSuccessfully loaded data for Awu\nDEM dimensions: (400, 401)\nLand cover dimensions: (400, 401)\nResolution: 100.0 meters\nNumber of hiking paths: 1\nNumber of streams: 16\n</pre> In\u00a0[37]: Copied! <pre># Rasterize vector data for Awu\nawu_stream_raster, awu_hiking_path_raster = rasterize_vector_data(awu_data)\n\n# Calculate terrain effects for Awu\nawu_slope_array, awu_walking_speed_array, awu_normalized_walking_speed_array, awu_inverted_walking_speed_array = calculate_terrain_effects(awu_data, \"Awu\")\n\n# Visualize terrain effects for Awu\n# Visualize terrain effects for Awu\nvisualize_terrain_effects(\n    awu_data, \n    awu_normalized_walking_speed_array\n)\n\n# Create land cover cost rasters for Awu\nawu_cost_rasters = {}\nfor name, cost_mapping in cost_mappings:\n    awu_cost_rasters[name] = create_land_cover_cost_raster(\n        awu_data, \n        awu_stream_raster, \n        awu_hiking_path_raster, \n        cost_mapping, \n        name\n    )\n# Process and save cost rasters for Awu\nawu_cost_rasters = {}\nawu_inverted_cost_rasters = {}\n\nfor name, cost_mapping in cost_mappings:\n    updated_raster, inverted_raster = create_land_cover_cost_raster(\n        awu_data, \n        awu_stream_raster, \n        awu_hiking_path_raster, \n        cost_mapping, \n        name\n    )\n    awu_cost_rasters[name] = updated_raster\n    awu_inverted_cost_rasters[name] = inverted_raster\n\n\n# Combine costs for Awu\nawu_combined_costs = {}\nfor name, cost_mapping in cost_mappings:\n    awu_combined_costs[name] = combine_costs(\n        awu_data, \n        awu_normalized_walking_speed_array, \n        awu_cost_rasters[name], \n        name\n    )\n\nprint(\"Completed processing for Mount Awu\")\n</pre> # Rasterize vector data for Awu awu_stream_raster, awu_hiking_path_raster = rasterize_vector_data(awu_data)  # Calculate terrain effects for Awu awu_slope_array, awu_walking_speed_array, awu_normalized_walking_speed_array, awu_inverted_walking_speed_array = calculate_terrain_effects(awu_data, \"Awu\")  # Visualize terrain effects for Awu # Visualize terrain effects for Awu visualize_terrain_effects(     awu_data,      awu_normalized_walking_speed_array )  # Create land cover cost rasters for Awu awu_cost_rasters = {} for name, cost_mapping in cost_mappings:     awu_cost_rasters[name] = create_land_cover_cost_raster(         awu_data,          awu_stream_raster,          awu_hiking_path_raster,          cost_mapping,          name     ) # Process and save cost rasters for Awu awu_cost_rasters = {} awu_inverted_cost_rasters = {}  for name, cost_mapping in cost_mappings:     updated_raster, inverted_raster = create_land_cover_cost_raster(         awu_data,          awu_stream_raster,          awu_hiking_path_raster,          cost_mapping,          name     )     awu_cost_rasters[name] = updated_raster     awu_inverted_cost_rasters[name] = inverted_raster   # Combine costs for Awu awu_combined_costs = {} for name, cost_mapping in cost_mappings:     awu_combined_costs[name] = combine_costs(         awu_data,          awu_normalized_walking_speed_array,          awu_cost_rasters[name],          name     )  print(\"Completed processing for Mount Awu\")  <pre>C:\\Users\\Mojan\\Desktop\\RA Volcano\\Python Code\\contribution rate\\plotting_utils.py:344: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Adjust layout to give space for colorbar\n</pre> <pre>Created terrain effect visualizations for Awu\nCreated and saved land cover cost raster: Awu_LandCover_Cost_Stream_HikingPath_100m_original.tif\nCreated and saved inverted land cover cost raster: invert_landcovercost_original_Awu.tif\nCreated and saved land cover cost raster: Awu_LandCover_Cost_Stream_HikingPath_100m_modified.tif\nCreated and saved inverted land cover cost raster: invert_landcovercost_modified_Awu.tif\nCreated and saved land cover cost raster: Awu_LandCover_Cost_Stream_HikingPath_100m_original.tif\nCreated and saved inverted land cover cost raster: invert_landcovercost_original_Awu.tif\nCreated and saved land cover cost raster: Awu_LandCover_Cost_Stream_HikingPath_100m_modified.tif\nCreated and saved inverted land cover cost raster: invert_landcovercost_modified_Awu.tif\n</pre> <pre>C:\\Users\\Mojan\\miniconda3\\envs\\py38\\lib\\site-packages\\numpy\\lib\\function_base.py:4737: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n</pre> <pre>Combined costs for Awu with original mapping\n</pre> <pre>C:\\Users\\Mojan\\miniconda3\\envs\\py38\\lib\\site-packages\\numpy\\lib\\function_base.py:4737: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n</pre> <pre>Combined costs for Awu with modified mapping\nCompleted processing for Mount Awu\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demo/cost-calculation.html#cost-surface-generation-for-volcano-evacuation","title":"Cost Surface Generation for Volcano Evacuation\u00b6","text":"<p>This notebook demonstrates how to create cost surfaces for volcano evacuation modeling. Cost surfaces represent the difficulty of traveling through different terrain and are essential for finding optimal evacuation routes.</p>"},{"location":"demo/cost-calculation.html#what-are-cost-surfaces","title":"What are Cost Surfaces?\u00b6","text":"<p>In evacuation modeling, a cost surface represents how difficult or time-consuming it is to traverse each part of the landscape. The cost is influenced by:</p> <ol> <li>Land cover (forests, grasslands, urban areas, water bodies)</li> <li>Terrain slope (uphill, downhill, flat areas)</li> <li>Infrastructure (hiking paths, roads)</li> <li>Natural barriers (rivers, cliffs)</li> </ol> <p>In this notebook, we'll create comprehensive cost surfaces for two Indonesian volcanoes: Mount Marapi and Mount Awu. These cost surfaces will be used for evacuation path analysis in subsequent notebooks.</p>"},{"location":"demo/cost-calculation.html#import-libraries-and-custom-modules","title":"Import Libraries and Custom Modules\u00b6","text":"<p>First, we need to import the necessary libraries and custom modules:</p> <ul> <li>Basic libraries: numpy, rasterio, geopandas</li> <li>Custom modules:<ul> <li><code>data_loading</code>: Functions for reading raster and vector data</li> <li><code>dem_processing</code>: Functions for slope and walking speed calculations</li> <li><code>cost_calculations</code>: Functions for creating and modifying cost surfaces</li> <li><code>plotting_utils</code>: Functions for visualizing results</li> </ul> </li> </ul>"},{"location":"demo/cost-calculation.html#define-input-data-for-multiple-volcanoes","title":"Define Input Data for Multiple Volcanoes\u00b6","text":"<p>To make our analysis more robust and comparative, we'll process two different volcanoes:</p> <ol> <li>Mount Marapi in West Sumatra, Indonesia</li> <li>Mount Awu in the Sangihe Islands, Indonesia</li> </ol> <p>For each volcano, we need:</p> <ul> <li>Digital Elevation Model (DEM)</li> <li>Land cover classification raster</li> <li>Hiking path vector data</li> <li>Stream network vector data</li> <li>Summit location vector data</li> </ul> <p>We organize this data in a list of dictionaries for easy iteration.</p>"},{"location":"demo/cost-calculation.html#define-cost-mappings-for-land-cover","title":"Define Cost Mappings for Land Cover\u00b6","text":"<p>Different land cover types affect travel speed differently. We define \"speed conservation values\" for each land cover class:</p> <ul> <li>Values close to 1 represent areas where most of the maximum speed can be maintained</li> <li>Values close to 0 represent areas where travel is very slow</li> <li>Value of 0 represents impassable areas (like water bodies)</li> <li>NaN values represent no data or areas outside our study region</li> </ul> <p>We create two different cost mappings to explore different scenarios:</p> <ol> <li>Original mapping: Realistic travel speeds based on literature and expert knowledge</li> <li>Modified mapping: An extreme scenario where land cover severely restricts movement (useful for sensitivity analysis)</li> </ol> <p>The cost values correspond to the Copernicus Global Land Cover classification system:</p> <ul> <li>0: No data</li> <li>30, 40: Grassland and cropland</li> <li>50: Urban/built-up</li> <li>60, 90: Shrubland and herbaceous vegetation</li> <li>112-126: Various forest types</li> <li>80: Water bodies (impassable)</li> </ul>"},{"location":"demo/cost-calculation.html#create-data-loading-function","title":"Create Data Loading Function\u00b6","text":"<p>we'll create a function that loads all data for a specific volcano. This approach allows each section to stand on its own while maintaining the logic of the original script.</p>"},{"location":"demo/cost-calculation.html#load-data-for-mount-marapi","title":"Load Data for Mount Marapi\u00b6","text":"<p>Let's load the data for our first volcano, Mount Marapi. This includes:</p> <ol> <li>Land cover raster: Classification of surface types (forest, grassland, etc.)</li> <li>DEM raster: Digital elevation model providing terrain height</li> <li>Hiking paths: Vector data of existing trails</li> <li>Streams: Vector data of rivers and water courses</li> <li>Summit location: Point data marking the volcano's peak</li> </ol> <p>We also extract important metadata like spatial resolution and extent, which will be used in subsequent calculations.</p>"},{"location":"demo/cost-calculation.html#6-rasterize-vector-data","title":"6. Rasterize Vector Data\u00b6","text":"<p>To incorporate our vector data (hiking paths and streams) into the cost surface calculations, we need to convert them to raster format with the same dimensions as our other rasters.This process is called \"rasterization\".</p> <p>For each vector layer:</p> <ul> <li>Streams: Assigned a value of 0 (impassable)</li> <li>Hiking paths: Assigned a value of 1 (full speed possible)</li> </ul> <p>This allows us to include these features in our cost surface calculations.</p>"},{"location":"demo/cost-calculation.html#calculate-slopes-and-walking-speeds","title":"Calculate Slopes and Walking Speeds\u00b6","text":"<p>Slope is a critical factor affecting travel speed in mountainous terrain.We calculate slopes in 8 directions (N, NE, E, SE, S, SW, W, NW) to account for directional effects. This is important because:</p> <ol> <li>Going downhill is generally faster than going uphill</li> <li>The direction of travel affects the experienced slope</li> <li>In evacuation scenarios, people may need to travel in different directions</li> </ol> <p>After calculating slopes, we use Tobler's hiking function to estimate walking speeds:</p> <p>$W = 6 \\times e^{-3.5 \\times |S + 0.05|}$</p> <p>Where:</p> <ul> <li>$W$ is the walking speed in km/h</li> <li>$S$ is the slope in rise/run form (not degrees)</li> <li>The function is maximized at a slight downhill slope of -0.05 (approximately -3\u00b0)</li> </ul> <p>Finally, we normalize these speeds to get a ratio of actual speed to maximum possible speed.</p>"},{"location":"demo/cost-calculation.html#visualize-slope-and-walking-speed","title":"Visualize Slope and Walking Speed\u00b6","text":"<p>To better understand the terrain effects on travel, we'll create several visualizations:</p> <ol> <li>Slopes in 8 Directions: Shows how slope varies in different directions around the volcano</li> <li>Walking Speed in 8 Directions: Shows how Tobler's hiking function predicts walking speed in each direction</li> <li>Walking Speed vs. Slope: Shows the relationship between slope angle and walking speed</li> <li>Normalized Walking Speed: Shows the speed conservation values based on slope</li> </ol> <p>These visualizations help us understand:</p> <ul> <li>How terrain affects evacuation speed</li> <li>Which areas might be challenging to traverse</li> <li>The directional dependencies of evacuation speed</li> </ul>"},{"location":"demo/cost-calculation.html#create-land-cover-cost-raster","title":"Create Land Cover Cost Raster\u00b6","text":"<p>Now, we'll create a cost raster based on land cover types. We'll do this for both the original and modified cost mappings to explore different scenarios.</p> <p>For each cost mapping scenario:</p> <ol> <li>We apply our cost mapping to the land cover data, translating land cover classes into speed conservation values</li> <li>We update this raster by incorporating streams (as barriers) and hiking paths (as preferred routes)</li> <li>We save the updated cost raster for documentation and potential reuse</li> <li>We create a visualization showing the speed conservation values across the study area</li> </ol> <p>This step focuses on the effects of surface features (land cover, streams, paths) on travel speed, before considering terrain slope.</p>"},{"location":"demo/cost-calculation.html#combine-land-cover-and-slope-effects","title":"Combine Land Cover and Slope Effects\u00b6","text":"<p>Next, we combine the effects of land cover and slope on walking speed:</p> <ol> <li><p>For each of the 8 directions, we multiply:</p> <ul> <li>The normalized walking speed due to slope</li> <li>The speed conservation value due to land cover</li> </ul> </li> <li><p>This gives us a comprehensive cost surface that accounts for both factors</p> </li> <li><p>We also create a scaled version (multiplied by 1.22 m/s) for visualization purposes, which represents actual walking speeds rather than just conservation values</p> </li> <li><p>We save both versions as multi-band GeoTIFF files, where each band represents a direction</p> </li> </ol> <p>This combined cost surface provides a more realistic model of evacuation speed across the landscape, accounting for both terrain and surface features.</p>"},{"location":"demo/cost-calculation.html#11-repeat-for-mount-awu","title":"11. Repeat for Mount Awu\u00b6","text":"<p>Now we'll repeat the same process for Mount Awu. This allows us to compare results between different volcanoes and identify common patterns or site-specific challenges.</p> <p>First, we'll load the data for Mount Awu:</p>"},{"location":"demo/cost-calculation.html#now-well-process-mount-awu-through-all-the-same-steps","title":"Now we'll process Mount Awu through all the same steps:\u00b6","text":"<ol> <li>Rasterize vector data</li> <li>Calculate terrain effects</li> <li>Visualize terrain effects</li> <li>Create land cover cost rasters</li> <li>Combine costs</li> </ol>"},{"location":"demo/cost-calculation.html#12-conclusion-understanding-cost-surfaces-for-evacuation","title":"12. Conclusion: Understanding Cost Surfaces for Evacuation\u00b6","text":"<p>This notebook has demonstrated how to create comprehensive cost surfaces for volcano evacuation analysis. The key insights from this process are:</p> <ol> <li><p>Multiple Factors Affect Travel Speed:</p> <ul> <li>Land cover types have varying effects on travel speed</li> <li>Terrain slope significantly impacts walking speed, with different effects uphill vs. downhill</li> <li>Infrastructure like hiking paths can greatly improve evacuation speed</li> <li>Natural barriers like streams can block evacuation routes</li> </ul> </li> <li><p>Directional Considerations Are Important:</p> <ul> <li>Slope effects depend on direction of travel</li> <li>Creating cost surfaces for 8 directions provides more realistic modeling</li> <li>Evacuation routes may differ significantly depending on direction</li> </ul> </li> <li><p>Scenario Testing Is Valuable:</p> <ul> <li>Different cost mappings allow us to test sensitivity to our assumptions</li> <li>Comparing multiple volcanoes helps identify common patterns vs. site-specific challenges</li> </ul> </li> </ol> <p>The cost surfaces we've created will be used in the next notebook to analyze evacuation routes and times, helping to:</p> <ul> <li>Identify optimal evacuation paths</li> <li>Estimate evacuation times for different scenarios</li> <li>Plan strategic improvements to evacuation infrastructure</li> <li>Develop better emergency management plans</li> </ul> <p>The multi-directional, multi-scenario approach provides a robust foundation for these analyses.</p>"},{"location":"demo/data-download.html","title":"Volcano Data Acquisition with Google Earth Engine","text":"In\u00a0[19]: Copied! <pre># Import required packages\nimport ee\n</pre> # Import required packages import ee  In\u00a0[20]: Copied! <pre>def authenticate_gee():\n    \"\"\"\n    Authenticate with Google Earth Engine.\n    \n    This function checks if you're already authenticated and if not,\n    initiates the authentication process.\n    \"\"\"\n    try:\n        # Try to initialize without authentication\n        ee.Initialize()\n        print(\"Already authenticated with Google Earth Engine\")\n    except:\n        # If initialization fails, authenticate first\n        ee.Authenticate()\n        ee.Initialize()\n        print(\"Authentication successful\")\n\n# Run the authentication function\nauthenticate_gee()\n</pre> def authenticate_gee():     \"\"\"     Authenticate with Google Earth Engine.          This function checks if you're already authenticated and if not,     initiates the authentication process.     \"\"\"     try:         # Try to initialize without authentication         ee.Initialize()         print(\"Already authenticated with Google Earth Engine\")     except:         # If initialization fails, authenticate first         ee.Authenticate()         ee.Initialize()         print(\"Authentication successful\")  # Run the authentication function authenticate_gee() <pre>Already authenticated with Google Earth Engine\n</pre> In\u00a0[21]: Copied! <pre>def get_utm_epsg(latitude, longitude):\n    \"\"\"\n    Determine the EPSG code for the UTM zone appropriate for the given coordinates.\n    \n    Args:\n        latitude (float): Latitude in decimal degrees\n        longitude (float): Longitude in decimal degrees\n        \n    Returns:\n        str: EPSG code in the format \"EPSG:xxxxx\"\n    \"\"\"\n    # Make sure longitude is between -180 and 180\n    longitude = ((longitude + 180) % 360) - 180\n    \n    # Calculate UTM zone number\n    zone_number = int(((longitude + 180) / 6) + 1)\n    \n    # Special cases for Norway and Svalbard\n    if 56 &lt;= latitude &lt; 64 and 3 &lt;= longitude &lt; 12:\n        zone_number = 32\n    elif 72 &lt;= latitude &lt; 84 and longitude &gt;= 0:\n        if longitude &lt; 9:\n            zone_number = 31\n        elif longitude &lt; 21:\n            zone_number = 33\n        elif longitude &lt; 33:\n            zone_number = 35\n        elif longitude &lt; 42:\n            zone_number = 37\n    \n    # Determine EPSG code\n    if latitude &gt;= 0:\n        # Northern hemisphere\n        epsg = f\"EPSG:326{zone_number:02d}\"\n    else:\n        # Southern hemisphere\n        epsg = f\"EPSG:327{zone_number:02d}\"\n    \n    print(f\"Coordinates ({latitude}, {longitude}) are in UTM zone {zone_number}\")\n    print(f\"Using projection {epsg}\")\n    \n    return epsg\n</pre> def get_utm_epsg(latitude, longitude):     \"\"\"     Determine the EPSG code for the UTM zone appropriate for the given coordinates.          Args:         latitude (float): Latitude in decimal degrees         longitude (float): Longitude in decimal degrees              Returns:         str: EPSG code in the format \"EPSG:xxxxx\"     \"\"\"     # Make sure longitude is between -180 and 180     longitude = ((longitude + 180) % 360) - 180          # Calculate UTM zone number     zone_number = int(((longitude + 180) / 6) + 1)          # Special cases for Norway and Svalbard     if 56 &lt;= latitude &lt; 64 and 3 &lt;= longitude &lt; 12:         zone_number = 32     elif 72 &lt;= latitude &lt; 84 and longitude &gt;= 0:         if longitude &lt; 9:             zone_number = 31         elif longitude &lt; 21:             zone_number = 33         elif longitude &lt; 33:             zone_number = 35         elif longitude &lt; 42:             zone_number = 37          # Determine EPSG code     if latitude &gt;= 0:         # Northern hemisphere         epsg = f\"EPSG:326{zone_number:02d}\"     else:         # Southern hemisphere         epsg = f\"EPSG:327{zone_number:02d}\"          print(f\"Coordinates ({latitude}, {longitude}) are in UTM zone {zone_number}\")     print(f\"Using projection {epsg}\")          return epsg In\u00a0[22]: Copied! <pre>def define_aoi(coords, buffer_distance=20000):\n    \"\"\"\n    Define an area of interest around a point.\n    \n    Args:\n        coords (list): [longitude, latitude] of the point\n        buffer_distance (int): Buffer distance in meters\n        \n    Returns:\n        tuple: (ee.Geometry, list) - AOI and region coordinates\n    \"\"\"\n    # Create a point from coordinates\n    point = ee.Geometry.Point(coords)\n    \n    # Create a buffer around the point\n    aoi = point.buffer(buffer_distance)\n    \n    # Get the region coordinates\n    region_coords = aoi.bounds().getInfo()['coordinates']\n    \n    print(f\"Created AOI with {buffer_distance/1000} km buffer around {coords}\")\n    \n    return aoi, region_coords\n</pre> def define_aoi(coords, buffer_distance=20000):     \"\"\"     Define an area of interest around a point.          Args:         coords (list): [longitude, latitude] of the point         buffer_distance (int): Buffer distance in meters              Returns:         tuple: (ee.Geometry, list) - AOI and region coordinates     \"\"\"     # Create a point from coordinates     point = ee.Geometry.Point(coords)          # Create a buffer around the point     aoi = point.buffer(buffer_distance)          # Get the region coordinates     region_coords = aoi.bounds().getInfo()['coordinates']          print(f\"Created AOI with {buffer_distance/1000} km buffer around {coords}\")          return aoi, region_coords In\u00a0[23]: Copied! <pre>def download_dem(aoi, region_coords, scale, description, utm_epsg, collection=\"JAXA/ALOS/AW3D30/V3_2\", band=\"DSM\"):\n    \"\"\"\n    Download DEM data for the specified area.\n    \n    Args:\n        aoi (ee.Geometry): Area of interest\n        region_coords (list): Region coordinates\n        scale (int): Resolution in meters\n        description (str): Description for the export task\n        utm_epsg (str): UTM projection code\n        collection (str): GEE collection ID\n        band (str): Band to select\n        \n    Returns:\n        ee.batch.Task: Export task\n    \"\"\"\n    # Load DEM collection\n    dem_collection = ee.ImageCollection(collection).select(band)\n    \n    # Create a mosaic (combines all images in the collection)\n    dem_mosaic = dem_collection.mosaic().clip(aoi)\n    \n    # Reproject to UTM with specified scale\n    utm_projection = ee.Projection(utm_epsg).atScale(scale)\n    dem_utm = dem_mosaic.reproject(utm_projection)\n    \n    # Export to Google Drive\n    task = ee.batch.Export.image.toDrive(\n        image=dem_utm,\n        description=f\"{description}_{scale}m_Buffer_UTM\",\n        scale=scale,\n        region=region_coords,\n        crs=utm_epsg,\n        maxPixels=1e13,\n        fileFormat='GeoTIFF'\n    )\n    \n    # Start the task\n    task.start()\n    \n    print(f\"DEM export started: {description}_{scale}m_Buffer_UTM.tif\")\n    print(f\"This file will be available in your Google Drive when processing completes\")\n    \n    return task\n</pre> def download_dem(aoi, region_coords, scale, description, utm_epsg, collection=\"JAXA/ALOS/AW3D30/V3_2\", band=\"DSM\"):     \"\"\"     Download DEM data for the specified area.          Args:         aoi (ee.Geometry): Area of interest         region_coords (list): Region coordinates         scale (int): Resolution in meters         description (str): Description for the export task         utm_epsg (str): UTM projection code         collection (str): GEE collection ID         band (str): Band to select              Returns:         ee.batch.Task: Export task     \"\"\"     # Load DEM collection     dem_collection = ee.ImageCollection(collection).select(band)          # Create a mosaic (combines all images in the collection)     dem_mosaic = dem_collection.mosaic().clip(aoi)          # Reproject to UTM with specified scale     utm_projection = ee.Projection(utm_epsg).atScale(scale)     dem_utm = dem_mosaic.reproject(utm_projection)          # Export to Google Drive     task = ee.batch.Export.image.toDrive(         image=dem_utm,         description=f\"{description}_{scale}m_Buffer_UTM\",         scale=scale,         region=region_coords,         crs=utm_epsg,         maxPixels=1e13,         fileFormat='GeoTIFF'     )          # Start the task     task.start()          print(f\"DEM export started: {description}_{scale}m_Buffer_UTM.tif\")     print(f\"This file will be available in your Google Drive when processing completes\")          return task In\u00a0[24]: Copied! <pre>def download_landcover(aoi, region_coords, year, scale, description, utm_epsg):\n    \"\"\"\n    Download land cover data for the specified area.\n    \n    Args:\n        aoi (ee.Geometry): Area of interest\n        region_coords (list): Region coordinates\n        year (int): Year of land cover data\n        scale (int): Resolution in meters\n        description (str): Description for the export task\n        utm_epsg (str): UTM projection code\n        \n    Returns:\n        ee.batch.Task: Export task\n    \"\"\"\n    # Load land cover dataset for the specified year\n    dataset = ee.Image(f'COPERNICUS/Landcover/100m/Proba-V-C3/Global/{year}').select('discrete_classification')\n    \n    # Clip to AOI\n    clipped_dataset = dataset.clip(aoi)\n    \n    # Reproject to UTM\n    utm_projection = ee.Projection(utm_epsg).atScale(scale)\n    dataset_utm = clipped_dataset.reproject(utm_projection)\n    \n    # Export to Google Drive\n    task = ee.batch.Export.image.toDrive(\n        image=dataset_utm,\n        description=f\"{description}_{year}_{scale}m_Buffer_UTM\",\n        scale=scale,\n        region=region_coords,\n        crs=utm_epsg,\n        maxPixels=1e13,\n        fileFormat='GeoTIFF'\n    )\n    \n    # Start the task\n    task.start()\n    \n    print(f\"Land cover export started: {description}_{year}_{scale}m_Buffer_UTM.tif\")\n    print(f\"This file will be available in your Google Drive when processing completes\")\n    \n    return task\n</pre> def download_landcover(aoi, region_coords, year, scale, description, utm_epsg):     \"\"\"     Download land cover data for the specified area.          Args:         aoi (ee.Geometry): Area of interest         region_coords (list): Region coordinates         year (int): Year of land cover data         scale (int): Resolution in meters         description (str): Description for the export task         utm_epsg (str): UTM projection code              Returns:         ee.batch.Task: Export task     \"\"\"     # Load land cover dataset for the specified year     dataset = ee.Image(f'COPERNICUS/Landcover/100m/Proba-V-C3/Global/{year}').select('discrete_classification')          # Clip to AOI     clipped_dataset = dataset.clip(aoi)          # Reproject to UTM     utm_projection = ee.Projection(utm_epsg).atScale(scale)     dataset_utm = clipped_dataset.reproject(utm_projection)          # Export to Google Drive     task = ee.batch.Export.image.toDrive(         image=dataset_utm,         description=f\"{description}_{year}_{scale}m_Buffer_UTM\",         scale=scale,         region=region_coords,         crs=utm_epsg,         maxPixels=1e13,         fileFormat='GeoTIFF'     )          # Start the task     task.start()          print(f\"Land cover export started: {description}_{year}_{scale}m_Buffer_UTM.tif\")     print(f\"This file will be available in your Google Drive when processing completes\")          return task In\u00a0[25]: Copied! <pre>def download_volcano_data(volcano_name, lat, lon, buffer_distance=20000, scale=100, year=2019, utm_epsg=None):\n    \"\"\"\n    Download DEM and land cover data for a volcano with automatic UTM projection detection.\n    \n    Args:\n        volcano_name (str): Name of the volcano\n        lat (float): Latitude of the volcano summit\n        lon (float): Longitude of the volcano summit\n        buffer_distance (int): Buffer distance in meters\n        scale (int): Resolution in meters\n        year (int): Year of land cover data\n        utm_epsg (str, optional): UTM projection code. If None, it will be determined automatically.\n        \n    Returns:\n        tuple: (ee.batch.Task, ee.batch.Task) - DEM and land cover export tasks\n    \"\"\"\n    # Get the proper UTM EPSG if not provided\n    if utm_epsg is None:\n        utm_epsg = get_utm_epsg(lat, lon)\n    \n    # Define area of interest\n    coords = [lon, lat]  # GEE uses [longitude, latitude] order\n    aoi, region_coords = define_aoi(coords, buffer_distance)\n    \n    # Download DEM\n    dem_task = download_dem(\n        aoi, \n        region_coords, \n        scale, \n        f\"{volcano_name}_DEM\", \n        utm_epsg\n    )\n    \n    # Download land cover\n    lc_task = download_landcover(\n        aoi, \n        region_coords, \n        year, \n        scale, \n        f\"{volcano_name}_LandCover\", \n        utm_epsg\n    )\n    \n    print(f\"Data export initiated for {volcano_name}\")\n    print(f\"Using {utm_epsg} projection (automatically determined)\")\n    print(f\"Buffer: {buffer_distance/1000} km, Resolution: {scale} m\")\n    print(f\"The processing will continue in the cloud even if you close this notebook\")\n    \n    return dem_task, lc_task\n</pre> def download_volcano_data(volcano_name, lat, lon, buffer_distance=20000, scale=100, year=2019, utm_epsg=None):     \"\"\"     Download DEM and land cover data for a volcano with automatic UTM projection detection.          Args:         volcano_name (str): Name of the volcano         lat (float): Latitude of the volcano summit         lon (float): Longitude of the volcano summit         buffer_distance (int): Buffer distance in meters         scale (int): Resolution in meters         year (int): Year of land cover data         utm_epsg (str, optional): UTM projection code. If None, it will be determined automatically.              Returns:         tuple: (ee.batch.Task, ee.batch.Task) - DEM and land cover export tasks     \"\"\"     # Get the proper UTM EPSG if not provided     if utm_epsg is None:         utm_epsg = get_utm_epsg(lat, lon)          # Define area of interest     coords = [lon, lat]  # GEE uses [longitude, latitude] order     aoi, region_coords = define_aoi(coords, buffer_distance)          # Download DEM     dem_task = download_dem(         aoi,          region_coords,          scale,          f\"{volcano_name}_DEM\",          utm_epsg     )          # Download land cover     lc_task = download_landcover(         aoi,          region_coords,          year,          scale,          f\"{volcano_name}_LandCover\",          utm_epsg     )          print(f\"Data export initiated for {volcano_name}\")     print(f\"Using {utm_epsg} projection (automatically determined)\")     print(f\"Buffer: {buffer_distance/1000} km, Resolution: {scale} m\")     print(f\"The processing will continue in the cloud even if you close this notebook\")          return dem_task, lc_task In\u00a0[26]: Copied! <pre># Mount Marapi coordinates in Sumatra\nMARAPI_LAT = -0.3775\nMARAPI_LON = 100.4721\n\n# Download data for Mount Marapi with automatic UTM detection\nmarapi_tasks = download_volcano_data(\n    volcano_name=\"Marapi\", \n    lat=MARAPI_LAT,\n    lon=MARAPI_LON,\n    buffer_distance=20000, \n    scale=100\n)\n</pre> # Mount Marapi coordinates in Sumatra MARAPI_LAT = -0.3775 MARAPI_LON = 100.4721  # Download data for Mount Marapi with automatic UTM detection marapi_tasks = download_volcano_data(     volcano_name=\"Marapi\",      lat=MARAPI_LAT,     lon=MARAPI_LON,     buffer_distance=20000,      scale=100 ) <pre>Coordinates (-0.3775, 100.47210000000001) are in UTM zone 47\nUsing projection EPSG:32747\nCreated AOI with 20.0 km buffer around [100.4721, -0.3775]\nDEM export started: Marapi_DEM_100m_Buffer_UTM.tif\nThis file will be available in your Google Drive when processing completes\nLand cover export started: Marapi_LandCover_2019_100m_Buffer_UTM.tif\nThis file will be available in your Google Drive when processing completes\nData export initiated for Marapi\nUsing EPSG:32747 projection (automatically determined)\nBuffer: 20.0 km, Resolution: 100 m\nThe processing will continue in the cloud even if you close this notebook\n</pre> In\u00a0[\u00a0]: Copied! <pre># Mount Awu coordinates in Sangihe Islands\nAWU_LAT = 3.6828460\nAWU_LON = 125.455980\n\n# Download data for Mount Awu with automatic UTM detection\nawu_tasks = download_volcano_data(\n    volcano_name=\"Awu\", \n    lat=AWU_LAT,\n    lon=AWU_LON,\n    buffer_distance=20000,\n    scale=100\n)\n</pre> # Mount Awu coordinates in Sangihe Islands AWU_LAT = 3.6828460 AWU_LON = 125.455980  # Download data for Mount Awu with automatic UTM detection awu_tasks = download_volcano_data(     volcano_name=\"Awu\",      lat=AWU_LAT,     lon=AWU_LON,     buffer_distance=20000,     scale=100 )  <pre>Coordinates (3.682846, 125.45598000000001) are in UTM zone 51\nUsing projection EPSG:32651\nCreated AOI with 15.0 km buffer around [125.45598, 3.682846]\nDEM export started: Awu_DEM_100m_Buffer_UTM.tif\nThis file will be available in your Google Drive when processing completes\nLand cover export started: Awu_LandCover_2019_100m_Buffer_UTM.tif\nThis file will be available in your Google Drive when processing completes\nData export initiated for Awu\nUsing EPSG:32651 projection (automatically determined)\nBuffer: 15.0 km, Resolution: 100 m\nThe processing will continue in the cloud even if you close this notebook\n</pre>"},{"location":"demo/data-download.html#volcano-data-acquisition-with-google-earth-engine","title":"Volcano Data Acquisition with Google Earth Engine\u00b6","text":"<p>This notebook demonstrates how to download Digital Elevation Models (DEMs) and Land Cover data for volcanic regions using Google Earth Engine. These datasets are essential for creating cost surfaces and analyzing evacuation routes.</p>"},{"location":"demo/data-download.html#why-is-this-data-important","title":"Why is this data important?\u00b6","text":"<p>For volcanic evacuation analysis, we need:</p> <ol> <li>Digital Elevation Model (DEM) - Provides terrain elevation data used to calculate slopes and identify evacuation challenges</li> <li>Land Cover Classification - Identifies different types of terrain (forests, fields, water bodies, etc.) that affect travel speed</li> <li>Geographic Context - Buffers around volcanoes provide the necessary spatial extent for comprehensive evacuation planning</li> </ol> <p>In this notebook, we'll download this data for Mount Marapi and Mount Awu in Indonesia, but the process can be applied to any volcano worldwide.</p>"},{"location":"demo/data-download.html#importing-required-packages","title":"Importing Required Packages\u00b6","text":"<p>First, we need to import the necessary library:</p> <ul> <li>earthengine-api: Provides access to Google Earth Engine</li> </ul>"},{"location":"demo/data-download.html#google-earth-engine-authentication","title":"Google Earth Engine Authentication\u00b6","text":"<p>Before accessing any data, we need to authenticate with Google Earth Engine. This function will:</p> <ol> <li>Check if you're already authenticated</li> <li>If not, it will open a browser window for you to sign in with your Google account</li> <li>Authorize the Earth Engine application</li> </ol>"},{"location":"demo/data-download.html#determining-the-correct-utm-projection","title":"Determining the Correct UTM Projection\u00b6","text":"<p>When working with geospatial data for specific locations, using the correct Universal Transverse Mercator (UTM) projection is crucial for accurate distance and area measurements. Rather than manually specifying the EPSG code, we can calculate it automatically based on the volcano's coordinates.</p> <p>The function below:</p> <ol> <li>Takes latitude and longitude as input</li> <li>Calculates the appropriate UTM zone number (1-60)</li> <li>Determines whether the location is in the northern or southern hemisphere</li> <li>Returns the correct EPSG code for that UTM zone</li> </ol> <p>This ensures that our data will always be processed in the most appropriate projection for the volcano's location.</p>"},{"location":"demo/data-download.html#defining-the-area-of-interest-aoi","title":"Defining the Area of Interest (AOI)\u00b6","text":"<p>For each volcano, we need to define an area of interest by:</p> <ol> <li>Creating a point at the volcano's coordinates (longitude, latitude)</li> <li>Creating a buffer around this point</li> <li>Extracting the region coordinates for export boundaries</li> </ol> <p>This function returns both the Earth Engine geometry object and the region coordinates.</p>"},{"location":"demo/data-download.html#downloading-digital-elevation-model-dem-data","title":"Downloading Digital Elevation Model (DEM) Data\u00b6","text":"<p>Now we'll create a function to download DEM data. We're using the ALOS World 3D-30m dataset from JAXA, which provides global elevation data at approximately 30-meter resolution.</p> <p>This function:</p> <ol> <li>Loads the DEM collection from Google Earth Engine</li> <li>Creates a mosaic of all available images</li> <li>Clips it to our area of interest</li> <li>Reprojects to a UTM projection appropriate for the location</li> <li>Exports the result to Google Drive as a GeoTIFF file</li> </ol> <p>The function returns an export task that runs asynchronously in the cloud.</p>"},{"location":"demo/data-download.html#downloading-land-cover-data","title":"Downloading Land Cover Data\u00b6","text":"<p>Next, we'll create a function to download land cover data. We're using the Copernicus Global Land Cover dataset, which provides global land cover classification at 100-meter resolution.</p> <p>The land cover dataset classifies each pixel into categories such as:</p> <ul> <li>Forests (different types)</li> <li>Shrublands</li> <li>Grasslands</li> <li>Croplands</li> <li>Urban areas</li> <li>Water bodies</li> </ul> <p>Like the DEM function, this function exports the data to Google Drive as a GeoTIFF file.</p>"},{"location":"demo/data-download.html#main-function-for-downloading-volcano-data","title":"Main Function for Downloading Volcano Data\u00b6","text":"<p>Now let's create a main function that combines all the previous functions to download both DEM and land cover data for a specified volcano. This function:</p> <ol> <li>Authenticates with Google Earth Engine</li> <li>Defines the area of interest around the volcano</li> <li>Downloads the DEM data</li> <li>Downloads the land cover data</li> <li>Returns the export tasks for status monitoring</li> </ol> <p>This modular approach makes it easy to download data for multiple volcanoes with different parameters.</p>"},{"location":"demo/data-download.html#parameters","title":"Parameters:\u00b6","text":"<ul> <li>volcano_name: Name of the volcano (used in output filenames)</li> <li>coords: [longitude, latitude] coordinates of the volcano</li> <li>buffer_distance: Buffer distance in meters (default: 20000 m, or 20 km)</li> <li>scale: Resolution in meters (default: 100 m)</li> <li>year: Year of land cover data (default: 2019)</li> <li>utm_epsg: UTM projection code appropriate for the volcano's location</li> </ul>"},{"location":"demo/data-download.html#download-data-for-mount-marapi","title":"Download Data for Mount Marapi\u00b6","text":"<p>Let's use our function to download data for Mount Marapi in Sumatra, Indonesia. Mount Marapi is an active stratovolcano and one of the most active volcanoes in Sumatra.</p> <ul> <li>Location: 100.4721\u00b0E, -0.3775\u00b0N</li> </ul> <p>For Mount Marapi, we'll:</p> <ol> <li>We just provide the name and coordinates</li> <li>We use a 20 km buffer and 100 m resolution</li> </ol>"},{"location":"demo/data-download.html#download-data-for-mount-awu","title":"Download Data for Mount Awu\u00b6","text":"<p>Now let's download data for Mount Awu in the Sangihe Islands, Indonesia. Mount Awu is a stratovolcano with a history of explosive eruptions.</p> <ul> <li>Location: 125.4542\u00b0E, 3.6735\u00b0N</li> </ul> <p>For Mount Awu, we'll:</p> <ol> <li>Use a 20 km buffer</li> <li>Set the resolution to 100 meters (as with Marapi)</li> </ol>"},{"location":"demo/data-download.html#after-downloading-next-steps","title":"After Downloading: Next Steps\u00b6","text":"<p>After your export tasks complete, here's what to do next:</p> <ol> <li><p>Download the files from Google Drive:</p> <ul> <li>Navigate to your Google Drive</li> <li>Look for files named <code>Marapi_DEM_100m_Buffer_UTM.tif</code> and <code>Marapi_LandCover_2019_100m_Buffer_UTM.tif</code> (similarly for Awu)</li> <li>Download these files to your local project directory</li> </ul> </li> <li><p>Verify the data:</p> <ul> <li>Open the files in a GIS software like QGIS or ArcGIS</li> <li>Check that the data covers the expected area</li> <li>Verify that there are no major gaps or errors</li> </ul> </li> <li><p>Proceed to the next step:</p> </li> </ol> <ul> <li>Continue to the Cost Surface Generation notebook</li> <li>There you'll use these datasets to create cost surfaces for evacuation analysis</li> </ul>"},{"location":"demo/data-download.html#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we've:</p> <ol> <li>Set up Google Earth Engine for geospatial data access</li> <li>Downloaded DEM data from the ALOS World 3D-30m dataset</li> <li>Downloaded land cover data from the Copernicus Global Land Cover dataset</li> <li>Created a reusable module for downloading data for any volcano</li> </ol> <p>These datasets will form the foundation for our volcano evacuation analysis. In the next notebook, we'll use this data to:</p> <ol> <li>Calculate slopes from the DEM</li> <li>Assign travel speeds to different land cover types</li> <li>Combine these factors to create cost surfaces</li> <li>Use these cost surfaces for evacuation routing</li> </ol>"},{"location":"demo/awu/evacuation-analysis.html","title":"Volcanic Evacuation Analysis","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport os\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom config import *\nfrom io_utils import (read_shapefile, read_raster, save_raster, \n                     save_analysis_report, save_metrics_csv)\nfrom grid_utils import (coords_to_raster, process_raster, \n                       calculate_distance_from_summit, raster_coord_to_map_coords, to_1d)\nfrom path_utils import build_adjacency_matrix, reconstruct_path\nfrom analysis import (run_dijkstra_analysis, process_travel_times, \n                     analyze_safe_zones)\nfrom visualization import (plot_travel_time_comparison, create_cost_surface_subplots,\n                          load_raster, create_decomposition_table, create_final_evacuation_table)\nfrom pyproj import CRS, Transformer\nfrom decomposition import run_decomposition_analysis\n</pre> import pandas as pd import os import numpy as np import time import matplotlib.pyplot as plt from config import * from io_utils import (read_shapefile, read_raster, save_raster,                       save_analysis_report, save_metrics_csv) from grid_utils import (coords_to_raster, process_raster,                         calculate_distance_from_summit, raster_coord_to_map_coords, to_1d) from path_utils import build_adjacency_matrix, reconstruct_path from analysis import (run_dijkstra_analysis, process_travel_times,                       analyze_safe_zones) from visualization import (plot_travel_time_comparison, create_cost_surface_subplots,                           load_raster, create_decomposition_table, create_final_evacuation_table) from pyproj import CRS, Transformer from decomposition import run_decomposition_analysis In\u00a0[2]: Copied! <pre># Initialize dictionaries to store results\nall_results = {}\ndataset_info = {}\nmin_coords_all = {}\n\nprint(\"All required packages have been imported.\")\nprint(f\"The analysis will use {len(COST_PATHS)} cost datasets: {list(COST_PATHS.keys())}\")\nprint(f\"Walking speeds: {WALKING_SPEEDS}\")\nprint(f\"Safe zone distances: {SAFE_ZONE_DISTANCES}\")\n</pre> # Initialize dictionaries to store results all_results = {} dataset_info = {} min_coords_all = {}  print(\"All required packages have been imported.\") print(f\"The analysis will use {len(COST_PATHS)} cost datasets: {list(COST_PATHS.keys())}\") print(f\"Walking speeds: {WALKING_SPEEDS}\") print(f\"Safe zone distances: {SAFE_ZONE_DISTANCES}\") <pre>All required packages have been imported.\nThe analysis will use 4 cost datasets: ['final', 'modify_landcover', 'walking_speed', 'base_cost']\nWalking speeds: {'slow': 0.91, 'medium': 1.22, 'fast': 1.52}\nSafe zone distances: [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]\n</pre> In\u00a0[3]: Copied! <pre>print(\"\\n=== SECTION 1: LOADING GEOGRAPHIC DATA ===\")\nstart_time = time.time()\n\n# Read common shapefiles\nprint(\"Reading shapefiles...\")\nsummit_gdf = read_shapefile(SUMMIT_PATH)\ncamp_gdf = read_shapefile(CAMP_SPOT_PATH)\nhiking_gdf = read_shapefile(HIKING_PATH)\n\nend_time = time.time()\nprint(f\"Geographic data loaded in {end_time - start_time:.2f} seconds\")\n\n# Display basic information about the loaded data\nprint(f\"Summit data: {summit_gdf.shape[0]} points loaded\")\nprint(f\"Camp data: {camp_gdf.shape[0]} points loaded\")\nprint(f\"Hiking path data: {hiking_gdf.shape[0]} features loaded\")\n</pre> print(\"\\n=== SECTION 1: LOADING GEOGRAPHIC DATA ===\") start_time = time.time()  # Read common shapefiles print(\"Reading shapefiles...\") summit_gdf = read_shapefile(SUMMIT_PATH) camp_gdf = read_shapefile(CAMP_SPOT_PATH) hiking_gdf = read_shapefile(HIKING_PATH)  end_time = time.time() print(f\"Geographic data loaded in {end_time - start_time:.2f} seconds\")  # Display basic information about the loaded data print(f\"Summit data: {summit_gdf.shape[0]} points loaded\") print(f\"Camp data: {camp_gdf.shape[0]} points loaded\") print(f\"Hiking path data: {hiking_gdf.shape[0]} features loaded\")  <pre>\n=== SECTION 1: LOADING GEOGRAPHIC DATA ===\nReading shapefiles...\nReading shapefile: summit_awu_correct_proj_final.shp\nReading shapefile: campspot_awu_correct_proj_final.shp\nReading shapefile: hikingpath_awu_buffer_correctproj_final.shp\nGeographic data loaded in 0.05 seconds\nSummit data: 1 points loaded\nCamp data: 1 points loaded\nHiking path data: 1 features loaded\n</pre> In\u00a0[4]: Copied! <pre>for dataset_key, current_cost_path in COST_PATHS.items():\n    print(f\"\\n=== PROCESSING COST DATASET: {dataset_key} ===\")\n    process_start_time = time.time()\n    \n    print(f\"Step 1: Reading cost raster from {os.path.basename(current_cost_path)}\")\n    cost_array, meta, transform, nodata, bounds, resolution = read_raster(current_cost_path)\n    print(f\"Cost raster shape: {cost_array.shape}\")\n    \n    # Expand single-band raster to 8 directional bands if needed\n    if cost_array.shape[0] == 1:\n        print(f\"Step 2: Expanding single-band raster to 8 directional bands\")\n        single_band = cost_array[0]\n        expanded = np.zeros((8, *single_band.shape), dtype=single_band.dtype)\n        for i in range(8):\n            expanded[i] = single_band.copy()\n            if i &gt;= 4:  # Diagonal directions\n                expanded[i] *= np.sqrt(2)\n        cost_array = expanded\n        print(f\"Expanded cost array shape: {cost_array.shape}\")\n    else:\n        print(f\"Step 2: Cost raster already has {cost_array.shape[0]} bands, no expansion needed\")\n    \n    # Convert coordinates to raster indices\n    print(\"Step 3: Converting geographic coordinates to raster indices\")\n    summit_rc = coords_to_raster(summit_gdf, transform, bounds, resolution)\n    camp_rc = coords_to_raster(camp_gdf, transform, bounds, resolution)\n    source_coords = [summit_rc[0]] + camp_rc\n    print(f\"Summit raster coordinates: {summit_rc[0]}\")\n    print(f\"Camp raster coordinates: {camp_rc}\")\n    \n    # Build adjacency matrix\n    print(\"Step 4: Building adjacency matrix for path finding\")\n    adj_start_time = time.time()\n    bands, rows, cols = cost_array.shape\n    graph_csr = build_adjacency_matrix(cost_array, rows, cols, DIRECTIONS)\n    adj_end_time = time.time()\n    print(f\"Adjacency matrix built in {adj_end_time - adj_start_time:.2f} seconds\")\n    print(f\"Matrix shape: {graph_csr.shape}, Non-zero elements: {graph_csr.nnz}\")\n    \n    # Run Dijkstra's algorithm\n    print(\"Step 5: Running Dijkstra's algorithm for shortest paths\")\n    dijkstra_start_time = time.time()\n    source_nodes = [to_1d(r, c, cols) for (r, c) in source_coords]\n    distances, predecessors = run_dijkstra_analysis(graph_csr, source_nodes)\n    dijkstra_end_time = time.time()\n    print(f\"Dijkstra's algorithm completed in {dijkstra_end_time - dijkstra_start_time:.2f} seconds\")\n    \n    # Save base cost distance rasters\n    print(\"Step 6: Saving base cost distance rasters\")\n    for i, source_name in enumerate(SOURCE_NAMES):\n        source_distance = distances[i, :].reshape(rows, cols)\n        source_distance[np.isinf(source_distance)] = -1\n        out_filename = f'cost_distance_{source_name}_{dataset_key}.tif'\n        save_raster(os.path.join(DATA_FOLDER, out_filename), source_distance, meta)\n        print(f\"Saved: {out_filename}\")\n    \n    # Store dataset info for later use\n    dataset_info[dataset_key] = {\n        \"pred_summit\": predecessors[0],\n        \"rows\": rows,\n        \"cols\": cols,\n        \"transform\": transform,\n        \"meta\": meta,  # Add this line to store the metadata\n        \"summit_raster_coords\": summit_rc[0]\n    }\n    \n    process_end_time = time.time()\n    print(f\"Dataset {dataset_key} processed in {process_end_time - process_start_time:.2f} seconds\")\n</pre> for dataset_key, current_cost_path in COST_PATHS.items():     print(f\"\\n=== PROCESSING COST DATASET: {dataset_key} ===\")     process_start_time = time.time()          print(f\"Step 1: Reading cost raster from {os.path.basename(current_cost_path)}\")     cost_array, meta, transform, nodata, bounds, resolution = read_raster(current_cost_path)     print(f\"Cost raster shape: {cost_array.shape}\")          # Expand single-band raster to 8 directional bands if needed     if cost_array.shape[0] == 1:         print(f\"Step 2: Expanding single-band raster to 8 directional bands\")         single_band = cost_array[0]         expanded = np.zeros((8, *single_band.shape), dtype=single_band.dtype)         for i in range(8):             expanded[i] = single_band.copy()             if i &gt;= 4:  # Diagonal directions                 expanded[i] *= np.sqrt(2)         cost_array = expanded         print(f\"Expanded cost array shape: {cost_array.shape}\")     else:         print(f\"Step 2: Cost raster already has {cost_array.shape[0]} bands, no expansion needed\")          # Convert coordinates to raster indices     print(\"Step 3: Converting geographic coordinates to raster indices\")     summit_rc = coords_to_raster(summit_gdf, transform, bounds, resolution)     camp_rc = coords_to_raster(camp_gdf, transform, bounds, resolution)     source_coords = [summit_rc[0]] + camp_rc     print(f\"Summit raster coordinates: {summit_rc[0]}\")     print(f\"Camp raster coordinates: {camp_rc}\")          # Build adjacency matrix     print(\"Step 4: Building adjacency matrix for path finding\")     adj_start_time = time.time()     bands, rows, cols = cost_array.shape     graph_csr = build_adjacency_matrix(cost_array, rows, cols, DIRECTIONS)     adj_end_time = time.time()     print(f\"Adjacency matrix built in {adj_end_time - adj_start_time:.2f} seconds\")     print(f\"Matrix shape: {graph_csr.shape}, Non-zero elements: {graph_csr.nnz}\")          # Run Dijkstra's algorithm     print(\"Step 5: Running Dijkstra's algorithm for shortest paths\")     dijkstra_start_time = time.time()     source_nodes = [to_1d(r, c, cols) for (r, c) in source_coords]     distances, predecessors = run_dijkstra_analysis(graph_csr, source_nodes)     dijkstra_end_time = time.time()     print(f\"Dijkstra's algorithm completed in {dijkstra_end_time - dijkstra_start_time:.2f} seconds\")          # Save base cost distance rasters     print(\"Step 6: Saving base cost distance rasters\")     for i, source_name in enumerate(SOURCE_NAMES):         source_distance = distances[i, :].reshape(rows, cols)         source_distance[np.isinf(source_distance)] = -1         out_filename = f'cost_distance_{source_name}_{dataset_key}.tif'         save_raster(os.path.join(DATA_FOLDER, out_filename), source_distance, meta)         print(f\"Saved: {out_filename}\")          # Store dataset info for later use     dataset_info[dataset_key] = {         \"pred_summit\": predecessors[0],         \"rows\": rows,         \"cols\": cols,         \"transform\": transform,         \"meta\": meta,  # Add this line to store the metadata         \"summit_raster_coords\": summit_rc[0]     }          process_end_time = time.time()     print(f\"Dataset {dataset_key} processed in {process_end_time - process_start_time:.2f} seconds\") <pre>\n=== PROCESSING COST DATASET: final ===\nStep 1: Reading cost raster from Awu_inverted_cost_8_directions_original.tif\nReading raster: Awu_inverted_cost_8_directions_original.tif\nCost raster shape: (8, 400, 401)\nStep 2: Cost raster already has 8 bands, no expansion needed\nStep 3: Converting geographic coordinates to raster indices\nConverting coordinates to raster indices...\nConverting coordinates to raster indices...\nSummit raster coordinates: (198, 200)\nCamp raster coordinates: [(215, 209)]\nStep 4: Building adjacency matrix for path finding\n\nBuilding adjacency matrix...\n</pre> <pre>Processing rows: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:13&lt;00:00, 29.64it/s]\n</pre> <pre>Adjacency matrix built in 13.78 seconds\nMatrix shape: (160400, 160400)\nNumber of non-zero elements: 1278398\nAdjacency matrix built in 13.83 seconds\nMatrix shape: (160400, 160400), Non-zero elements: 1278398\nStep 5: Running Dijkstra's algorithm for shortest paths\n\nRunning Dijkstra's algorithm...\nDijkstra's algorithm completed in 0.15 seconds\nDijkstra's algorithm completed in 0.15 seconds\nStep 6: Saving base cost distance rasters\nSaved: cost_distance_summit_final.tif\nSaved: cost_distance_camp1_final.tif\nDataset final processed in 14.51 seconds\n\n=== PROCESSING COST DATASET: modify_landcover ===\nStep 1: Reading cost raster from Awu_inverted_cost_8_directions_modified.tif\nReading raster: Awu_inverted_cost_8_directions_modified.tif\nCost raster shape: (8, 400, 401)\nStep 2: Cost raster already has 8 bands, no expansion needed\nStep 3: Converting geographic coordinates to raster indices\nConverting coordinates to raster indices...\nConverting coordinates to raster indices...\nSummit raster coordinates: (198, 200)\nCamp raster coordinates: [(215, 209)]\nStep 4: Building adjacency matrix for path finding\n\nBuilding adjacency matrix...\n</pre> <pre>Processing rows: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:14&lt;00:00, 27.19it/s]\n</pre> <pre>Adjacency matrix built in 15.05 seconds\nMatrix shape: (160400, 160400)\nNumber of non-zero elements: 1278398\nAdjacency matrix built in 15.11 seconds\nMatrix shape: (160400, 160400), Non-zero elements: 1278398\nStep 5: Running Dijkstra's algorithm for shortest paths\n\nRunning Dijkstra's algorithm...\nDijkstra's algorithm completed in 0.18 seconds\nDijkstra's algorithm completed in 0.18 seconds\nStep 6: Saving base cost distance rasters\nSaved: cost_distance_summit_modify_landcover.tif\nSaved: cost_distance_camp1_modify_landcover.tif\nDataset modify_landcover processed in 15.84 seconds\n\n=== PROCESSING COST DATASET: walking_speed ===\nStep 1: Reading cost raster from inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif\nReading raster: inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif\nCost raster shape: (8, 400, 401)\nStep 2: Cost raster already has 8 bands, no expansion needed\nStep 3: Converting geographic coordinates to raster indices\nConverting coordinates to raster indices...\nConverting coordinates to raster indices...\nSummit raster coordinates: (198, 200)\nCamp raster coordinates: [(215, 209)]\nStep 4: Building adjacency matrix for path finding\n\nBuilding adjacency matrix...\n</pre> <pre>Processing rows: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:15&lt;00:00, 26.41it/s]\n</pre> <pre>Adjacency matrix built in 15.48 seconds\nMatrix shape: (160400, 160400)\nNumber of non-zero elements: 1278398\nAdjacency matrix built in 15.54 seconds\nMatrix shape: (160400, 160400), Non-zero elements: 1278398\nStep 5: Running Dijkstra's algorithm for shortest paths\n\nRunning Dijkstra's algorithm...\nDijkstra's algorithm completed in 0.17 seconds\nDijkstra's algorithm completed in 0.17 seconds\nStep 6: Saving base cost distance rasters\nSaved: cost_distance_summit_walking_speed.tif\nSaved: cost_distance_camp1_walking_speed.tif\nDataset walking_speed processed in 16.17 seconds\n\n=== PROCESSING COST DATASET: base_cost ===\nStep 1: Reading cost raster from invert_landcovercost_original_Awu.tif\nReading raster: invert_landcovercost_original_Awu.tif\nCost raster shape: (1, 400, 401)\nStep 2: Expanding single-band raster to 8 directional bands\nExpanded cost array shape: (8, 400, 401)\nStep 3: Converting geographic coordinates to raster indices\nConverting coordinates to raster indices...\nConverting coordinates to raster indices...\nSummit raster coordinates: (198, 200)\nCamp raster coordinates: [(215, 209)]\nStep 4: Building adjacency matrix for path finding\n\nBuilding adjacency matrix...\n</pre> <pre>Processing rows: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:15&lt;00:00, 25.61it/s]\n</pre> <pre>Adjacency matrix built in 16.00 seconds\nMatrix shape: (160400, 160400)\nNumber of non-zero elements: 1278398\nAdjacency matrix built in 16.06 seconds\nMatrix shape: (160400, 160400), Non-zero elements: 1278398\nStep 5: Running Dijkstra's algorithm for shortest paths\n\nRunning Dijkstra's algorithm...\nDijkstra's algorithm completed in 0.16 seconds\nDijkstra's algorithm completed in 0.16 seconds\nStep 6: Saving base cost distance rasters\nSaved: cost_distance_summit_base_cost.tif\nSaved: cost_distance_camp1_base_cost.tif\nDataset base_cost processed in 16.69 seconds\n</pre> In\u00a0[5]: Copied! <pre># Process travel times for each dataset\nfor dataset_key in COST_PATHS.keys():\n    print(f\"\\n=== CALCULATING TRAVEL TIMES FOR: {dataset_key} ===\")\n    travel_time_start = time.time()\n    \n    # Process travel times for each walking speed and source\n    travel_time_data = {}\n    for speed_name, speed_value in WALKING_SPEEDS.items():\n        print(f\"Processing {speed_name} walking speed ({speed_value} m/s)\")\n        travel_time_data[speed_name] = {}\n        \n        for source_name in SOURCE_NAMES:\n            print(f\"  Source: {source_name}\")\n            base_filename = f'cost_distance_{source_name}_{dataset_key}.tif'\n            base_path = os.path.join(DATA_FOLDER, base_filename)\n            \n            # Load the base cost distance and convert to travel time\n            travel_time_array = process_travel_times(\n                base_path, source_name, dataset_key, speed_name, speed_value, \n                meta=dataset_info[dataset_key].get(\"meta\", None)\n            )\n            \n            # Store the travel time data\n            travel_time_data[speed_name][source_name] = {\n                'cost_array': travel_time_array,\n                'cost_array_flat': travel_time_array.ravel()\n            }\n            print(f\"  Travel time range: {np.nanmin(travel_time_array):.2f} to {np.nanmax(travel_time_array):.2f} hours\")\n    \n    # Calculate distance from summit for each cell\n    print(\"Calculating distance from summit for safe zone analysis\")\n    summit_rc = dataset_info[dataset_key][\"summit_raster_coords\"]\n    rows = dataset_info[dataset_key][\"rows\"]\n    cols = dataset_info[dataset_key][\"cols\"]\n    distance_from_summit = calculate_distance_from_summit(summit_rc, rows, cols)\n    print(f\"Maximum distance from summit: {np.max(distance_from_summit):.2f} meters\")\n    \n    # Analyze safe zones\n    print(\"Analyzing minimum travel times within safe zones\")\n    results, min_coords = analyze_safe_zones(\n        distance_from_summit, travel_time_data, SAFE_ZONE_DISTANCES, SOURCE_NAMES\n    )\n    \n    # Save results\n    print(\"Saving analysis results\")\n    base_name = f\"safe_zone_travel_time_{dataset_key}\"\n    save_analysis_report(\n        os.path.join(DATA_FOLDER, f\"{base_name}_report.txt\"),\n        results, min_coords, SOURCE_NAMES, WALKING_SPEEDS, SAFE_ZONE_DISTANCES\n    )\n    save_metrics_csv(\n        os.path.join(DATA_FOLDER, f\"{base_name}_metrics.csv\"),\n        results, min_coords, SOURCE_NAMES, WALKING_SPEEDS, SAFE_ZONE_DISTANCES\n    )\n    \n    # Store results for comparison\n    all_results[dataset_key] = results\n    min_coords_all[dataset_key] = min_coords\n    \n    travel_time_end = time.time()\n    print(f\"Travel times for {dataset_key} calculated in {travel_time_end - travel_time_start:.2f} seconds\")\n</pre> # Process travel times for each dataset for dataset_key in COST_PATHS.keys():     print(f\"\\n=== CALCULATING TRAVEL TIMES FOR: {dataset_key} ===\")     travel_time_start = time.time()          # Process travel times for each walking speed and source     travel_time_data = {}     for speed_name, speed_value in WALKING_SPEEDS.items():         print(f\"Processing {speed_name} walking speed ({speed_value} m/s)\")         travel_time_data[speed_name] = {}                  for source_name in SOURCE_NAMES:             print(f\"  Source: {source_name}\")             base_filename = f'cost_distance_{source_name}_{dataset_key}.tif'             base_path = os.path.join(DATA_FOLDER, base_filename)                          # Load the base cost distance and convert to travel time             travel_time_array = process_travel_times(                 base_path, source_name, dataset_key, speed_name, speed_value,                  meta=dataset_info[dataset_key].get(\"meta\", None)             )                          # Store the travel time data             travel_time_data[speed_name][source_name] = {                 'cost_array': travel_time_array,                 'cost_array_flat': travel_time_array.ravel()             }             print(f\"  Travel time range: {np.nanmin(travel_time_array):.2f} to {np.nanmax(travel_time_array):.2f} hours\")          # Calculate distance from summit for each cell     print(\"Calculating distance from summit for safe zone analysis\")     summit_rc = dataset_info[dataset_key][\"summit_raster_coords\"]     rows = dataset_info[dataset_key][\"rows\"]     cols = dataset_info[dataset_key][\"cols\"]     distance_from_summit = calculate_distance_from_summit(summit_rc, rows, cols)     print(f\"Maximum distance from summit: {np.max(distance_from_summit):.2f} meters\")          # Analyze safe zones     print(\"Analyzing minimum travel times within safe zones\")     results, min_coords = analyze_safe_zones(         distance_from_summit, travel_time_data, SAFE_ZONE_DISTANCES, SOURCE_NAMES     )          # Save results     print(\"Saving analysis results\")     base_name = f\"safe_zone_travel_time_{dataset_key}\"     save_analysis_report(         os.path.join(DATA_FOLDER, f\"{base_name}_report.txt\"),         results, min_coords, SOURCE_NAMES, WALKING_SPEEDS, SAFE_ZONE_DISTANCES     )     save_metrics_csv(         os.path.join(DATA_FOLDER, f\"{base_name}_metrics.csv\"),         results, min_coords, SOURCE_NAMES, WALKING_SPEEDS, SAFE_ZONE_DISTANCES     )          # Store results for comparison     all_results[dataset_key] = results     min_coords_all[dataset_key] = min_coords          travel_time_end = time.time()     print(f\"Travel times for {dataset_key} calculated in {travel_time_end - travel_time_start:.2f} seconds\") <pre>\n=== CALCULATING TRAVEL TIMES FOR: final ===\nProcessing slow walking speed (0.91 m/s)\n  Source: summit\nProcessing travel times for summit at slow speed...\n  Travel time range: 0.00 to 6942791.50 hours\n  Source: camp1\nProcessing travel times for camp1 at slow speed...\n  Travel time range: 0.00 to 6942790.50 hours\nProcessing medium walking speed (1.22 m/s)\n  Source: summit\nProcessing travel times for summit at medium speed...\n  Travel time range: 0.00 to 5178640.00 hours\n  Source: camp1\nProcessing travel times for camp1 at medium speed...\n  Travel time range: 0.00 to 5178639.00 hours\nProcessing fast walking speed (1.52 m/s)\n  Source: summit\nProcessing travel times for summit at fast speed...\n  Travel time range: 0.00 to 4156540.00 hours\n  Source: camp1\nProcessing travel times for camp1 at fast speed...\n  Travel time range: 0.00 to 4156539.25 hours\nCalculating distance from summit for safe zone analysis\nCalculating distance from summit...\nMaximum distance from summit: 28355.07 meters\nAnalyzing minimum travel times within safe zones\n\nAnalyzing safe zones...\n\nProcessing slow speed...\nsummit at 500m: min time = 0.27 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.64 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.92 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 1.21 hrs\ncamp1 at 2000m: min time = 0.03 hrs\nsummit at 2500m: min time = 1.59 hrs\ncamp1 at 2500m: min time = 0.40 hrs\nsummit at 3000m: min time = 1.92 hrs\ncamp1 at 3000m: min time = 0.70 hrs\nsummit at 3500m: min time = 2.13 hrs\ncamp1 at 3500m: min time = 0.88 hrs\nsummit at 4000m: min time = 2.36 hrs\ncamp1 at 4000m: min time = 1.05 hrs\nsummit at 4500m: min time = 2.57 hrs\ncamp1 at 4500m: min time = 1.24 hrs\n\nProcessing medium speed...\nsummit at 500m: min time = 0.20 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.48 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.69 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.90 hrs\ncamp1 at 2000m: min time = 0.03 hrs\nsummit at 2500m: min time = 1.18 hrs\ncamp1 at 2500m: min time = 0.30 hrs\nsummit at 3000m: min time = 1.43 hrs\ncamp1 at 3000m: min time = 0.52 hrs\nsummit at 3500m: min time = 1.59 hrs\ncamp1 at 3500m: min time = 0.66 hrs\nsummit at 4000m: min time = 1.76 hrs\ncamp1 at 4000m: min time = 0.79 hrs\nsummit at 4500m: min time = 1.92 hrs\ncamp1 at 4500m: min time = 0.92 hrs\n\nProcessing fast speed...\nsummit at 500m: min time = 0.16 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.38 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.55 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.72 hrs\ncamp1 at 2000m: min time = 0.02 hrs\nsummit at 2500m: min time = 0.95 hrs\ncamp1 at 2500m: min time = 0.24 hrs\nsummit at 3000m: min time = 1.15 hrs\ncamp1 at 3000m: min time = 0.42 hrs\nsummit at 3500m: min time = 1.27 hrs\ncamp1 at 3500m: min time = 0.53 hrs\nsummit at 4000m: min time = 1.41 hrs\ncamp1 at 4000m: min time = 0.63 hrs\nsummit at 4500m: min time = 1.54 hrs\ncamp1 at 4500m: min time = 0.74 hrs\nSaving analysis results\nTravel times for final calculated in 2.08 seconds\n\n=== CALCULATING TRAVEL TIMES FOR: modify_landcover ===\nProcessing slow walking speed (0.91 m/s)\n  Source: summit\nProcessing travel times for summit at slow speed...\n  Travel time range: 0.00 to 6942952.00 hours\n  Source: camp1\nProcessing travel times for camp1 at slow speed...\n  Travel time range: 0.00 to 6942951.00 hours\nProcessing medium walking speed (1.22 m/s)\n  Source: summit\nProcessing travel times for summit at medium speed...\n  Travel time range: 0.00 to 5178759.50 hours\n  Source: camp1\nProcessing travel times for camp1 at medium speed...\n  Travel time range: 0.00 to 5178759.00 hours\nProcessing fast walking speed (1.52 m/s)\n  Source: summit\nProcessing travel times for summit at fast speed...\n  Travel time range: 0.00 to 4156636.25 hours\n  Source: camp1\nProcessing travel times for camp1 at fast speed...\n  Travel time range: 0.00 to 4156635.25 hours\nCalculating distance from summit for safe zone analysis\nCalculating distance from summit...\nMaximum distance from summit: 28355.07 meters\nAnalyzing minimum travel times within safe zones\n\nAnalyzing safe zones...\n\nProcessing slow speed...\nsummit at 500m: min time = 0.42 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.91 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 1.20 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 1.39 hrs\ncamp1 at 2000m: min time = 0.03 hrs\nsummit at 2500m: min time = 1.76 hrs\ncamp1 at 2500m: min time = 0.40 hrs\nsummit at 3000m: min time = 2.08 hrs\ncamp1 at 3000m: min time = 0.73 hrs\nsummit at 3500m: min time = 2.29 hrs\ncamp1 at 3500m: min time = 0.94 hrs\nsummit at 4000m: min time = 2.46 hrs\ncamp1 at 4000m: min time = 1.11 hrs\nsummit at 4500m: min time = 2.64 hrs\ncamp1 at 4500m: min time = 1.29 hrs\n\nProcessing medium speed...\nsummit at 500m: min time = 0.32 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.68 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.89 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 1.03 hrs\ncamp1 at 2000m: min time = 0.03 hrs\nsummit at 2500m: min time = 1.31 hrs\ncamp1 at 2500m: min time = 0.30 hrs\nsummit at 3000m: min time = 1.55 hrs\ncamp1 at 3000m: min time = 0.55 hrs\nsummit at 3500m: min time = 1.71 hrs\ncamp1 at 3500m: min time = 0.70 hrs\nsummit at 4000m: min time = 1.84 hrs\ncamp1 at 4000m: min time = 0.83 hrs\nsummit at 4500m: min time = 1.97 hrs\ncamp1 at 4500m: min time = 0.96 hrs\n\nProcessing fast speed...\nsummit at 500m: min time = 0.25 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.54 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.72 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.83 hrs\ncamp1 at 2000m: min time = 0.02 hrs\nsummit at 2500m: min time = 1.05 hrs\ncamp1 at 2500m: min time = 0.24 hrs\nsummit at 3000m: min time = 1.25 hrs\ncamp1 at 3000m: min time = 0.44 hrs\nsummit at 3500m: min time = 1.37 hrs\ncamp1 at 3500m: min time = 0.56 hrs\nsummit at 4000m: min time = 1.47 hrs\ncamp1 at 4000m: min time = 0.66 hrs\nsummit at 4500m: min time = 1.58 hrs\ncamp1 at 4500m: min time = 0.77 hrs\nSaving analysis results\nTravel times for modify_landcover calculated in 2.24 seconds\n\n=== CALCULATING TRAVEL TIMES FOR: walking_speed ===\nProcessing slow walking speed (0.91 m/s)\n  Source: summit\nProcessing travel times for summit at slow speed...\n  Travel time range: 0.00 to 11.19 hours\n  Source: camp1\nProcessing travel times for camp1 at slow speed...\n  Travel time range: 0.00 to 10.88 hours\nProcessing medium walking speed (1.22 m/s)\n  Source: summit\nProcessing travel times for summit at medium speed...\n  Travel time range: 0.00 to 8.35 hours\n  Source: camp1\nProcessing travel times for camp1 at medium speed...\n  Travel time range: 0.00 to 8.12 hours\nProcessing fast walking speed (1.52 m/s)\n  Source: summit\nProcessing travel times for summit at fast speed...\n  Travel time range: 0.00 to 6.70 hours\n  Source: camp1\nProcessing travel times for camp1 at fast speed...\n  Travel time range: 0.00 to 6.52 hours\nCalculating distance from summit for safe zone analysis\nCalculating distance from summit...\nMaximum distance from summit: 28355.07 meters\nAnalyzing minimum travel times within safe zones\n\nAnalyzing safe zones...\n\nProcessing slow speed...\nsummit at 500m: min time = 0.24 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.49 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.74 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 1.01 hrs\ncamp1 at 2000m: min time = 0.03 hrs\nsummit at 2500m: min time = 1.28 hrs\ncamp1 at 2500m: min time = 0.37 hrs\nsummit at 3000m: min time = 1.59 hrs\ncamp1 at 3000m: min time = 0.60 hrs\nsummit at 3500m: min time = 1.80 hrs\ncamp1 at 3500m: min time = 0.75 hrs\nsummit at 4000m: min time = 2.00 hrs\ncamp1 at 4000m: min time = 0.89 hrs\nsummit at 4500m: min time = 2.18 hrs\ncamp1 at 4500m: min time = 1.08 hrs\n\nProcessing medium speed...\nsummit at 500m: min time = 0.18 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.36 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.55 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.76 hrs\ncamp1 at 2000m: min time = 0.03 hrs\nsummit at 2500m: min time = 0.96 hrs\ncamp1 at 2500m: min time = 0.28 hrs\nsummit at 3000m: min time = 1.18 hrs\ncamp1 at 3000m: min time = 0.45 hrs\nsummit at 3500m: min time = 1.35 hrs\ncamp1 at 3500m: min time = 0.56 hrs\nsummit at 4000m: min time = 1.49 hrs\ncamp1 at 4000m: min time = 0.67 hrs\nsummit at 4500m: min time = 1.62 hrs\ncamp1 at 4500m: min time = 0.81 hrs\n\nProcessing fast speed...\nsummit at 500m: min time = 0.14 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.29 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.44 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.61 hrs\ncamp1 at 2000m: min time = 0.02 hrs\nsummit at 2500m: min time = 0.77 hrs\ncamp1 at 2500m: min time = 0.22 hrs\nsummit at 3000m: min time = 0.95 hrs\ncamp1 at 3000m: min time = 0.36 hrs\nsummit at 3500m: min time = 1.08 hrs\ncamp1 at 3500m: min time = 0.45 hrs\nsummit at 4000m: min time = 1.19 hrs\ncamp1 at 4000m: min time = 0.53 hrs\nsummit at 4500m: min time = 1.30 hrs\ncamp1 at 4500m: min time = 0.65 hrs\nSaving analysis results\nTravel times for walking_speed calculated in 3.39 seconds\n\n=== CALCULATING TRAVEL TIMES FOR: base_cost ===\nProcessing slow walking speed (0.91 m/s)\n  Source: summit\nProcessing travel times for summit at slow speed...\n  Travel time range: 0.00 to 8882786.00 hours\n  Source: camp1\nProcessing travel times for camp1 at slow speed...\n  Travel time range: 0.00 to 8882788.00 hours\nProcessing medium walking speed (1.22 m/s)\n  Source: summit\nProcessing travel times for summit at medium speed...\n  Travel time range: 0.00 to 6625685.00 hours\n  Source: camp1\nProcessing travel times for camp1 at medium speed...\n  Travel time range: 0.00 to 6625686.00 hours\nProcessing fast walking speed (1.52 m/s)\n  Source: summit\nProcessing travel times for summit at fast speed...\n  Travel time range: 0.00 to 5317984.00 hours\n  Source: camp1\nProcessing travel times for camp1 at fast speed...\n  Travel time range: 0.00 to 5317985.50 hours\nCalculating distance from summit for safe zone analysis\nCalculating distance from summit...\nMaximum distance from summit: 28355.07 meters\nAnalyzing minimum travel times within safe zones\n\nAnalyzing safe zones...\n\nProcessing slow speed...\nsummit at 500m: min time = 0.17 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.35 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.54 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.72 hrs\ncamp1 at 2000m: min time = 0.03 hrs\nsummit at 2500m: min time = 0.90 hrs\ncamp1 at 2500m: min time = 0.21 hrs\nsummit at 3000m: min time = 1.09 hrs\ncamp1 at 3000m: min time = 0.40 hrs\nsummit at 3500m: min time = 1.27 hrs\ncamp1 at 3500m: min time = 0.58 hrs\nsummit at 4000m: min time = 1.45 hrs\ncamp1 at 4000m: min time = 0.76 hrs\nsummit at 4500m: min time = 1.62 hrs\ncamp1 at 4500m: min time = 0.98 hrs\n\nProcessing medium speed...\nsummit at 500m: min time = 0.13 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.26 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.40 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.54 hrs\ncamp1 at 2000m: min time = 0.02 hrs\nsummit at 2500m: min time = 0.67 hrs\ncamp1 at 2500m: min time = 0.16 hrs\nsummit at 3000m: min time = 0.81 hrs\ncamp1 at 3000m: min time = 0.30 hrs\nsummit at 3500m: min time = 0.95 hrs\ncamp1 at 3500m: min time = 0.43 hrs\nsummit at 4000m: min time = 1.08 hrs\ncamp1 at 4000m: min time = 0.57 hrs\nsummit at 4500m: min time = 1.21 hrs\ncamp1 at 4500m: min time = 0.73 hrs\n\nProcessing fast speed...\nsummit at 500m: min time = 0.10 hrs\ncamp1 at 500m: min time = 0.00 hrs\nsummit at 1000m: min time = 0.21 hrs\ncamp1 at 1000m: min time = 0.00 hrs\nsummit at 1500m: min time = 0.32 hrs\ncamp1 at 1500m: min time = 0.00 hrs\nsummit at 2000m: min time = 0.43 hrs\ncamp1 at 2000m: min time = 0.02 hrs\nsummit at 2500m: min time = 0.54 hrs\ncamp1 at 2500m: min time = 0.13 hrs\nsummit at 3000m: min time = 0.65 hrs\ncamp1 at 3000m: min time = 0.24 hrs\nsummit at 3500m: min time = 0.76 hrs\ncamp1 at 3500m: min time = 0.35 hrs\nsummit at 4000m: min time = 0.87 hrs\ncamp1 at 4000m: min time = 0.46 hrs\nsummit at 4500m: min time = 0.97 hrs\ncamp1 at 4500m: min time = 0.58 hrs\nSaving analysis results\nTravel times for base_cost calculated in 3.00 seconds\n</pre> In\u00a0[6]: Copied! <pre>print(\"\\n=== SECTION 4: DECOMPOSITION ANALYSIS ===\")\ndecomp_start_time = time.time()\n\n# Run decomposition analysis\nprint(\"Running decomposition analysis...\")\ndecomposition_data = run_decomposition_analysis(dataset_info, min_coords_all)\n\n# Create and save the decomposition table\nprint(\"Creating decomposition table visualization\")\ntable_output_path = os.path.join(DATA_FOLDER, \"decomposition_table.png\")\ndf_decomp = create_decomposition_table(decomposition_data, table_output_path)\n\n# Also save as CSV\ndf_decomp.to_csv(os.path.join(DATA_FOLDER, \"decomposition_table.csv\"), index=False)\n\ndecomp_end_time = time.time()\nprint(f\"Decomposition analysis completed in {decomp_end_time - decomp_start_time:.2f} seconds\")\nprint(f\"Results saved to: {table_output_path}\")\n\n# Show the decomposition results\nprint(\"\\nDecomposition Results Summary:\")\nfor row in decomposition_data:\n    sz = row[\"Safe Zone Threshold (m)\"]\n    slope = row[\"Slope Contribution (%)\"]\n    landcover = row[\"Landcover Contribution (%)\"]\n    print(f\"Safe Zone {sz}m: Slope {slope:.1f}%, Land Cover {landcover:.1f}%\")\n</pre> print(\"\\n=== SECTION 4: DECOMPOSITION ANALYSIS ===\") decomp_start_time = time.time()  # Run decomposition analysis print(\"Running decomposition analysis...\") decomposition_data = run_decomposition_analysis(dataset_info, min_coords_all)  # Create and save the decomposition table print(\"Creating decomposition table visualization\") table_output_path = os.path.join(DATA_FOLDER, \"decomposition_table.png\") df_decomp = create_decomposition_table(decomposition_data, table_output_path)  # Also save as CSV df_decomp.to_csv(os.path.join(DATA_FOLDER, \"decomposition_table.csv\"), index=False)  decomp_end_time = time.time() print(f\"Decomposition analysis completed in {decomp_end_time - decomp_start_time:.2f} seconds\") print(f\"Results saved to: {table_output_path}\")  # Show the decomposition results print(\"\\nDecomposition Results Summary:\") for row in decomposition_data:     sz = row[\"Safe Zone Threshold (m)\"]     slope = row[\"Slope Contribution (%)\"]     landcover = row[\"Landcover Contribution (%)\"]     print(f\"Safe Zone {sz}m: Slope {slope:.1f}%, Land Cover {landcover:.1f}%\")  <pre>\n=== SECTION 4: DECOMPOSITION ANALYSIS ===\nRunning decomposition analysis...\n\n=== Running Decomposition Analysis ===\nReading raster: Awu_inverted_cost_8_directions_original.tif\nReading raster: inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif\nReading raster: invert_landcovercost_original_Awu.tif\nExpanding single-band base_cost raster to 8 directions...\nDecomposition analysis for 'final' dataset, source: summit, speed: medium\nCreating decomposition table visualization\n</pre> <pre>Decomposition analysis completed in 1.10 seconds\nResults saved to: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\decomposition_table.png\n\nDecomposition Results Summary:\nSafe Zone 500m: Slope 43.7%, Land Cover 56.3%\nSafe Zone 1000m: Slope 53.2%, Land Cover 46.8%\nSafe Zone 1500m: Slope 41.0%, Land Cover 59.0%\nSafe Zone 2000m: Slope 39.2%, Land Cover 60.8%\nSafe Zone 2500m: Slope 55.8%, Land Cover 44.2%\nSafe Zone 3000m: Slope 58.3%, Land Cover 41.7%\nSafe Zone 3500m: Slope 55.3%, Land Cover 44.7%\nSafe Zone 4000m: Slope 48.0%, Land Cover 52.0%\nSafe Zone 4500m: Slope 46.2%, Land Cover 53.8%\n</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> In\u00a0[8]: Copied! <pre>print(\"\\n=== SECTION 5: CREATING VISUALIZATIONS ===\")\nviz_start_time = time.time()\n\n# Add a verbose flag to control detailed output\nverbose = False  # Set to False to suppress detailed messages\n\n# 1. Travel Time Comparison Plot\nprint(\"Creating travel time comparison plot...\")\nspeed_colors = {\n    'slow': 'red',\n    'medium': 'blue',\n    'fast': 'green'\n}\ncomparison_fig = plot_travel_time_comparison(\n    all_results, SAFE_ZONE_DISTANCES, SOURCE_NAMES, speed_colors\n)\ncomparison_fig.savefig(\n    os.path.join(DATA_FOLDER, \"comparison_travel_time_by_source.png\"), \n    dpi=300, bbox_inches='tight'\n)\nplt.close(comparison_fig)\nprint(\"Travel time comparison plot saved\")\n\n# 2. Process evacuation paths for visualization\nprint(\"Processing evacuation paths...\")\nselected_speed = 'medium'\nevacuation_paths = {ds_key: {} for ds_key in dataset_info.keys()}\n\n# Process paths for each dataset\nfor ds_key, info in dataset_info.items():\n    if verbose:\n        print(f\"Processing paths for {ds_key}...\")\n    \n    pred_summit = info[\"pred_summit\"]\n    rows, cols = info[\"rows\"], info[\"cols\"]\n    summit_rc = info[\"summit_raster_coords\"]\n    summit_node = to_1d(summit_rc[0], summit_rc[1], cols)\n    \n    for sz in SAFE_ZONE_DISTANCES:\n        target_coord = min_coords_all[ds_key][selected_speed][sz][0]\n        if np.isnan(target_coord[0]) or np.isnan(target_coord[1]):\n            if verbose:\n                print(f\"  No valid target for {sz}m safe zone\")\n            evacuation_paths[ds_key][sz] = []\n            continue\n        \n        target_node = to_1d(int(target_coord[0]), int(target_coord[1]), cols)\n        path_coords = reconstruct_path(pred_summit, summit_node, target_node, cols)\n        evacuation_paths[ds_key][sz] = path_coords\n        \n        if verbose:\n            print(f\"  Path to {sz}m safe zone: {len(path_coords)} steps\")\n\n# 3. Create Cost Surface Visualization\nprint(\"Creating cost surface visualization...\")\n\n# Load cost rasters for visualization\ncost_arrays = []\ntransforms = []\nfor ds_key in dataset_info.keys():\n    if verbose:\n        print(f\"Loading cost raster for {ds_key}...\")\n    \n    cost_raster_path = os.path.join(DATA_FOLDER, f'cost_distance_summit_{ds_key}_medium_hours.tif')\n    cost_array, cost_meta = load_raster(cost_raster_path)\n    cost_arrays.append(cost_array)\n    transforms.append(cost_meta['transform'])\n\noutput_path = os.path.join(DATA_FOLDER, 'cost_surface_with_paths_enhanced.png')\ncreate_cost_surface_subplots(\n    dataset_info=dataset_info,\n    cost_arrays=cost_arrays,\n    transforms=transforms,\n    evacuation_paths=evacuation_paths,\n    summit_coords={k: v['summit_raster_coords'] for k, v in dataset_info.items()},\n    safe_zone_distances=SAFE_ZONE_DISTANCES,\n    hiking_gdf=hiking_gdf,\n    output_path=output_path\n)\nprint(f\"Cost surface visualization saved to {output_path}\")\n\n# Display total time for visualization section\nviz_end_time = time.time()\nprint(f\"Visualization completed in {viz_end_time - viz_start_time:.2f} seconds\")\n</pre> print(\"\\n=== SECTION 5: CREATING VISUALIZATIONS ===\") viz_start_time = time.time()  # Add a verbose flag to control detailed output verbose = False  # Set to False to suppress detailed messages  # 1. Travel Time Comparison Plot print(\"Creating travel time comparison plot...\") speed_colors = {     'slow': 'red',     'medium': 'blue',     'fast': 'green' } comparison_fig = plot_travel_time_comparison(     all_results, SAFE_ZONE_DISTANCES, SOURCE_NAMES, speed_colors ) comparison_fig.savefig(     os.path.join(DATA_FOLDER, \"comparison_travel_time_by_source.png\"),      dpi=300, bbox_inches='tight' ) plt.close(comparison_fig) print(\"Travel time comparison plot saved\")  # 2. Process evacuation paths for visualization print(\"Processing evacuation paths...\") selected_speed = 'medium' evacuation_paths = {ds_key: {} for ds_key in dataset_info.keys()}  # Process paths for each dataset for ds_key, info in dataset_info.items():     if verbose:         print(f\"Processing paths for {ds_key}...\")          pred_summit = info[\"pred_summit\"]     rows, cols = info[\"rows\"], info[\"cols\"]     summit_rc = info[\"summit_raster_coords\"]     summit_node = to_1d(summit_rc[0], summit_rc[1], cols)          for sz in SAFE_ZONE_DISTANCES:         target_coord = min_coords_all[ds_key][selected_speed][sz][0]         if np.isnan(target_coord[0]) or np.isnan(target_coord[1]):             if verbose:                 print(f\"  No valid target for {sz}m safe zone\")             evacuation_paths[ds_key][sz] = []             continue                  target_node = to_1d(int(target_coord[0]), int(target_coord[1]), cols)         path_coords = reconstruct_path(pred_summit, summit_node, target_node, cols)         evacuation_paths[ds_key][sz] = path_coords                  if verbose:             print(f\"  Path to {sz}m safe zone: {len(path_coords)} steps\")  # 3. Create Cost Surface Visualization print(\"Creating cost surface visualization...\")  # Load cost rasters for visualization cost_arrays = [] transforms = [] for ds_key in dataset_info.keys():     if verbose:         print(f\"Loading cost raster for {ds_key}...\")          cost_raster_path = os.path.join(DATA_FOLDER, f'cost_distance_summit_{ds_key}_medium_hours.tif')     cost_array, cost_meta = load_raster(cost_raster_path)     cost_arrays.append(cost_array)     transforms.append(cost_meta['transform'])  output_path = os.path.join(DATA_FOLDER, 'cost_surface_with_paths_enhanced.png') create_cost_surface_subplots(     dataset_info=dataset_info,     cost_arrays=cost_arrays,     transforms=transforms,     evacuation_paths=evacuation_paths,     summit_coords={k: v['summit_raster_coords'] for k, v in dataset_info.items()},     safe_zone_distances=SAFE_ZONE_DISTANCES,     hiking_gdf=hiking_gdf,     output_path=output_path ) print(f\"Cost surface visualization saved to {output_path}\")  # Display total time for visualization section viz_end_time = time.time() print(f\"Visualization completed in {viz_end_time - viz_start_time:.2f} seconds\") <pre>\n=== SECTION 5: CREATING VISUALIZATIONS ===\nCreating travel time comparison plot...\n\nCreating travel time comparison plot...\n</pre> <pre>Travel time comparison plot created in 0.36 seconds\nTravel time comparison plot saved\nProcessing evacuation paths...\nCreating cost surface visualization...\n\nCreating cost surface subplots...\n</pre> <pre>C:\\Users\\Mojan\\Desktop\\RA Volcano\\Python Code\\github\\visualization.py:382: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> <pre>Cost surface subplots created in 0.98 seconds\nCost surface visualization saved to C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\cost_surface_with_paths_enhanced.png\nVisualization completed in 2.21 seconds\n</pre>"},{"location":"demo/awu/evacuation-analysis.html#volcanic-evacuation-analysis","title":"Volcanic Evacuation Analysis\u00b6","text":"<p>This script performs a comprehensive analysis of evacuation routes from volcanoes, calculating optimal paths and travel times from various points to safe zones under different walking speed scenarios.</p> <p>The analysis is broken down into sequential steps that can be executed one by one to better understand each part of the process.</p>"},{"location":"demo/awu/evacuation-analysis.html#importing-required-packages","title":"IMPORTING REQUIRED PACKAGES\u00b6","text":"<p>First, let's import all the packages we need for the analysis</p>"},{"location":"demo/awu/evacuation-analysis.html#loading-geographic-data","title":"Loading Geographic Data\u00b6","text":"<p>Before we can analyze evacuation routes, we need to load the geographic data that defines the area of interest:</p> <ol> <li>Summit Location: The point representing the volcano's peak</li> <li>Camping Spots: Locations where people may be staying (evacuation sources)</li> <li>Hiking Paths: Existing trails that could be used for evacuation</li> </ol> <p>These datasets are stored as shapefiles and are loaded using the <code>read_shapefile</code> function from our utilities module.</p>"},{"location":"demo/awu/evacuation-analysis.html#processing-cost-datasets","title":"Processing Cost Datasets\u00b6","text":"<p>Now we'll process each cost raster dataset. Cost rasters represent the \"difficulty\" of traversing different terrain types. We have multiple cost datasets to compare different scenarios:</p> <ol> <li>Final: Cost raster with original LandCover</li> <li>Modified Landcover: Cost raster with modified Landcover</li> <li>Walking Speed: Cost raster based solely on Slope</li> <li>Base Cost:Cost raster based solely on Landcover</li> </ol> <p>For each dataset, we:</p> <ol> <li>Read the cost raster from disk</li> <li>Expand it to 8 directional bands if needed (for 8-direction path finding)</li> <li>Convert geographic coordinates to raster indices</li> <li>Build an adjacency matrix representing possible movements</li> <li>Run Dijkstra's algorithm to find optimal paths</li> <li>Save the resulting distance rasters</li> </ol>"},{"location":"demo/awu/evacuation-analysis.html#calculating-travel-times","title":"Calculating Travel Times\u00b6","text":"<p>Now that we have the base cost distances, we can calculate actual travel times for different walking speeds. The travel time represents how long it would take to reach any point from each source location.</p> <p>For each dataset, walking speed, and source point combination, we:</p> <ol> <li>Load the corresponding cost distance raster</li> <li>Process it to calculate travel times in hours</li> <li>Store both the 2D array and a flattened version for analysis</li> </ol> <p>We consider three walking speeds as defined in the config:</p> <ul> <li>Slow: 0.91 m/s (3.3 km/h)</li> <li>Medium: 1.22 m/s (4.4 km/h)</li> <li>Fast: 1.52 m/s (5.5 km/h)</li> </ul>"},{"location":"demo/awu/evacuation-analysis.html#decomposition-analysis","title":"Decomposition Analysis\u00b6","text":"<p>Now we'll analyze what factors contribute most to the evacuation time:</p> <ol> <li>Slope Contribution: How much the terrain's steepness affects travel time</li> <li>Land Cover Contribution: How much the land cover (forests, fields, etc.) affects travel time</li> </ol> <p>The decomposition analysis helps understand which factors most impact evacuation routes, which can guide planning and mitigation efforts.</p>"},{"location":"demo/awu/evacuation-analysis.html#creating-visualizations","title":"Creating Visualizations\u00b6","text":"<p>Finally, we'll create visualizations to help understand the results:</p> <ol> <li><p>Travel Time Comparison Plot: Compares minimum travel times for different sources, walking speeds, and terrain scenarios</p> </li> <li><p>Evacuation Path Visualization: Shows optimal paths from the summit to different safe zones overlaid on cost surfaces</p> </li> <li><p>Final Evacuation Table: Summarizes evacuation times in a tabular format</p> </li> </ol> <p>These visualizations help emergency planners understand evacuation options and identify the most efficient routes.</p>"},{"location":"demo/awu/probability-analysis.html","title":"Probability Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nMain script for Awu volcanic evacuation analysis.\nShows plots and saves results to the original data folder.\n\"\"\"\nfrom matplotlib.patheffects import withStroke\nimport rasterio\nfrom rasterio.warp import reproject, Resampling\nfrom pyproj import CRS, Transformer\nimport contextily as ctx\nfrom shapely.geometry import box\nimport geopandas as gpd\nimport os\nimport numpy as np\nfrom datetime import datetime\nimport sys\nimport matplotlib.pyplot as plt\n\n# Add current directory to path to ensure module imports work\ncwd = os.getcwd()\nsys.path.append(cwd)\n\n# Import modules from the package - using absolute imports\nfrom analysis import (\n    perform_evacuation_analysis,\n    analyze_safe_zones,\n    load_travel_time_data,\n    read_raster\n)\nfrom data_utils import read_shapefile\nfrom raster_utils import (\n    resample_raster,\n    coords_to_raster,\n    to_1d\n)\nfrom visualization import (\n    plot_travel_time_comparison,\n    plot_cost_surface_with_paths,\n)\nfrom graph_utils import reconstruct_path\n</pre> #!/usr/bin/env python3 # -*- coding: utf-8 -*-  \"\"\" Main script for Awu volcanic evacuation analysis. Shows plots and saves results to the original data folder. \"\"\" from matplotlib.patheffects import withStroke import rasterio from rasterio.warp import reproject, Resampling from pyproj import CRS, Transformer import contextily as ctx from shapely.geometry import box import geopandas as gpd import os import numpy as np from datetime import datetime import sys import matplotlib.pyplot as plt  # Add current directory to path to ensure module imports work cwd = os.getcwd() sys.path.append(cwd)  # Import modules from the package - using absolute imports from analysis import (     perform_evacuation_analysis,     analyze_safe_zones,     load_travel_time_data,     read_raster ) from data_utils import read_shapefile from raster_utils import (     resample_raster,     coords_to_raster,     to_1d ) from visualization import (     plot_travel_time_comparison,     plot_cost_surface_with_paths, ) from graph_utils import reconstruct_path In\u00a0[12]: Copied! <pre># Use the data folder directly for output instead of creating a new directory\ndata_folder = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results'\noutput_dir = data_folder\n\nprint(f\"\\nOutput directory: {output_dir}\")\n\n# Define input file paths with your specific paths\ncost_paths = {\n    'final': os.path.join(data_folder, 'Awu_inverted_cost_8_directions_original.tif'),\n    'modify_landcover': os.path.join(data_folder, 'Awu_inverted_cost_8_directions_modified.tif')\n}\nsummit_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\"\ncamp_spot_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\"\nhiking_path = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp'\neruption_probability_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\resampled_eruption_probability_VEI4.tif\"\n\n# Define parameters\nwalking_speeds = {\n    'slow': 0.91,    # m/s\n    'medium': 1.22,  # m/s\n    'fast': 1.52     # m/s\n}\n\n# Define source names\nsource_names = ['summit', 'camp1']\n\n# Define eruption probability thresholds\nthresholds = [0.9, 0.75, 0.5, 0.25, 0.1, 0.05]\n</pre> # Use the data folder directly for output instead of creating a new directory data_folder = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results' output_dir = data_folder  print(f\"\\nOutput directory: {output_dir}\")  # Define input file paths with your specific paths cost_paths = {     'final': os.path.join(data_folder, 'Awu_inverted_cost_8_directions_original.tif'),     'modify_landcover': os.path.join(data_folder, 'Awu_inverted_cost_8_directions_modified.tif') } summit_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\" camp_spot_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\" hiking_path = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp' eruption_probability_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\resampled_eruption_probability_VEI4.tif\"  # Define parameters walking_speeds = {     'slow': 0.91,    # m/s     'medium': 1.22,  # m/s     'fast': 1.52     # m/s }  # Define source names source_names = ['summit', 'camp1']  # Define eruption probability thresholds thresholds = [0.9, 0.75, 0.5, 0.25, 0.1, 0.05] <pre>\nOutput directory: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\n</pre> In\u00a0[13]: Copied! <pre># Print file paths to confirm\nprint(\"\\nUsing file paths:\")\nprint(f\"Cost (Original): {cost_paths['final']}\")\nprint(f\"Cost (Modified): {cost_paths['modify_landcover']}\")\nprint(f\"Summit: {summit_path}\")\nprint(f\"Camp spots: {camp_spot_path}\")\nprint(f\"Hiking path: {hiking_path}\")\nprint(f\"Eruption probability: {eruption_probability_path}\")\n\n# Get source coordinates\nprint(\"\\nLoading summit points...\")\nsummit_gdf = read_shapefile(summit_path)\ncost_data, cost_meta, transform, nodata, bounds, resolution = read_raster(cost_paths['final'])\nsource_coords = coords_to_raster(summit_gdf, transform, bounds, resolution)\n\n# Add camp spot coordinates if present\ncamp_gdf = read_shapefile(camp_spot_path)\ncamp_coords = coords_to_raster(camp_gdf, transform, bounds, resolution)\nsource_coords = [source_coords[0]] if isinstance(source_coords, list) else [source_coords]\nsource_coords.extend(camp_coords)\n\nprint(f\"Source points: {source_names}\")\nprint(f\"Source coordinates (row, col): {source_coords}\")\n</pre> # Print file paths to confirm print(\"\\nUsing file paths:\") print(f\"Cost (Original): {cost_paths['final']}\") print(f\"Cost (Modified): {cost_paths['modify_landcover']}\") print(f\"Summit: {summit_path}\") print(f\"Camp spots: {camp_spot_path}\") print(f\"Hiking path: {hiking_path}\") print(f\"Eruption probability: {eruption_probability_path}\")  # Get source coordinates print(\"\\nLoading summit points...\") summit_gdf = read_shapefile(summit_path) cost_data, cost_meta, transform, nodata, bounds, resolution = read_raster(cost_paths['final']) source_coords = coords_to_raster(summit_gdf, transform, bounds, resolution)  # Add camp spot coordinates if present camp_gdf = read_shapefile(camp_spot_path) camp_coords = coords_to_raster(camp_gdf, transform, bounds, resolution) source_coords = [source_coords[0]] if isinstance(source_coords, list) else [source_coords] source_coords.extend(camp_coords)  print(f\"Source points: {source_names}\") print(f\"Source coordinates (row, col): {source_coords}\") <pre>\nUsing file paths:\nCost (Original): C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\Awu_inverted_cost_8_directions_original.tif\nCost (Modified): C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\Awu_inverted_cost_8_directions_modified.tif\nSummit: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\nCamp spots: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\nHiking path: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp\nEruption probability: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\resampled_eruption_probability_VEI4.tif\n\nLoading summit points...\nSource points: ['summit', 'camp1']\nSource coordinates (row, col): [(198, 200), (215, 209)]\n</pre> In\u00a0[14]: Copied! <pre># Step 1: Perform evacuation analysis to generate cost distance rasters\nprint(\"\\n--- Starting Evacuation Analysis ---\")\nall_results, dataset_info = perform_evacuation_analysis(\n    cost_paths, source_coords, source_names, walking_speeds, output_dir\n)\n\n# Initialize evacuation paths dictionary\nevacuation_paths = {ds_key: {} for ds_key in cost_paths.keys()}\n</pre> # Step 1: Perform evacuation analysis to generate cost distance rasters print(\"\\n--- Starting Evacuation Analysis ---\") all_results, dataset_info = perform_evacuation_analysis(     cost_paths, source_coords, source_names, walking_speeds, output_dir )  # Initialize evacuation paths dictionary evacuation_paths = {ds_key: {} for ds_key in cost_paths.keys()} <pre>\n--- Starting Evacuation Analysis ---\n\nProcessing cost dataset: final\nRaster shape: (8, 400, 401)\nSource Nodes (1D): [79598, 86424]\nBuilding adjacency matrix...\n</pre> <pre>Processing rows: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:13&lt;00:00, 29.55it/s]\n</pre> <pre>Adjacency matrix created.\nRunning Dijkstra's algorithm for all sources...\nDijkstra's algorithm completed in 0.17 seconds.\nSaved base cost distance raster for summit: cost_distance_summit_final.tif\nSaved base cost distance raster for camp1: cost_distance_camp1_final.tif\n\nProcessing travel time rasters for walking speed 'slow' (0.91 m/s)...\nSaved travel time raster for summit at speed 'slow': cost_distance_summit_final_slow_hours.tif\nSaved travel time raster for camp1 at speed 'slow': cost_distance_camp1_final_slow_hours.tif\n\nProcessing travel time rasters for walking speed 'medium' (1.22 m/s)...\nSaved travel time raster for summit at speed 'medium': cost_distance_summit_final_medium_hours.tif\nSaved travel time raster for camp1 at speed 'medium': cost_distance_camp1_final_medium_hours.tif\n\nProcessing travel time rasters for walking speed 'fast' (1.52 m/s)...\nSaved travel time raster for summit at speed 'fast': cost_distance_summit_final_fast_hours.tif\nSaved travel time raster for camp1 at speed 'fast': cost_distance_camp1_final_fast_hours.tif\nProcessing for dataset 'final' complete!\n\nProcessing cost dataset: modify_landcover\nRaster shape: (8, 400, 401)\nSource Nodes (1D): [79598, 86424]\nBuilding adjacency matrix...\n</pre> <pre>Processing rows: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [00:14&lt;00:00, 28.06it/s]\n</pre> <pre>Adjacency matrix created.\nRunning Dijkstra's algorithm for all sources...\nDijkstra's algorithm completed in 0.14 seconds.\nSaved base cost distance raster for summit: cost_distance_summit_modify_landcover.tif\nSaved base cost distance raster for camp1: cost_distance_camp1_modify_landcover.tif\n\nProcessing travel time rasters for walking speed 'slow' (0.91 m/s)...\nSaved travel time raster for summit at speed 'slow': cost_distance_summit_modify_landcover_slow_hours.tif\nSaved travel time raster for camp1 at speed 'slow': cost_distance_camp1_modify_landcover_slow_hours.tif\n\nProcessing travel time rasters for walking speed 'medium' (1.22 m/s)...\nSaved travel time raster for summit at speed 'medium': cost_distance_summit_modify_landcover_medium_hours.tif\nSaved travel time raster for camp1 at speed 'medium': cost_distance_camp1_modify_landcover_medium_hours.tif\n\nProcessing travel time rasters for walking speed 'fast' (1.52 m/s)...\nSaved travel time raster for summit at speed 'fast': cost_distance_summit_modify_landcover_fast_hours.tif\nSaved travel time raster for camp1 at speed 'fast': cost_distance_camp1_modify_landcover_fast_hours.tif\nProcessing for dataset 'modify_landcover' complete!\n</pre> In\u00a0[15]: Copied! <pre># Step 2: Analyze safe zones for each dataset\nfor dataset_key in cost_paths.keys():\n    print(f\"\\n--- Analyzing Safe Zones for Dataset: {dataset_key} ---\")\n    \n    # Load travel time data for this dataset\n    travel_time_data = load_travel_time_data(\n        dataset_key, source_names, walking_speeds, output_dir\n    )\n    \n    # Analyze safe zones based on probability thresholds\n    results, min_coords = analyze_safe_zones(\n        eruption_probability_path, travel_time_data, thresholds,\n        source_names, walking_speeds, dataset_key, output_dir\n    )\n    \n    # Store results for plotting\n    all_results[dataset_key] = results\n    \n    # Extract evacuation paths for medium walking speed\n    for thresh in thresholds:\n        # Get the safe point coordinates for summit at this threshold\n        summit_idx = 0  # Assuming summit is first source\n        if \"medium\" in min_coords and thresh in min_coords[\"medium\"]:\n            safe_point = min_coords[\"medium\"][thresh][summit_idx]\n            \n            if not np.isnan(safe_point[0]) and not np.isnan(safe_point[1]):\n                # Get source and target nodes\n                summit_coords = dataset_info[dataset_key][\"summit_raster_coords\"]\n                cols = dataset_info[dataset_key][\"cols\"]\n                \n                # Convert to 1D indices\n                source_1d = to_1d(summit_coords[0], summit_coords[1], cols)\n                target_1d = to_1d(int(safe_point[0]), int(safe_point[1]), cols)\n                \n                # Use predecessor array to reconstruct path\n                pred_array = dataset_info[dataset_key][\"pred_summit\"]\n                path = reconstruct_path(pred_array, source_1d, target_1d, cols)\n                evacuation_paths[dataset_key][thresh] = path\n            else:\n                evacuation_paths[dataset_key][thresh] = []\n        else:\n            evacuation_paths[dataset_key][thresh] = []\n</pre> # Step 2: Analyze safe zones for each dataset for dataset_key in cost_paths.keys():     print(f\"\\n--- Analyzing Safe Zones for Dataset: {dataset_key} ---\")          # Load travel time data for this dataset     travel_time_data = load_travel_time_data(         dataset_key, source_names, walking_speeds, output_dir     )          # Analyze safe zones based on probability thresholds     results, min_coords = analyze_safe_zones(         eruption_probability_path, travel_time_data, thresholds,         source_names, walking_speeds, dataset_key, output_dir     )          # Store results for plotting     all_results[dataset_key] = results          # Extract evacuation paths for medium walking speed     for thresh in thresholds:         # Get the safe point coordinates for summit at this threshold         summit_idx = 0  # Assuming summit is first source         if \"medium\" in min_coords and thresh in min_coords[\"medium\"]:             safe_point = min_coords[\"medium\"][thresh][summit_idx]                          if not np.isnan(safe_point[0]) and not np.isnan(safe_point[1]):                 # Get source and target nodes                 summit_coords = dataset_info[dataset_key][\"summit_raster_coords\"]                 cols = dataset_info[dataset_key][\"cols\"]                                  # Convert to 1D indices                 source_1d = to_1d(summit_coords[0], summit_coords[1], cols)                 target_1d = to_1d(int(safe_point[0]), int(safe_point[1]), cols)                                  # Use predecessor array to reconstruct path                 pred_array = dataset_info[dataset_key][\"pred_summit\"]                 path = reconstruct_path(pred_array, source_1d, target_1d, cols)                 evacuation_paths[dataset_key][thresh] = path             else:                 evacuation_paths[dataset_key][thresh] = []         else:             evacuation_paths[dataset_key][thresh] = [] <pre>\n--- Analyzing Safe Zones for Dataset: final ---\nLoaded travel time data for speed 'slow'\nLoaded travel time data for speed 'medium'\nLoaded travel time data for speed 'fast'\n\nPerforming safe zone analysis based on eruption probability thresholds...\n\n--- Walking speed: slow ---\n\nEruption Probability Threshold: 0.9\nsummit: min travel time = 0.19 hrs at cell (200, 201)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.75\nsummit: min travel time = 0.42 hrs at cell (202, 203)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.5\nsummit: min travel time = 0.68 hrs at cell (196, 210)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.25\nsummit: min travel time = 1.01 hrs at cell (195, 215)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.1\nsummit: min travel time = 1.30 hrs at cell (197, 219)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.05\nsummit: min travel time = 1.44 hrs at cell (200, 221)\ncamp1: min travel time = 0.16 hrs at cell (217, 209)\n\n--- Walking speed: medium ---\n\nEruption Probability Threshold: 0.9\nsummit: min travel time = 0.14 hrs at cell (200, 201)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.75\nsummit: min travel time = 0.32 hrs at cell (202, 203)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.5\nsummit: min travel time = 0.51 hrs at cell (196, 210)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.25\nsummit: min travel time = 0.75 hrs at cell (195, 215)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.1\nsummit: min travel time = 0.97 hrs at cell (197, 219)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.05\nsummit: min travel time = 1.07 hrs at cell (200, 221)\ncamp1: min travel time = 0.12 hrs at cell (217, 209)\n\n--- Walking speed: fast ---\n\nEruption Probability Threshold: 0.9\nsummit: min travel time = 0.12 hrs at cell (200, 201)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.75\nsummit: min travel time = 0.25 hrs at cell (202, 203)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.5\nsummit: min travel time = 0.41 hrs at cell (196, 210)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.25\nsummit: min travel time = 0.60 hrs at cell (195, 215)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.1\nsummit: min travel time = 0.78 hrs at cell (197, 219)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.05\nsummit: min travel time = 0.86 hrs at cell (200, 221)\ncamp1: min travel time = 0.09 hrs at cell (217, 209)\nSaved analysis report: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\eruption_probability_travel_time_report_final.txt\nSaved metrics CSV: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\eruption_probability_travel_time_metrics_final.csv\nSaved statistics table: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\travel_time_statistics_by_source_final.png\nSaved statistics CSV: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\travel_time_statistics_by_source_final.csv\n\n--- Analyzing Safe Zones for Dataset: modify_landcover ---\nLoaded travel time data for speed 'slow'\nLoaded travel time data for speed 'medium'\nLoaded travel time data for speed 'fast'\n\nPerforming safe zone analysis based on eruption probability thresholds...\n\n--- Walking speed: slow ---\n\nEruption Probability Threshold: 0.9\nsummit: min travel time = 0.19 hrs at cell (200, 201)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.75\nsummit: min travel time = 0.42 hrs at cell (202, 203)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.5\nsummit: min travel time = 0.88 hrs at cell (206, 205)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.25\nsummit: min travel time = 1.20 hrs at cell (211, 208)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.1\nsummit: min travel time = 1.35 hrs at cell (215, 209)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.05\nsummit: min travel time = 1.52 hrs at cell (217, 209)\ncamp1: min travel time = 0.16 hrs at cell (217, 209)\n\n--- Walking speed: medium ---\n\nEruption Probability Threshold: 0.9\nsummit: min travel time = 0.14 hrs at cell (200, 201)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.75\nsummit: min travel time = 0.32 hrs at cell (202, 203)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.5\nsummit: min travel time = 0.66 hrs at cell (206, 205)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.25\nsummit: min travel time = 0.89 hrs at cell (211, 208)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.1\nsummit: min travel time = 1.01 hrs at cell (215, 209)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.05\nsummit: min travel time = 1.13 hrs at cell (217, 209)\ncamp1: min travel time = 0.12 hrs at cell (217, 209)\n\n--- Walking speed: fast ---\n\nEruption Probability Threshold: 0.9\nsummit: min travel time = 0.12 hrs at cell (200, 201)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.75\nsummit: min travel time = 0.25 hrs at cell (202, 203)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.5\nsummit: min travel time = 0.53 hrs at cell (206, 205)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.25\nsummit: min travel time = 0.72 hrs at cell (211, 208)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.1\nsummit: min travel time = 0.81 hrs at cell (215, 209)\ncamp1: min travel time = 0.00 hrs at cell (215, 209)\n\nEruption Probability Threshold: 0.05\nsummit: min travel time = 0.91 hrs at cell (217, 209)\ncamp1: min travel time = 0.10 hrs at cell (217, 209)\nSaved analysis report: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\eruption_probability_travel_time_report_modify_landcover.txt\nSaved metrics CSV: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\eruption_probability_travel_time_metrics_modify_landcover.csv\nSaved statistics table: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\travel_time_statistics_by_source_modify_landcover.png\nSaved statistics CSV: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\travel_time_statistics_by_source_modify_landcover.csv\n</pre> In\u00a0[16]: Copied! <pre># Step 3: Generate comparison plots\nprint(\"\\n--- Generating Travel Time Comparison Plots ---\")\ncomparison_plot = plot_travel_time_comparison(\n    all_results, source_names, thresholds, walking_speeds, output_dir\n)\nplt.figure()  # Create a new figure for the next plot\nplt.show()    # Display the current figure\n</pre> # Step 3: Generate comparison plots print(\"\\n--- Generating Travel Time Comparison Plots ---\") comparison_plot = plot_travel_time_comparison(     all_results, source_names, thresholds, walking_speeds, output_dir ) plt.figure()  # Create a new figure for the next plot plt.show()    # Display the current figure <pre>\n--- Generating Travel Time Comparison Plots ---\n</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> In\u00a0[17]: Copied! <pre># Step 4: Generate cost surface plots with evacuation paths\nprint(\"\\n--- Generating Cost Surface Plots with Evacuation Paths ---\")\nselected_speed = \"medium\"  # Use medium speed for visualization\npath_plots = plot_cost_surface_with_paths(\n    dataset_info, evacuation_paths, eruption_probability_path,\n    hiking_path, selected_speed, thresholds, output_dir, vei_label=\"VEI4\"\n)\nplt.figure()  # Create a new figure for the next plot\nplt.show()    # Display the current figure\n</pre> # Step 4: Generate cost surface plots with evacuation paths print(\"\\n--- Generating Cost Surface Plots with Evacuation Paths ---\") selected_speed = \"medium\"  # Use medium speed for visualization path_plots = plot_cost_surface_with_paths(     dataset_info, evacuation_paths, eruption_probability_path,     hiking_path, selected_speed, thresholds, output_dir, vei_label=\"VEI4\" ) plt.figure()  # Create a new figure for the next plot plt.show()    # Display the current figure <pre>\n--- Generating Cost Surface Plots with Evacuation Paths ---\n\nCreating cost surface subplots with evacuation paths...\n</pre> <pre>C:\\Users\\Mojan\\Desktop\\RA Volcano\\Python Code\\github\\probability\\visualization.py:282: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n  plt.tight_layout()\n</pre> <pre>Cost surface visualization with evacuation paths saved to: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\evacuation_paths_by_probability_threshold_VEI4_medium.png\n</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># Import functions from your existing code\nfrom data_utils import read_shapefile\n\ndef create_vei_comparison_plot(eruption_probability_paths, hiking_path, summit_path, output_dir):\n    \"\"\"\n    Create a comparison plot of contour maps for different VEI levels side by side.\n    \n    Args:\n        eruption_probability_paths (dict): Dictionary of VEI levels and their file paths\n        hiking_path (str): Path to the hiking path shapefile\n        summit_path (str): Path to the summit point shapefile\n        output_dir (str): Directory to save the plot\n        \n    Returns:\n        str: Path to the saved plot\n    \"\"\"\n    print(\"\\nCreating VEI comparison plot...\")\n    \n    # Load hiking trail shapefile\n    hiking_gdf = read_shapefile(hiking_path)\n    \n    # Load summit points\n    summit_gdf = read_shapefile(summit_path)\n    \n    # Create figure with subplots for each VEI level - make them square and equally sized\n    fig, axes = plt.subplots(1, len(eruption_probability_paths), figsize=(18, 6), \n                           sharey=True, sharex=True, constrained_layout=True)\n    \n    # If only one VEI level, convert axes to list\n    if len(eruption_probability_paths) == 1:\n        axes = [axes]\n        \n    # Make all subplots have equal aspect ratio (square)\n    for ax in axes:\n        ax.set_aspect('equal')\n    \n    # Define eruption probability thresholds - CRITICAL: Must be in ASCENDING order for matplotlib contour\n    thresholds = [ 0.1,0.5, 0.9]\n    \n    # Define CRS for UTM Zone 51N and Web Mercator\n    utm_crs = CRS.from_epsg(32651)  # WGS84 UTM Zone 51N\n    web_mercator_crs = CRS.from_epsg(3857)  # Web Mercator\n    \n    # Create transformer from UTM to Web Mercator\n    transformer = Transformer.from_crs(utm_crs, web_mercator_crs, always_xy=True)\n    \n    # Transform hiking_gdf to Web Mercator for basemap\n    if hiking_gdf.crs is None:\n        hiking_gdf.crs = utm_crs\n    hiking_gdf_web_mercator = hiking_gdf.to_crs(web_mercator_crs)\n    \n    # Transform summit_gdf to Web Mercator for basemap\n    if summit_gdf.crs is None:\n        summit_gdf.crs = utm_crs\n    summit_gdf_web_mercator = summit_gdf.to_crs(web_mercator_crs)\n    \n    # Process each VEI level\n    for i, (vei, path) in enumerate(eruption_probability_paths.items()):\n        ax = axes[i]\n        \n        # Load eruption probability raster\n        with rasterio.open(path) as src:\n            prob_array = src.read(1)\n            transform = src.transform\n            bounds = src.bounds\n            \n            # Get CRS information for the raster\n            raster_crs = src.crs\n            print(f\"Raster CRS for {vei}: {raster_crs}\")\n            \n            # Create coordinate grid in UTM\n            x = np.linspace(bounds.left, bounds.right, prob_array.shape[1])\n            y = np.linspace(bounds.bottom, bounds.top, prob_array.shape[0])\n            X, Y = np.meshgrid(x, y)\n            \n            # Get summit coordinates in UTM\n            summit_x = summit_gdf.geometry.x.values[0]\n            summit_y = summit_gdf.geometry.y.values[0]\n            \n            # Set radius to 10km (10,000m) for all VEI levels for consistency\n            radius = 8000\n            \n            # Create a bounding box for the area around the summit in UTM\n            bbox_utm = box(summit_x - radius, summit_y - radius, \n                          summit_x + radius, summit_y + radius)\n            \n            # Create a GeoDataFrame from the bounding box in UTM\n            bbox_gdf_utm = gpd.GeoDataFrame({'geometry': [bbox_utm]}, crs=utm_crs)\n            \n            # Convert the bounding box to Web Mercator\n            bbox_gdf_web_mercator = bbox_gdf_utm.to_crs(web_mercator_crs)\n            bbox_web_mercator = bbox_gdf_web_mercator.geometry.values[0]\n            \n            # Get Web Mercator bounding box coordinates\n            wm_minx, wm_miny, wm_maxx, wm_maxy = bbox_web_mercator.bounds\n            \n            # Set up the axis for Web Mercator\n            ax.set_xlim(wm_minx, wm_maxx)\n            ax.set_ylim(wm_miny, wm_maxy)\n            \n            # Add OpenStreetMap basemap using contextily with specific provider and parameters\n            try:\n                ctx.add_basemap(\n                    ax,\n                    source=ctx.providers.OpenStreetMap.Mapnik,\n                    zoom=11,  # Adjust zoom level for better detail\n                    attribution_size=6  # Smaller attribution text\n                )\n                print(f\"Successfully added OSM background for {vei}\")\n            except Exception as e:\n                print(f\"Warning: Could not add OpenStreetMap basemap for {vei}. Error: {e}\")\n                # Create a light blue background as fallback\n                ax.imshow(np.ones((100, 100, 3)), \n                         extent=[wm_minx, wm_maxx, wm_miny, wm_maxy],\n                         cmap='Blues', alpha=0.1, vmin=0, vmax=1)\n            \n            # Transform the contour data to Web Mercator\n            X_web_mercator = np.zeros_like(X)\n            Y_web_mercator = np.zeros_like(Y)\n            \n            for row in range(X.shape[0]):\n                for col in range(X.shape[1]):\n                    X_web_mercator[row, col], Y_web_mercator[row, col] = transformer.transform(X[row, col], Y[row, col])\n            \n            # Create contours for probability thresholds using transformed coordinates\n            contours = ax.contour(X_web_mercator, Y_web_mercator, prob_array, levels=thresholds,\n                                 colors='black', linestyles='--', alpha=0.7,\n                                 linewidths=1.0)\n            ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)\n            \n            # Plot hiking trail (already in Web Mercator)\n            hiking_gdf_web_mercator.plot(ax=ax, color='red', linewidth=2.0)\n            \n            # Plot summit with star marker (in Web Mercator)\n            summit_x_wm = summit_gdf_web_mercator.geometry.x.values[0]\n            summit_y_wm = summit_gdf_web_mercator.geometry.y.values[0]\n            ax.plot(summit_x_wm, summit_y_wm, '*', color='blue', markersize=12)\n            \n            # Set title\n            ax.set_title(f'Contour Map Showing Summit Location and Hiking Trail for {vei}', fontsize=10)\n            \n            # Add axis labels\n            ax.set_xlabel('Easting (m)')\n            if i == 0:  # Only add y-label for the first subplot\n                ax.set_ylabel('Northing (m)')\n            \n            # Add legend for each subplot\n            ax.plot([], [], '*', color='blue', markersize=12, label='Summit')\n            ax.plot([], [], '-', color='red', linewidth=2.0, label='Hiking Path')\n            ax.legend(loc='upper right', fontsize=8)\n    \n    # Add a common title to the figure\n    fig.suptitle('Volcano Hazard Maps with OpenStreetMap Basemap', fontsize=16)\n    \n    # Don't use tight_layout with constrained_layout=True\n    # plt.tight_layout()\n    \n    # Save the plot\n    output_path = os.path.join(output_dir, 'vei_comparison_contour_maps.png')\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()\n    print(f\"VEI comparison plot saved to: {output_path}\")\n    return output_path\n\n# Main execution\nif __name__ == \"__main__\":\n    # Define paths\n    data_folder = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results'\n    output_dir = data_folder\n    \n    # Define eruption probability paths for different VEI levels\n    eruption_probability_paths = {\n        'VEI3': r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI3_Project11.tif\",\n        'VEI4': r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI4_cor_proj1.tif\",\n        'VEI5': r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI5_Project11.tif\"\n    }\n    \n    # Define paths for other required files\n    summit_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\"\n    hiking_path = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp'\n    \n    # Print file paths to confirm\n    print(\"\\nUsing file paths:\")\n    for vei, path in eruption_probability_paths.items():\n        print(f\"Eruption probability {vei}: {path}\")\n    print(f\"Summit: {summit_path}\")\n    print(f\"Hiking path: {hiking_path}\")\n    \n    # Create the comparison plot\n    print(\"\\n--- Creating VEI Comparison Plot ---\")\n    comparison_plot = create_vei_comparison_plot(\n        eruption_probability_paths, hiking_path, summit_path, output_dir\n    )\n    print(f\"\\nVEI comparison plot saved to: {comparison_plot}\")\n</pre>   # Import functions from your existing code from data_utils import read_shapefile  def create_vei_comparison_plot(eruption_probability_paths, hiking_path, summit_path, output_dir):     \"\"\"     Create a comparison plot of contour maps for different VEI levels side by side.          Args:         eruption_probability_paths (dict): Dictionary of VEI levels and their file paths         hiking_path (str): Path to the hiking path shapefile         summit_path (str): Path to the summit point shapefile         output_dir (str): Directory to save the plot              Returns:         str: Path to the saved plot     \"\"\"     print(\"\\nCreating VEI comparison plot...\")          # Load hiking trail shapefile     hiking_gdf = read_shapefile(hiking_path)          # Load summit points     summit_gdf = read_shapefile(summit_path)          # Create figure with subplots for each VEI level - make them square and equally sized     fig, axes = plt.subplots(1, len(eruption_probability_paths), figsize=(18, 6),                             sharey=True, sharex=True, constrained_layout=True)          # If only one VEI level, convert axes to list     if len(eruption_probability_paths) == 1:         axes = [axes]              # Make all subplots have equal aspect ratio (square)     for ax in axes:         ax.set_aspect('equal')          # Define eruption probability thresholds - CRITICAL: Must be in ASCENDING order for matplotlib contour     thresholds = [ 0.1,0.5, 0.9]          # Define CRS for UTM Zone 51N and Web Mercator     utm_crs = CRS.from_epsg(32651)  # WGS84 UTM Zone 51N     web_mercator_crs = CRS.from_epsg(3857)  # Web Mercator          # Create transformer from UTM to Web Mercator     transformer = Transformer.from_crs(utm_crs, web_mercator_crs, always_xy=True)          # Transform hiking_gdf to Web Mercator for basemap     if hiking_gdf.crs is None:         hiking_gdf.crs = utm_crs     hiking_gdf_web_mercator = hiking_gdf.to_crs(web_mercator_crs)          # Transform summit_gdf to Web Mercator for basemap     if summit_gdf.crs is None:         summit_gdf.crs = utm_crs     summit_gdf_web_mercator = summit_gdf.to_crs(web_mercator_crs)          # Process each VEI level     for i, (vei, path) in enumerate(eruption_probability_paths.items()):         ax = axes[i]                  # Load eruption probability raster         with rasterio.open(path) as src:             prob_array = src.read(1)             transform = src.transform             bounds = src.bounds                          # Get CRS information for the raster             raster_crs = src.crs             print(f\"Raster CRS for {vei}: {raster_crs}\")                          # Create coordinate grid in UTM             x = np.linspace(bounds.left, bounds.right, prob_array.shape[1])             y = np.linspace(bounds.bottom, bounds.top, prob_array.shape[0])             X, Y = np.meshgrid(x, y)                          # Get summit coordinates in UTM             summit_x = summit_gdf.geometry.x.values[0]             summit_y = summit_gdf.geometry.y.values[0]                          # Set radius to 10km (10,000m) for all VEI levels for consistency             radius = 8000                          # Create a bounding box for the area around the summit in UTM             bbox_utm = box(summit_x - radius, summit_y - radius,                            summit_x + radius, summit_y + radius)                          # Create a GeoDataFrame from the bounding box in UTM             bbox_gdf_utm = gpd.GeoDataFrame({'geometry': [bbox_utm]}, crs=utm_crs)                          # Convert the bounding box to Web Mercator             bbox_gdf_web_mercator = bbox_gdf_utm.to_crs(web_mercator_crs)             bbox_web_mercator = bbox_gdf_web_mercator.geometry.values[0]                          # Get Web Mercator bounding box coordinates             wm_minx, wm_miny, wm_maxx, wm_maxy = bbox_web_mercator.bounds                          # Set up the axis for Web Mercator             ax.set_xlim(wm_minx, wm_maxx)             ax.set_ylim(wm_miny, wm_maxy)                          # Add OpenStreetMap basemap using contextily with specific provider and parameters             try:                 ctx.add_basemap(                     ax,                     source=ctx.providers.OpenStreetMap.Mapnik,                     zoom=11,  # Adjust zoom level for better detail                     attribution_size=6  # Smaller attribution text                 )                 print(f\"Successfully added OSM background for {vei}\")             except Exception as e:                 print(f\"Warning: Could not add OpenStreetMap basemap for {vei}. Error: {e}\")                 # Create a light blue background as fallback                 ax.imshow(np.ones((100, 100, 3)),                           extent=[wm_minx, wm_maxx, wm_miny, wm_maxy],                          cmap='Blues', alpha=0.1, vmin=0, vmax=1)                          # Transform the contour data to Web Mercator             X_web_mercator = np.zeros_like(X)             Y_web_mercator = np.zeros_like(Y)                          for row in range(X.shape[0]):                 for col in range(X.shape[1]):                     X_web_mercator[row, col], Y_web_mercator[row, col] = transformer.transform(X[row, col], Y[row, col])                          # Create contours for probability thresholds using transformed coordinates             contours = ax.contour(X_web_mercator, Y_web_mercator, prob_array, levels=thresholds,                                  colors='black', linestyles='--', alpha=0.7,                                  linewidths=1.0)             ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)                          # Plot hiking trail (already in Web Mercator)             hiking_gdf_web_mercator.plot(ax=ax, color='red', linewidth=2.0)                          # Plot summit with star marker (in Web Mercator)             summit_x_wm = summit_gdf_web_mercator.geometry.x.values[0]             summit_y_wm = summit_gdf_web_mercator.geometry.y.values[0]             ax.plot(summit_x_wm, summit_y_wm, '*', color='blue', markersize=12)                          # Set title             ax.set_title(f'Contour Map Showing Summit Location and Hiking Trail for {vei}', fontsize=10)                          # Add axis labels             ax.set_xlabel('Easting (m)')             if i == 0:  # Only add y-label for the first subplot                 ax.set_ylabel('Northing (m)')                          # Add legend for each subplot             ax.plot([], [], '*', color='blue', markersize=12, label='Summit')             ax.plot([], [], '-', color='red', linewidth=2.0, label='Hiking Path')             ax.legend(loc='upper right', fontsize=8)          # Add a common title to the figure     fig.suptitle('Volcano Hazard Maps with OpenStreetMap Basemap', fontsize=16)          # Don't use tight_layout with constrained_layout=True     # plt.tight_layout()          # Save the plot     output_path = os.path.join(output_dir, 'vei_comparison_contour_maps.png')     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.show()     plt.close()     print(f\"VEI comparison plot saved to: {output_path}\")     return output_path  # Main execution if __name__ == \"__main__\":     # Define paths     data_folder = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results'     output_dir = data_folder          # Define eruption probability paths for different VEI levels     eruption_probability_paths = {         'VEI3': r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI3_Project11.tif\",         'VEI4': r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI4_cor_proj1.tif\",         'VEI5': r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI5_Project11.tif\"     }          # Define paths for other required files     summit_path = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\"     hiking_path = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp'          # Print file paths to confirm     print(\"\\nUsing file paths:\")     for vei, path in eruption_probability_paths.items():         print(f\"Eruption probability {vei}: {path}\")     print(f\"Summit: {summit_path}\")     print(f\"Hiking path: {hiking_path}\")          # Create the comparison plot     print(\"\\n--- Creating VEI Comparison Plot ---\")     comparison_plot = create_vei_comparison_plot(         eruption_probability_paths, hiking_path, summit_path, output_dir     )     print(f\"\\nVEI comparison plot saved to: {comparison_plot}\") <pre>\nUsing file paths:\nEruption probability VEI3: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI3_Project11.tif\nEruption probability VEI4: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI4_cor_proj1.tif\nEruption probability VEI5: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\VEI5_Project11.tif\nSummit: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\nHiking path: C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp\n\n--- Creating VEI Comparison Plot ---\n\nCreating VEI comparison plot...\nRaster CRS for VEI3: PROJCS[\"WGS_1984_UTM_Zone_51N\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\nSuccessfully added OSM background for VEI3\nRaster CRS for VEI4: PROJCS[\"WGS_1984_UTM_Zone_51N\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\nSuccessfully added OSM background for VEI4\nRaster CRS for VEI5: PROJCS[\"WGS_1984_UTM_Zone_51N\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",123],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\nSuccessfully added OSM background for VEI5\n</pre> <pre>VEI comparison plot saved to: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\vei_comparison_contour_maps.png\n\nVEI comparison plot saved to: C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results\\vei_comparison_contour_maps.png\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"demo/awu/probability-analysis.html#probability-analysis","title":"Probability Analysis\u00b6","text":""},{"location":"demo/awu/probability-analysis.html#explanation","title":"Explanation\u00b6","text":"<p>This initial section sets up the Python environment and imports the necessary libraries.</p>"},{"location":"demo/awu/probability-analysis.html#standard-libraries","title":"Standard Libraries\u00b6","text":"<p>The following libraries are used for basic operations, numerical processing, and visualization:</p> <ul> <li><code>os</code> \u2013 Handles file system operations.</li> <li><code>numpy</code> \u2013 Provides numerical processing capabilities.</li> <li><code>datetime</code> \u2013 Manages time-related data.</li> <li><code>sys</code> \u2013 Enables system-specific operations.</li> <li><code>matplotlib.pyplot</code> \u2013 Creates visualizations.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#custom-modules","title":"Custom Modules\u00b6","text":"<p>Several custom modules are utilized for specialized tasks:</p> <ul> <li><code>analysis</code> \u2013 Core functions for evacuation analysis and safe zone identification.</li> <li><code>data_utils</code> \u2013 Utilities for reading geographic data.</li> <li><code>raster_utils</code> \u2013 Functions for processing raster (geospatial) data.</li> <li><code>visualization</code> \u2013 Functions for creating visual outputs.</li> <li><code>graph_utils</code> \u2013 Functions for path reconstruction.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#environment-configuration","title":"Environment Configuration\u00b6","text":"<p>The script ensures correct module imports by adding the current working directory to the Python path.</p>"},{"location":"demo/awu/probability-analysis.html#step-2-configuration-and-input-data-definition","title":"Step 2: Configuration and Input Data Definition\u00b6","text":""},{"location":"demo/awu/probability-analysis.html#explanation","title":"Explanation\u00b6","text":"<p>This section defines all input parameters and data paths required for the evacuation analysis.</p>"},{"location":"demo/awu/probability-analysis.html#output-directory","title":"Output Directory\u00b6","text":"<ul> <li>The output directory is set to the same folder as the input data for easy consolidation of results.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#input-data-paths","title":"Input Data Paths\u00b6","text":"<p>Several input datasets are used in the analysis:</p>"},{"location":"demo/awu/probability-analysis.html#cost-surface-rasters","title":"Cost Surface Rasters\u00b6","text":"<p>Stored in a dictionary (<code>cost_paths</code>), these rasters define the difficulty of movement across the terrain for different scenarios:</p> <ul> <li><code>final</code> \u2013 The original landcover cost surface.</li> <li><code>modify_landcover</code> \u2013 A modified landcover cost surface, incorporating penalties for difficult terrain.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#shapefile-inputs","title":"Shapefile Inputs\u00b6","text":"<p>These files provide geospatial points and paths relevant to the evacuation model:</p> <ul> <li><code>summit_path</code> \u2013 Shapefile containing the volcano summit location.</li> <li><code>camp_spot_path</code> \u2013 Shapefile containing camping locations.</li> <li><code>hiking_path</code> \u2013 Shapefile containing hiking paths.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#eruption-probability-data","title":"Eruption Probability Data\u00b6","text":"<ul> <li><code>eruption_probability_path</code> \u2013 Raster file containing eruption probability data for VEI4 eruptions.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#analysis-parameters","title":"Analysis Parameters\u00b6","text":"<p>Several parameters define the behavior of the evacuation model:</p> <ul> <li><code>walking_speeds</code> \u2013 A dictionary specifying different walking speeds (meters per second) based on terrain.</li> <li><code>source_names</code> \u2013 List of starting points for evacuation scenarios.</li> <li><code>thresholds</code> \u2013 A list of eruption probability thresholds to analyze, ranging from 0.9 to 0.05.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#step-3-data-loading-and-preprocessing","title":"Step 3: Data Loading and Preprocessing\u00b6","text":""},{"location":"demo/awu/probability-analysis.html#explanation","title":"Explanation\u00b6","text":"<p>This step involves loading and preprocessing the input data to ensure it is correctly formatted for analysis.</p>"},{"location":"demo/awu/probability-analysis.html#1-print-file-paths","title":"1. Print File Paths\u00b6","text":"<ul> <li>The script prints the file paths to confirm that they are correctly specified before loading the data.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#2-load-source-point-data","title":"2. Load Source Point Data\u00b6","text":"<ul> <li>Reads the summit shapefile to extract the volcano summit location.</li> <li>Loads the cost raster to obtain geospatial properties (extent, resolution, coordinate reference system).</li> <li>Converts summit coordinates to raster row/column indices for proper alignment with raster data.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#3-load-camping-locations","title":"3. Load Camping Locations\u00b6","text":"<ul> <li>Reads the camp spot shapefile to extract camping locations.</li> <li>Converts camp spot geographic coordinates to raster row/column indices.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#4-combine-source-points","title":"4. Combine Source Points\u00b6","text":"<ul> <li>Creates a list of source points that includes both:<ul> <li>The summit</li> <li>The camping locations</li> </ul> </li> <li>These source points serve as the starting locations for evacuation path analysis.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#5-coordinate-conversion-coords_to_raster","title":"5. Coordinate Conversion (<code>coords_to_raster</code>)\u00b6","text":"<ul> <li>This function is crucial for transforming real-world coordinates (latitude/longitude or projected coordinates) into row/column indices used in raster-based analysis.</li> <li>Ensures proper spatial alignment between vector and raster datasets.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#step-4-evacuation-analysis","title":"Step 4: Evacuation Analysis\u00b6","text":""},{"location":"demo/awu/probability-analysis.html#explanation","title":"Explanation\u00b6","text":"<p>This step initiates the core evacuation analysis, where travel times and optimal evacuation routes are computed.</p>"},{"location":"demo/awu/probability-analysis.html#1-the-perform_evacuation_analysis-function","title":"1. The <code>perform_evacuation_analysis</code> Function\u00b6","text":"<p>This function performs the following tasks:</p> <ul> <li>Processes each cost surface:<ul> <li>Uses both the original landcover and the modified landcover with penalties.</li> </ul> </li> <li>Calculates travel time from each source point.</li> <li>Generates cost distance rasters and predecessor arrays to map the most efficient evacuation routes.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#function-returns","title":"Function Returns:\u00b6","text":"<ul> <li><code>all_results</code> \u2013 A dictionary storing analysis results for:<ul> <li>Each dataset (original and modified landcover)</li> <li>Each source point (summit, camp spots, etc.)</li> <li>Different walking speeds</li> </ul> </li> <li><code>dataset_info</code> \u2013 A dictionary containing metadata about the datasets, including:<ul> <li>Raster dimensions</li> <li>Predecessor arrays (for backtracking paths)</li> </ul> </li> </ul>"},{"location":"demo/awu/probability-analysis.html#2-initialize-evacuation-paths-dictionary","title":"2. Initialize Evacuation Paths Dictionary\u00b6","text":"<ul> <li>A nested dictionary is created to store the optimal evacuation paths.</li> <li>Organized by:<ul> <li>Dataset key (original vs. modified cost surface)</li> <li>Probability threshold (evacuation scenarios based on eruption risk)</li> </ul> </li> </ul>"},{"location":"demo/awu/probability-analysis.html#3-dijkstras-algorithm-for-travel-time-calculation","title":"3. Dijkstra\u2019s Algorithm for Travel Time Calculation\u00b6","text":"<ul> <li>The <code>perform_evacuation_analysis</code> function implements Dijkstra\u2019s algorithm to compute:<ul> <li>Minimum travel time from each source point to all other points in the raster.</li> <li>The most efficient paths for evacuation, given terrain conditions and eruption risk.</li> </ul> </li> </ul> <p>This analysis is crucial for determining safe evacuation strategies based on real-world constraints.</p>"},{"location":"demo/awu/probability-analysis.html#step-5-safe-zone-analysis","title":"Step 5: Safe Zone Analysis\u00b6","text":""},{"location":"demo/awu/probability-analysis.html#explanation","title":"Explanation\u00b6","text":"<p>This step identifies safe zones for evacuation based on eruption probability thresholds and calculates optimal evacuation paths.</p>"},{"location":"demo/awu/probability-analysis.html#1-safe-zone-analysis-for-each-dataset","title":"1. Safe Zone Analysis for Each Dataset\u00b6","text":"<p>For both original and modified landcover datasets:</p> <ul> <li>Load travel time data \u2013 Retrieves previously computed travel times.</li> <li>Analyze safe zones \u2013 Identifies locations that meet two key criteria:<ol> <li>Below the specified eruption probability threshold.</li> <li>Reachable within the minimum travel time.</li> </ol> </li> </ul>"},{"location":"demo/awu/probability-analysis.html#2-extracting-evacuation-paths","title":"2. Extracting Evacuation Paths\u00b6","text":"<ul> <li>For each probability threshold, finds the optimal evacuation path from the summit.</li> <li>Uses the predecessor array to reconstruct the shortest evacuation route.</li> <li>Converts row/column indices to 1D indices as needed.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#3-analyze_safe_zones-function","title":"3. <code>analyze_safe_zones</code> Function\u00b6","text":"<ul> <li>Combines eruption probability data with travel time data.</li> <li>For each threshold, determines:<ul> <li>The minimum travel time to a safe location.</li> <li>The coordinates of the safest point within the probability limit.</li> </ul> </li> </ul>"},{"location":"demo/awu/probability-analysis.html#function-returns","title":"Function Returns:\u00b6","text":"<ul> <li>The minimum travel time required to reach safety.</li> <li>The coordinates of the safe zone.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#4-reconstruct_path-function","title":"4. <code>reconstruct_path</code> Function\u00b6","text":"<ul> <li>Uses the predecessor array from Dijkstra's algorithm to trace back the shortest path.</li> <li>Process:<ol> <li>Starts from the destination (safe point).</li> <li>Follows predecessors backward until it reaches the source (summit or camping spot).</li> </ol> </li> </ul> <p>This step ensures that each evacuation scenario accounts for realistic travel constraints, helping in effective evacuation planning.</p>"},{"location":"demo/awu/probability-analysis.html#step-6-visualization-travel-time-comparison","title":"Step 6: Visualization - Travel Time Comparison\u00b6","text":""},{"location":"demo/awu/probability-analysis.html#explanation","title":"Explanation\u00b6","text":"<p>This step generates comparison plots to visually analyze travel times across different evacuation scenarios.</p>"},{"location":"demo/awu/probability-analysis.html#1-plot_travel_time_comparison-function","title":"1. <code>plot_travel_time_comparison</code> Function\u00b6","text":"<p>This function creates plots that compare travel times under various conditions.</p>"},{"location":"demo/awu/probability-analysis.html#function-parameters","title":"Function Parameters:\u00b6","text":"<ul> <li><code>all_results</code> \u2013 Dictionary containing evacuation analysis results.</li> <li><code>source_names</code> \u2013 Names of the starting points for evacuation.</li> <li><code>thresholds</code> \u2013 List of eruption probability thresholds.</li> <li><code>walking_speeds</code> \u2013 Different walking speeds (meters per second).</li> <li><code>output_dir</code> \u2013 Directory where the generated plot will be saved.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#function-output","title":"Function Output:\u00b6","text":"<ul> <li>Returns the file path of the saved visualization.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#2-creating-and-displaying-plots","title":"2. Creating and Displaying Plots\u00b6","text":"<ul> <li>Creates a new figure to represent travel time data.</li> <li>Displays the visualization to provide a clear comparison of evacuation scenarios.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#3-purpose-of-visualization","title":"3. Purpose of Visualization\u00b6","text":"<ul> <li>Helps compare evacuation times across different:<ul> <li>Landcover scenarios (original vs. modified).</li> <li>Walking speeds (slow, moderate, fast).</li> <li>Probability thresholds (high to low eruption risks).</li> </ul> </li> <li>Enables data-driven decision-making for improving evacuation planning.</li> </ul> <p>This step ensures that evacuation strategies can be visually interpreted for better assessment and response.</p>"},{"location":"demo/awu/probability-analysis.html#step-7-visualization-cost-surface-and-paths","title":"Step 7: Visualization - Cost Surface and Paths\u00b6","text":""},{"location":"demo/awu/probability-analysis.html#explanation","title":"Explanation\u00b6","text":"<p>This step generates visual maps displaying the cost surface (terrain difficulty) with overlaid evacuation paths.</p>"},{"location":"demo/awu/probability-analysis.html#1-plot_cost_surface_with_paths-function","title":"1. <code>plot_cost_surface_with_paths</code> Function\u00b6","text":"<p>This function creates maps that visualize:</p> <ul> <li>The cost surface (difficulty of movement across terrain).</li> <li>The optimal evacuation paths for different eruption probability thresholds.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#function-parameters","title":"Function Parameters:\u00b6","text":"<ul> <li><code>dataset_info</code> \u2013 Dictionary containing dataset metadata.</li> <li><code>evacuation_paths</code> \u2013 Dictionary storing calculated evacuation paths.</li> <li><code>eruption_probability_path</code> \u2013 Path to the eruption probability raster.</li> <li><code>hiking_path</code> \u2013 Path to the hiking trail shapefile.</li> <li><code>selected_speed</code> \u2013 Walking speed to visualize (medium speed is typically used).</li> <li><code>thresholds</code> \u2013 List of eruption probability thresholds.</li> <li><code>output_dir</code> \u2013 Directory where plots will be saved.</li> <li><code>vei_label</code> \u2013 Label for the Volcanic Explosivity Index (e.g., VEI4).</li> </ul>"},{"location":"demo/awu/probability-analysis.html#function-output","title":"Function Output:\u00b6","text":"<ul> <li>Returns file paths of the saved visualization plots.</li> </ul>"},{"location":"demo/awu/probability-analysis.html#2-purpose-of-visualization","title":"2. Purpose of Visualization\u00b6","text":"<ul> <li>Provides a spatial understanding of:<ul> <li>How evacuation routes interact with the terrain.</li> <li>The relationship between safe zones and hiking paths.</li> </ul> </li> <li>Helps decision-makers assess evacuation feasibility visually.</li> </ul> <p>This step enhances situational awareness, ensuring better evacuation planning under different eruption risk scenarios.</p>"},{"location":"modules/cost-calculations.html","title":"Cost Calculation Module","text":""},{"location":"modules/cost-calculations.html#overview","title":"Overview","text":"<p>The Cost Calculation module provides the foundation for creating and analyzing cost surfaces for geographic terrain analysis. It consists of a collection of utilities that enable processing of Digital Elevation Models (DEMs), handling of land cover data, and creation of cost surfaces that represent the difficulty of traversing different types of terrain.</p>"},{"location":"modules/cost-calculations.html#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Converting land cover classes to cost values</li> <li>Calculating slopes in multiple directions from DEMs</li> <li>Implementing Tobler's Hiking Function to estimate realistic walking speeds</li> <li>Adjusting cost surfaces based on terrain features such as streams and hiking paths</li> <li>Visualizing analysis results with customizable maps and plots</li> </ul>"},{"location":"modules/cost-calculations.html#components","title":"Components","text":"<p>The Cost Calculation module consists of the following primary components:</p>"},{"location":"modules/cost-calculations.html#cost-calculations-cost_calculationspy","title":"Cost Calculations (<code>cost_calculations.py</code>)","text":"<p>This component handles the conversion of land cover and terrain data into cost surfaces:</p> <ul> <li><code>map_landcover_to_cost</code>: Maps land cover classes to associated traversal costs</li> <li><code>rasterize_layer</code>: Converts vector geometries to raster format</li> <li><code>update_cost_raster</code>: Updates cost values based on features like streams and hiking paths</li> <li><code>adjust_cost_with_walking_speed</code>: Integrates slope-based walking speeds with land cover costs</li> <li><code>invert_cost_array</code>: Prepares cost values for path finding by inverting them</li> </ul>"},{"location":"modules/cost-calculations.html#dem-processing-dem_processingpy","title":"DEM Processing (<code>dem_processing.py</code>)","text":"<p>This component focuses on extracting and analyzing slope information from DEMs:</p> <ul> <li><code>calculate_slope</code>: Computes slopes in eight cardinal and ordinal directions</li> <li><code>calculate_walking_speed</code>: Applies Tobler's Hiking Function to estimate walking speeds</li> <li><code>normalize_walking_speed</code>: Converts speeds to normalized values for analysis</li> <li><code>get_max_velocity</code>: Calculates the maximum theoretical walking speed</li> </ul>"},{"location":"modules/cost-calculations.html#data-loading-data_loadingpy","title":"Data Loading (<code>data_loading.py</code>)","text":"<p>This component provides functions for loading geospatial data:</p> <ul> <li><code>read_shapefile</code>: Loads vector data from shapefiles</li> <li><code>read_raster</code>: Loads raster data with associated metadata</li> </ul>"},{"location":"modules/cost-calculations.html#plotting-utilities-plotting_utilspy","title":"Plotting Utilities (<code>plotting_utils.py</code>)","text":"<p>This component contains visualization functions for presenting analysis results:</p> <ul> <li><code>plot_continuous_raster_with_points</code>: Creates maps with raster backgrounds and point overlays</li> <li><code>plot_normalized_walking_speed</code>: Visualizes walking speed data across terrain</li> <li><code>plot_walking_speed_vs_slope</code>: Creates slope-vs-speed relationship plots</li> <li>Various other specialized plotting functions for different analysis outputs</li> </ul>"},{"location":"modules/cost-calculations.html#workflow","title":"Workflow","text":"<p>A typical workflow using the Cost Calculation module involves:</p> <ol> <li>Loading terrain and land cover data using the data loading utilities</li> <li>Processing the DEM to calculate slopes and walking speeds</li> <li>Converting land cover data to cost values</li> <li>Combining the walking speed and land cover costs to create a comprehensive cost surface</li> <li>Inverting the cost surface for path finding</li> <li>Visualizing the results using the plotting utilities</li> </ol>"},{"location":"modules/cost-calculations.html#usage-example","title":"Usage Example","text":"<pre><code># Import necessary modules\nfrom data_loading import read_raster, read_shapefile\nfrom dem_processing import calculate_slope, calculate_walking_speed, normalize_walking_speed\nfrom cost_calculations import map_landcover_to_cost, update_cost_raster, invert_cost_array\nfrom plotting_utils import plot_continuous_raster_with_points\n\n# Load data\ndem_data, dem_meta, dem_transform, dem_nodata, dem_bounds, dem_res = read_raster(\"dem.tif\")\nlandcover_data, lc_meta, lc_transform, lc_nodata, lc_bounds, lc_res = read_raster(\"landcover.tif\")\nsummit_points = read_shapefile(\"summit.shp\")\n\n# Process DEM\nslope_array = calculate_slope(dem_data[0], dem_res[0], dem_res[1], dem_nodata)\nwalking_speed_array = calculate_walking_speed(slope_array)\nnormalized_ws = normalize_walking_speed(walking_speed_array)\n\n# Process land cover\nland_cover_cost_mapping = {1: 1.0, 2: 1.5, 3: 3.0, 4: 10.0}  # Example mapping\nlandcover_cost = map_landcover_to_cost(landcover_data[0], land_cover_cost_mapping)\n\n# Combine costs\nadjusted_cost = adjust_cost_with_walking_speed(normalized_ws, landcover_cost)\ninverted_cost = invert_cost_array(adjusted_cost)\n\n# Visualize results\nplot_continuous_raster_with_points(\n    inverted_cost[0],  # Use first direction for visualization\n    dem_transform.to_gdal(),\n    summit_points,\n    \"Cost Surface with Summit Point\",\n    \"Cost Value\",\n    \"cost_surface_map.jpg\"\n)\n</code></pre>"},{"location":"modules/cost-calculations.html#integration","title":"Integration","text":"<p>The Cost Calculation module serves as the foundation for the Evacuation Analysis module, which builds upon the cost surfaces to calculate evacuation paths and times. The outputs from this module can be directly fed into evacuation analysis workflows to model realistic terrain-based evacuation scenarios.</p>"},{"location":"modules/evacuation-analysis.html","title":"Evacuation Analysis Module","text":""},{"location":"modules/evacuation-analysis.html#overview","title":"Overview","text":"<p>The Evacuation Analysis module provides tools for analyzing evacuation scenarios in volcanic risk assessments. Building on the cost surfaces created by the Cost Calculation module, it performs path finding, calculates evacuation times, and analyzes safe zones based on distance from hazard sources.</p>"},{"location":"modules/evacuation-analysis.html#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Building graph representations of geographic terrain for path finding</li> <li>Computing optimal evacuation routes using Dijkstra's algorithm</li> <li>Analyzing minimum travel times to reach safe zones</li> <li>Decomposing evacuation paths to understand contributing factors</li> <li>Visualizing evacuation paths and travel times</li> </ul>"},{"location":"modules/evacuation-analysis.html#components","title":"Components","text":"<p>The Evacuation Analysis module consists of the following primary components:</p>"},{"location":"modules/evacuation-analysis.html#path-utilities-path_utilspy","title":"Path Utilities (<code>path_utils.py</code>)","text":"<p>This component provides functions for path finding and analysis:</p> <ul> <li><code>build_adjacency_matrix</code>: Converts cost rasters to graph representations</li> <li><code>reconstruct_path</code>: Converts shortest path results to coordinate sequences</li> <li><code>calculate_path_metrics</code>: Computes metrics for evacuation paths</li> </ul>"},{"location":"modules/evacuation-analysis.html#analysis-analysispy","title":"Analysis (<code>analysis.py</code>)","text":"<p>This component contains the core evacuation analysis functionality:</p> <ul> <li><code>run_dijkstra_analysis</code>: Executes Dijkstra's algorithm for shortest path finding</li> <li><code>process_travel_times</code>: Converts cost distances to time-based metrics</li> <li><code>analyze_safe_zones</code>: Identifies minimum travel times to safe zones</li> </ul>"},{"location":"modules/evacuation-analysis.html#decomposition-decompositionpy","title":"Decomposition (<code>decomposition.py</code>)","text":"<p>This component analyzes the relative contributions of different factors to evacuation paths:</p> <ul> <li><code>reconstruct_path</code>: Path reconstruction for detailed analysis</li> <li><code>expand_single_band</code>: Expands single-band cost data to directional analysis</li> <li><code>run_decomposition_analysis</code>: Quantifies slope vs. land cover contributions</li> </ul>"},{"location":"modules/evacuation-analysis.html#io-utilities-io_utilspy","title":"IO Utilities (<code>io_utils.py</code>)","text":"<p>This component provides input/output functions for evacuation analysis:</p> <ul> <li><code>read_shapefile</code> &amp; <code>read_raster</code>: Data loading functions</li> <li><code>save_raster</code>: Saves analysis results as geospatial rasters</li> <li><code>save_analysis_report</code> &amp; <code>save_metrics_csv</code>: Export analysis results</li> </ul>"},{"location":"modules/evacuation-analysis.html#configuration-configpy","title":"Configuration (<code>config.py</code>)","text":"<p>This component defines parameters and settings for the analysis:</p> <ul> <li>Walking speeds for different scenarios</li> <li>File paths for input data</li> <li>Source locations and safe zone distances</li> <li>Movement direction definitions</li> </ul>"},{"location":"modules/evacuation-analysis.html#grid-utilities-grid_utilspy","title":"Grid Utilities (<code>grid_utils.py</code>)","text":"<p>This component provides utilities for working with raster grids:</p> <ul> <li><code>coords_to_raster</code>: Converts geographic coordinates to raster indices</li> <li><code>to_1d</code>: Converts 2D grid coordinates to 1D indices</li> <li><code>raster_coord_to_map_coords</code>: Converts raster indices to geographic coordinates</li> <li><code>process_raster</code>: Converts cost values to travel times</li> <li><code>calculate_distance_from_summit</code>: Computes Euclidean distances from points</li> </ul>"},{"location":"modules/evacuation-analysis.html#workflow","title":"Workflow","text":"<p>A typical workflow using the Evacuation Analysis module involves:</p> <ol> <li>Loading cost surfaces from the Cost Calculation module</li> <li>Building a graph representation of the terrain</li> <li>Computing shortest paths from evacuation sources</li> <li>Analyzing travel times to safe zones</li> <li>Decomposing evacuation paths to understand contributing factors</li> <li>Generating visualizations and reports</li> </ol>"},{"location":"modules/evacuation-analysis.html#usage-example","title":"Usage Example","text":"<pre><code># Import necessary modules\nfrom io_utils import read_raster, save_analysis_report\nfrom path_utils import build_adjacency_matrix\nfrom analysis import run_dijkstra_analysis, analyze_safe_zones\nfrom grid_utils import calculate_distance_from_summit, to_1d\nfrom config import WALKING_SPEEDS, SOURCE_NAMES, SAFE_ZONE_DISTANCES\n\n# Load inverted cost raster\ncost_array, meta, transform, nodata, bounds, res = read_raster(\"inverted_cost.tif\")\nrows, cols = cost_array.shape[1:3]\n\n# Define source coordinates (summit and camps)\nsource_coords = [(100, 150), (120, 140)]  # Example coordinates\n\n# Convert 2D source coordinates to 1D node indices\nsource_nodes = [to_1d(r, c, cols) for r, c in source_coords]\n\n# Build adjacency matrix for path finding\ngraph_csr = build_adjacency_matrix(cost_array, rows, cols)\n\n# Run Dijkstra's algorithm\ndistances, predecessors = run_dijkstra_analysis(graph_csr, source_nodes)\n\n# Calculate distance from summit for safe zone analysis\ndistance_from_summit = calculate_distance_from_summit(source_coords[0], rows, cols)\n\n# Process travel times and analyze safe zones\ntravel_time_data = {}\nfor speed_name, speed_value in WALKING_SPEEDS.items():\n    travel_time_data[speed_name] = {}\n    for i, source_name in enumerate(SOURCE_NAMES):\n        distance_array = distances[i].reshape(rows, cols)\n        travel_time = distance_array * 100 / speed_value / 3600  # Convert to hours\n        travel_time_data[speed_name][source_name] = {\n            'cost_array': travel_time,\n            'cost_array_flat': travel_time.ravel()\n        }\n\n# Analyze safe zones\nresults, min_coords = analyze_safe_zones(\n    distance_from_summit, \n    travel_time_data, \n    SAFE_ZONE_DISTANCES, \n    SOURCE_NAMES\n)\n\n# Save analysis report\nsave_analysis_report(\n    \"evacuation_report.txt\",\n    results,\n    min_coords,\n    SOURCE_NAMES,\n    WALKING_SPEEDS,\n    SAFE_ZONE_DISTANCES\n)\n</code></pre>"},{"location":"modules/evacuation-analysis.html#integration","title":"Integration","text":"<p>The Evacuation Analysis module builds upon the cost surfaces created by the Cost Calculation module and is extended by the Probability Analysis module, which incorporates eruption probability thresholds into the evacuation analysis. Together, these modules provide a comprehensive framework for volcanic evacuation analysis and planning.</p>"},{"location":"modules/probability-analysis.html","title":"Probability Analysis Module","text":""},{"location":"modules/probability-analysis.html#overview","title":"Overview","text":"<p>The Probability Analysis module extends the evacuation analysis framework to incorporate volcanic eruption probability thresholds. This module enables the assessment of evacuation scenarios based on probabilistic hazard zones, allowing for more nuanced risk assessment across different Volcanic Explosivity Index (VEI) levels.</p>"},{"location":"modules/probability-analysis.html#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Integrating eruption probability data into evacuation analysis</li> <li>Defining safe zones based on probability thresholds rather than simple distances</li> <li>Comparing evacuation scenarios across different VEI levels</li> <li>Visualizing evacuation paths with probability contours</li> <li>Generating statistical reports and visualizations of analysis results</li> </ul>"},{"location":"modules/probability-analysis.html#components","title":"Components","text":"<p>The Probability Analysis module consists of the following primary components:</p>"},{"location":"modules/probability-analysis.html#graph-utilities-graph_utilspy","title":"Graph Utilities (<code>graph_utils.py</code>)","text":"<p>This component provides functions for graph-based path finding with probability considerations:</p> <ul> <li><code>build_adjacency_matrix</code>: Creates graph representations for path analysis</li> <li><code>compute_shortest_paths</code>: Calculates optimal evacuation routes</li> <li><code>reconstruct_path</code>: Converts path results to geographic coordinates</li> <li><code>calculate_path_metrics</code>: Analyzes metrics for evacuation paths</li> </ul>"},{"location":"modules/probability-analysis.html#raster-utilities-raster_utilspy","title":"Raster Utilities (<code>raster_utils.py</code>)","text":"<p>This component contains utilities for raster processing and coordinate conversion:</p> <ul> <li><code>resample_raster</code>: Aligns probability rasters with cost rasters</li> <li><code>coords_to_raster</code>: Converts geographic coordinates to raster indices</li> <li><code>raster_coord_to_map_coords</code>: Converts indices to geographic coordinates</li> <li><code>process_raster</code>: Converts costs to travel times</li> </ul>"},{"location":"modules/probability-analysis.html#visualization-visualizationpy","title":"Visualization (<code>visualization.py</code>)","text":"<p>This component provides specialized visualization functions for probability-based analysis:</p> <ul> <li><code>plot_travel_time_comparison</code>: Compares evacuation times for different thresholds</li> <li><code>plot_cost_surface_with_paths</code>: Visualizes evacuation paths with probability contours</li> <li><code>create_vei_comparison_plot</code>: Compares eruption scenarios across VEI levels</li> </ul>"},{"location":"modules/probability-analysis.html#data-utilities-data_utilspy","title":"Data Utilities (<code>data_utils.py</code>)","text":"<p>This component contains data handling functions specific to probability analysis:</p> <ul> <li><code>read_shapefile</code> &amp; <code>read_raster</code>: Data loading functions</li> <li><code>save_raster</code>: Saves analysis results as geospatial rasters</li> <li><code>save_analysis_report</code>: Creates reports of probability-based evacuation analysis</li> <li><code>create_statistics_table</code>: Generates statistical summaries of evacuation times</li> </ul>"},{"location":"modules/probability-analysis.html#analysis-analysispy","title":"Analysis (<code>analysis.py</code>)","text":"<p>This component extends evacuation analysis to incorporate probability thresholds:</p> <ul> <li><code>perform_evacuation_analysis</code>: Conducts evacuation analysis across datasets</li> <li><code>analyze_safe_zones</code>: Identifies safe areas based on probability thresholds</li> <li><code>load_travel_time_data</code>: Prepares travel time data for analysis</li> <li><code>read_raster</code>: Reads eruption probability and cost rasters</li> </ul>"},{"location":"modules/probability-analysis.html#workflow","title":"Workflow","text":"<p>A typical workflow using the Probability Analysis module involves:</p> <ol> <li>Loading eruption probability rasters for one or more VEI levels</li> <li>Aligning probability data with cost surfaces from previous analysis</li> <li>Computing evacuation paths for different probability thresholds</li> <li>Analyzing travel times to probability-based safe zones</li> <li>Comparing evacuation scenarios across different VEI levels</li> <li>Generating visualizations and statistical reports</li> </ol>"},{"location":"modules/probability-analysis.html#usage-example","title":"Usage Example","text":"<pre><code># Import necessary modules\nfrom data_utils import read_shapefile, read_raster, save_analysis_report\nfrom graph_utils import build_adjacency_matrix, compute_shortest_paths\nfrom raster_utils import resample_raster, coords_to_raster\nfrom analysis import analyze_safe_zones, load_travel_time_data\nfrom visualization import plot_cost_surface_with_paths, create_vei_comparison_plot\n\n# Define paths and parameters\ncost_paths = {\n    'final': 'inverted_cost_original.tif',\n    'modify_landcover': 'inverted_cost_modified.tif'\n}\nprobability_paths = {\n    'VEI3': 'eruption_probability_vei3.tif',\n    'VEI4': 'eruption_probability_vei4.tif',\n    'VEI5': 'eruption_probability_vei5.tif'\n}\nsource_names = ['summit', 'camp1']\nwalking_speeds = {'slow': 0.91, 'medium': 1.22, 'fast': 1.52}\nprobability_thresholds = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\noutput_dir = 'results'\n\n# Load source locations\nsummit_data = read_shapefile('summit.shp')\nhiking_data = read_shapefile('hiking_path.shp')\n\n# Process each VEI level\nfor vei, prob_path in probability_paths.items():\n    print(f\"\\nAnalyzing {vei} scenario...\")\n\n    # Process each cost dataset\n    for dataset_key, cost_path in cost_paths.items():\n        # Load travel time data\n        travel_time_data = load_travel_time_data(\n            dataset_key, source_names, walking_speeds, output_dir\n        )\n\n        # Analyze safe zones based on probability thresholds\n        results, min_coords = analyze_safe_zones(\n            prob_path, travel_time_data, probability_thresholds,\n            source_names, walking_speeds, dataset_key, output_dir\n        )\n\n        # Create evacuation path visualizations\n        plot_cost_surface_with_paths(\n            dataset_info, evacuation_paths, prob_path,\n            hiking_data, 'medium', probability_thresholds,\n            output_dir, vei_label=vei\n        )\n\n# Create VEI comparison visualization\ncreate_vei_comparison_plot(\n    probability_paths, 'hiking_path.shp', 'summit.shp', output_dir\n)\n</code></pre>"},{"location":"modules/probability-analysis.html#integration","title":"Integration","text":"<p>The Probability Analysis module builds upon both the Cost Calculation and Evacuation Analysis modules to provide a comprehensive framework for volcanic risk assessment. By incorporating eruption probability data, it enables more realistic and nuanced evacuation planning that accounts for the variable spatial distribution of volcanic hazards across different eruption scenarios.</p>"},{"location":"source-code/index.html","title":"Source Code","text":"<p>This section provides direct access to the Python source files that implement the Volcano Pedestrian Evacuation Analysis toolkit. You can view and download these files for use in your own projects.</p>"},{"location":"source-code/index.html#available-modules","title":"Available Modules","text":""},{"location":"source-code/index.html#cost-calculation","title":"Cost Calculation","text":"<ul> <li>data-loading.py: Functions for loading shapefiles and raster data</li> <li>dem-processing.py: Functions for calculating slopes and walking speeds</li> <li>cost-calculations.py: Core functions for creating and manipulating cost surfaces</li> <li>plotting-utils.py: Visualization functions for raster data and analysis results</li> </ul>"},{"location":"source-code/index.html#evacuation-analysis","title":"Evacuation Analysis","text":"<ul> <li>io-utils.py: Input/output utilities</li> <li>config.py: Configuration settings and parameters</li> <li>analysis.py: Core evacuation analysis functions</li> <li>grid-utils.py: Utilities for working with raster grids</li> <li>path-utils.py: Utilities for path finding and graph operations</li> <li>decomposition.py: Functions for analyzing factor contributions</li> <li>visualization.py: Functions for visualizing evacuation analysis results</li> </ul>"},{"location":"source-code/index.html#probability-analysis","title":"Probability Analysis","text":"<ul> <li>data-utils.py: Data handling utilities</li> <li>raster-utils.py: Raster processing and coordinate transformations</li> <li>probability-analysis.py: Probability-based evacuation analysis</li> <li>graph-utils.py: Graph construction and shortest path algorithms</li> <li>prob-visualization.py: Visualization functions for probability analysis</li> </ul>"},{"location":"source-code/index.html#dependencies","title":"Dependencies","text":"<p>These modules require several Python packages to function properly. Please see the Installation Guide for detailed dependency information.</p>"},{"location":"source-code/index.html#documentation","title":"Documentation","text":"<p>For detailed documentation of each module and function, please visit the API Reference section.</p>"},{"location":"source-code/cost-calculation/cost-calculations.html","title":"cost_calculations.py","text":"<p>cost_calculations.py</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport rasterio\nfrom rasterio.features import rasterize\n</pre> import numpy as np import rasterio from rasterio.features import rasterize In\u00a0[\u00a0]: Copied! <pre>def map_landcover_to_cost(landcover_data, land_cover_cost_mapping):\n    \"\"\"\n    Maps land cover classes to their corresponding cost values.\n    Parameters:\n    landcover_data (numpy.ndarray): A 2D array representing land cover classes.\n    land_cover_cost_mapping (dict): A dictionary where keys are land cover class identifiers \n    and values are the corresponding cost values.\n    Returns:\n    numpy.ndarray: A 2D array of the same shape as landcover_data, where each element is the \n    cost value corresponding to the land cover class at that position.\n    \"\"\"\n  \n    cost_raster = np.zeros_like(landcover_data, dtype=np.float32)\n    for land_cover_class, cost_value in land_cover_cost_mapping.items():\n        cost_raster[landcover_data == land_cover_class] = cost_value\n    return cost_raster\n</pre> def map_landcover_to_cost(landcover_data, land_cover_cost_mapping):     \"\"\"     Maps land cover classes to their corresponding cost values.     Parameters:     landcover_data (numpy.ndarray): A 2D array representing land cover classes.     land_cover_cost_mapping (dict): A dictionary where keys are land cover class identifiers      and values are the corresponding cost values.     Returns:     numpy.ndarray: A 2D array of the same shape as landcover_data, where each element is the      cost value corresponding to the land cover class at that position.     \"\"\"        cost_raster = np.zeros_like(landcover_data, dtype=np.float32)     for land_cover_class, cost_value in land_cover_cost_mapping.items():         cost_raster[landcover_data == land_cover_class] = cost_value     return cost_raster In\u00a0[\u00a0]: Copied! <pre>def rasterize_layer(geometry_list, out_shape, transform, burn_value, fill_value=np.nan):\n    \"\"\"\n    Rasterizes a given geometry.\n\n    Parameters:\n    geometry_list (list): A list of geometries to rasterize.\n    out_shape (tuple): The shape of the output array (height, width).\n    transform (Affine): The affine transformation to apply.\n    burn_value (float): The value to burn into the raster for the geometries.\n    fill_value (float, optional): The value to fill the raster where there are no geometries. Default is np.nan.\n\n    Returns:\n    numpy.ndarray: A 2D array with the rasterized geometries.\n    \"\"\"\n    return rasterize(\n        [(geom, burn_value) for geom in geometry_list],\n        out_shape=out_shape,\n        transform=transform,\n        fill=fill_value,\n        all_touched=True,\n        dtype='float32'\n    )\n</pre> def rasterize_layer(geometry_list, out_shape, transform, burn_value, fill_value=np.nan):     \"\"\"     Rasterizes a given geometry.      Parameters:     geometry_list (list): A list of geometries to rasterize.     out_shape (tuple): The shape of the output array (height, width).     transform (Affine): The affine transformation to apply.     burn_value (float): The value to burn into the raster for the geometries.     fill_value (float, optional): The value to fill the raster where there are no geometries. Default is np.nan.      Returns:     numpy.ndarray: A 2D array with the rasterized geometries.     \"\"\"     return rasterize(         [(geom, burn_value) for geom in geometry_list],         out_shape=out_shape,         transform=transform,         fill=fill_value,         all_touched=True,         dtype='float32'     ) In\u00a0[\u00a0]: Copied! <pre>def update_cost_raster(landcover_cost_data, stream_raster, hiking_path_raster):\n    \"\"\"\n    Updates the cost raster based on landcover data, stream locations, and hiking paths.\n    Parameters:\n    landcover_cost_data (numpy.ndarray): A 2D array representing the cost associated with different landcover types.\n    stream_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of streams.\n    hiking_path_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of hiking paths.\n    Returns:\n    numpy.ndarray: A 2D array with updated cost values where streams are marked as impassable (cost = 0) \n    and hiking paths are marked as passable (cost = 1).\n    \"\"\"\n   \n    updated_cost_raster = landcover_cost_data.copy()\n    landcover_mask = np.isnan(landcover_cost_data)\n    updated_cost_raster[~landcover_mask &amp; ~np.isnan(stream_raster)] = 0  # Streams impassable\n    updated_cost_raster[~landcover_mask &amp; ~np.isnan(hiking_path_raster)] = 1  # Hiking paths passable\n    return updated_cost_raster\n</pre> def update_cost_raster(landcover_cost_data, stream_raster, hiking_path_raster):     \"\"\"     Updates the cost raster based on landcover data, stream locations, and hiking paths.     Parameters:     landcover_cost_data (numpy.ndarray): A 2D array representing the cost associated with different landcover types.     stream_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of streams.     hiking_path_raster (numpy.ndarray): A 2D array where non-NaN values indicate the presence of hiking paths.     Returns:     numpy.ndarray: A 2D array with updated cost values where streams are marked as impassable (cost = 0)      and hiking paths are marked as passable (cost = 1).     \"\"\"         updated_cost_raster = landcover_cost_data.copy()     landcover_mask = np.isnan(landcover_cost_data)     updated_cost_raster[~landcover_mask &amp; ~np.isnan(stream_raster)] = 0  # Streams impassable     updated_cost_raster[~landcover_mask &amp; ~np.isnan(hiking_path_raster)] = 1  # Hiking paths passable     return updated_cost_raster In\u00a0[\u00a0]: Copied! <pre>def adjust_cost_with_walking_speed(normalized_walking_speed_array, combined_data):\n    \"\"\"\n    Adjusts the cost based on the normalized walking speed.\n\n    This function takes an array of normalized walking speeds and an array of combined data,\n    and returns an array where each element is the product of the corresponding elements\n    from the input arrays.\n\n    Parameters:\n    normalized_walking_speed_array (numpy.ndarray): An array of normalized walking speeds.\n    combined_data (numpy.ndarray): An array of combined data.\n\n    Returns:\n    numpy.ndarray: An array of adjusted costs.\n    \"\"\"\n\n    adjusted_cost_array = normalized_walking_speed_array * combined_data\n    return adjusted_cost_array\n</pre> def adjust_cost_with_walking_speed(normalized_walking_speed_array, combined_data):     \"\"\"     Adjusts the cost based on the normalized walking speed.      This function takes an array of normalized walking speeds and an array of combined data,     and returns an array where each element is the product of the corresponding elements     from the input arrays.      Parameters:     normalized_walking_speed_array (numpy.ndarray): An array of normalized walking speeds.     combined_data (numpy.ndarray): An array of combined data.      Returns:     numpy.ndarray: An array of adjusted costs.     \"\"\"      adjusted_cost_array = normalized_walking_speed_array * combined_data     return adjusted_cost_array In\u00a0[\u00a0]: Copied! <pre>def invert_cost_array(adjusted_cost_array):\n    \"\"\"\n    Inverts the values in the given cost array.\n    This function takes an array of adjusted costs and inverts each value. \n    If a value is zero, it is replaced with NaN before inversion to avoid \n    division by zero. After inversion, NaN values are replaced with a large \n    number (1e6).\n    Parameters:\n    adjusted_cost_array (numpy.ndarray): A numpy array of adjusted costs.\n    Returns:\n    numpy.ndarray: A numpy array with the inverted cost values.\n    \"\"\"\n  \n    def invert_values(combined_value):\n        combined_value[combined_value == 0] = np.nan\n        inverted_values = np.where(np.isnan(combined_value), np.nan, 1.0 / combined_value)\n        inverted_values[np.isnan(inverted_values)] = 1e6\n        return inverted_values\n\n    inverted_cost_array = np.apply_along_axis(invert_values, -1, adjusted_cost_array)\n    return inverted_cost_array\ndef invert_walking_speed(normalized_walking_speed_array):\n    \"\"\"\n    Inverts walking speed values and handles zeros/NaNs appropriately.\n    \"\"\"\n    inverted = np.zeros_like(normalized_walking_speed_array)\n    for i in range(8):\n        temp = normalized_walking_speed_array[i].copy()\n        temp[temp == 0] = np.nan\n        inverted[i] = np.where(np.isnan(temp), 1e6, 1.0 / temp)\n    return inverted\n</pre> def invert_cost_array(adjusted_cost_array):     \"\"\"     Inverts the values in the given cost array.     This function takes an array of adjusted costs and inverts each value.      If a value is zero, it is replaced with NaN before inversion to avoid      division by zero. After inversion, NaN values are replaced with a large      number (1e6).     Parameters:     adjusted_cost_array (numpy.ndarray): A numpy array of adjusted costs.     Returns:     numpy.ndarray: A numpy array with the inverted cost values.     \"\"\"        def invert_values(combined_value):         combined_value[combined_value == 0] = np.nan         inverted_values = np.where(np.isnan(combined_value), np.nan, 1.0 / combined_value)         inverted_values[np.isnan(inverted_values)] = 1e6         return inverted_values      inverted_cost_array = np.apply_along_axis(invert_values, -1, adjusted_cost_array)     return inverted_cost_array def invert_walking_speed(normalized_walking_speed_array):     \"\"\"     Inverts walking speed values and handles zeros/NaNs appropriately.     \"\"\"     inverted = np.zeros_like(normalized_walking_speed_array)     for i in range(8):         temp = normalized_walking_speed_array[i].copy()         temp[temp == 0] = np.nan         inverted[i] = np.where(np.isnan(temp), 1e6, 1.0 / temp)     return inverted In\u00a0[\u00a0]: Copied! <pre>def invert_cost_raster(updated_cost_raster):\n    \"\"\"\n    Inverts cost raster values and handles zeros/NaNs appropriately.\n    \"\"\"\n    inverted = updated_cost_raster.copy()\n    inverted[inverted == 0] = np.nan\n    return np.where(np.isnan(inverted), 1e6, 1.0 / inverted)\n</pre> def invert_cost_raster(updated_cost_raster):     \"\"\"     Inverts cost raster values and handles zeros/NaNs appropriately.     \"\"\"     inverted = updated_cost_raster.copy()     inverted[inverted == 0] = np.nan     return np.where(np.isnan(inverted), 1e6, 1.0 / inverted)"},{"location":"source-code/cost-calculation/data-loading.html","title":"data_loading.py","text":"<p>data_loading.py</p> In\u00a0[\u00a0]: Copied! <pre>import fiona\nimport geopandas as gpd\nimport rasterio\nimport numpy as np\nfrom affine import Affine\n</pre> import fiona import geopandas as gpd import rasterio import numpy as np from affine import Affine In\u00a0[\u00a0]: Copied! <pre>def read_shapefile(path):\n    \"\"\"\n    Load a shapefile and return it as a GeoDataFrame.\n    Parameters:\n    path (str): The file path to the shapefile.\n    Returns:\n    gpd.GeoDataFrame: A GeoDataFrame containing the data from the shapefile.\n    \"\"\"\n    \n    with fiona.open(path) as src:\n        return gpd.GeoDataFrame.from_features(src, crs=src.crs)\n</pre> def read_shapefile(path):     \"\"\"     Load a shapefile and return it as a GeoDataFrame.     Parameters:     path (str): The file path to the shapefile.     Returns:     gpd.GeoDataFrame: A GeoDataFrame containing the data from the shapefile.     \"\"\"          with fiona.open(path) as src:         return gpd.GeoDataFrame.from_features(src, crs=src.crs) In\u00a0[\u00a0]: Copied! <pre>def read_raster(path):\n    \"\"\"\n    Reads a raster file and returns its data, profile, transform, CRS, and nodata value.\n    Parameters:\n    path (str): The file path to the raster file.\n    Returns:\n    tuple: A tuple containing:\n        - data (numpy.ndarray): The raster data.\n        - profile (dict): The profile metadata of the raster.\n        - transform (affine.Affine): The affine transform of the raster.\n        - crs (rasterio.crs.CRS): The coordinate reference system of the raster.\n        - nodata (float or int): The nodata value of the raster.\n    \"\"\"\n    \n    with rasterio.open(path) as dataset:\n        data = dataset.read(1)\n        profile = dataset.profile\n        transform = dataset.transform\n        crs = dataset.crs\n        nodata = dataset.nodata\n    return data, profile, transform, crs, nodata\n</pre> def read_raster(path):     \"\"\"     Reads a raster file and returns its data, profile, transform, CRS, and nodata value.     Parameters:     path (str): The file path to the raster file.     Returns:     tuple: A tuple containing:         - data (numpy.ndarray): The raster data.         - profile (dict): The profile metadata of the raster.         - transform (affine.Affine): The affine transform of the raster.         - crs (rasterio.crs.CRS): The coordinate reference system of the raster.         - nodata (float or int): The nodata value of the raster.     \"\"\"          with rasterio.open(path) as dataset:         data = dataset.read(1)         profile = dataset.profile         transform = dataset.transform         crs = dataset.crs         nodata = dataset.nodata     return data, profile, transform, crs, nodata"},{"location":"source-code/cost-calculation/dem-processing.html","title":"dem_processing.py","text":"<p>dem_processing.py</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>def calculate_slope(dem_data, resolution_x, resolution_y, no_data):\n    \"\"\"\n    Calculates the slope in eight directions from a DEM.\n\n    Parameters:\n    dem_data (numpy.ndarray): 2D array representing the DEM data.\n    resolution_x (float): The resolution of the DEM in the x direction.\n    resolution_y (float): The resolution of the DEM in the y direction.\n    no_data (float): The value representing no data in the DEM.\n\n    Returns:\n    numpy.ndarray: A 3D array where the first dimension represents the eight directions \n    (North, South, East, West, North-East, North-West, South-East, South-West),\n    and the other two dimensions represent the slope values for each cell in the DEM.\n    \"\"\"\n    rows, cols = dem_data.shape\n    slope_array = np.full((8, rows, cols), np.nan)\n\n    for i in range(1, rows - 1):\n        for j in range(1, cols - 1):\n            if dem_data[i, j] == no_data:\n                continue\n\n            dz_n  = dem_data[i - 1, j] - dem_data[i, j]  # North\n            dz_s  = dem_data[i + 1, j] - dem_data[i, j]  # South\n            dz_e  = dem_data[i, j + 1] - dem_data[i, j]  # East\n            dz_w  = dem_data[i, j - 1] - dem_data[i, j]  # West\n            dz_ne = dem_data[i - 1, j + 1] - dem_data[i, j]  # North-East\n            dz_nw = dem_data[i - 1, j - 1] - dem_data[i, j]  # North-West\n            dz_se = dem_data[i + 1, j + 1] - dem_data[i, j]  # South-East\n            dz_sw = dem_data[i + 1, j - 1] - dem_data[i, j]  # South-West\n\n            d_h = resolution_x\n            d_d = np.sqrt(resolution_x**2 + resolution_y**2)\n\n            slope_array[0, i, j] = (dz_n / d_h)   # North\n            slope_array[1, i, j] = (dz_s / d_h)   # South\n            slope_array[2, i, j] = (dz_e / d_h)   # East\n            slope_array[3, i, j] = (dz_w / d_h)   # West\n            slope_array[4, i, j] = (dz_ne / d_d)  # North-East\n            slope_array[5, i, j] = (dz_nw / d_d)  # North-West\n            slope_array[6, i, j] = (dz_se / d_d)  # South-East\n            slope_array[7, i, j] = (dz_sw / d_d)  # South-West\n\n    return slope_array\n</pre> def calculate_slope(dem_data, resolution_x, resolution_y, no_data):     \"\"\"     Calculates the slope in eight directions from a DEM.      Parameters:     dem_data (numpy.ndarray): 2D array representing the DEM data.     resolution_x (float): The resolution of the DEM in the x direction.     resolution_y (float): The resolution of the DEM in the y direction.     no_data (float): The value representing no data in the DEM.      Returns:     numpy.ndarray: A 3D array where the first dimension represents the eight directions      (North, South, East, West, North-East, North-West, South-East, South-West),     and the other two dimensions represent the slope values for each cell in the DEM.     \"\"\"     rows, cols = dem_data.shape     slope_array = np.full((8, rows, cols), np.nan)      for i in range(1, rows - 1):         for j in range(1, cols - 1):             if dem_data[i, j] == no_data:                 continue              dz_n  = dem_data[i - 1, j] - dem_data[i, j]  # North             dz_s  = dem_data[i + 1, j] - dem_data[i, j]  # South             dz_e  = dem_data[i, j + 1] - dem_data[i, j]  # East             dz_w  = dem_data[i, j - 1] - dem_data[i, j]  # West             dz_ne = dem_data[i - 1, j + 1] - dem_data[i, j]  # North-East             dz_nw = dem_data[i - 1, j - 1] - dem_data[i, j]  # North-West             dz_se = dem_data[i + 1, j + 1] - dem_data[i, j]  # South-East             dz_sw = dem_data[i + 1, j - 1] - dem_data[i, j]  # South-West              d_h = resolution_x             d_d = np.sqrt(resolution_x**2 + resolution_y**2)              slope_array[0, i, j] = (dz_n / d_h)   # North             slope_array[1, i, j] = (dz_s / d_h)   # South             slope_array[2, i, j] = (dz_e / d_h)   # East             slope_array[3, i, j] = (dz_w / d_h)   # West             slope_array[4, i, j] = (dz_ne / d_d)  # North-East             slope_array[5, i, j] = (dz_nw / d_d)  # North-West             slope_array[6, i, j] = (dz_se / d_d)  # South-East             slope_array[7, i, j] = (dz_sw / d_d)  # South-West      return slope_array In\u00a0[\u00a0]: Copied! <pre>def calculate_walking_speed(slope_array):\n    \"\"\"\n    Calculates walking speed using Tobler's Hiking Function.\n    This function calculates the walking speed given an array of slope values.\n    The walking speed is computed using the formula:\n    speed = 6 * exp(-3.5 * abs(slope + 0.05))\n    Parameters:\n    slope_array (numpy.ndarray): An array of slope values.\n    Returns:\n    numpy.ndarray: An array of walking speeds corresponding to the input slope values.\n    \"\"\"\n    return 6 * np.exp(-3.5 * np.abs(slope_array + 0.05))\n</pre> def calculate_walking_speed(slope_array):     \"\"\"     Calculates walking speed using Tobler's Hiking Function.     This function calculates the walking speed given an array of slope values.     The walking speed is computed using the formula:     speed = 6 * exp(-3.5 * abs(slope + 0.05))     Parameters:     slope_array (numpy.ndarray): An array of slope values.     Returns:     numpy.ndarray: An array of walking speeds corresponding to the input slope values.     \"\"\"     return 6 * np.exp(-3.5 * np.abs(slope_array + 0.05)) In\u00a0[\u00a0]: Copied! <pre>def get_max_velocity():\n    \"\"\"\n    Calculates the maximum velocity based on Tobler's Hiking Function.\n    The maximum velocity occurs at a slope of zero.\n    Returns:\n    float: The maximum walking velocity.\n    \"\"\"\n    slope_zero = 0\n    return 6 * np.exp(-3.5 * np.abs(slope_zero + 0.05))\n</pre> def get_max_velocity():     \"\"\"     Calculates the maximum velocity based on Tobler's Hiking Function.     The maximum velocity occurs at a slope of zero.     Returns:     float: The maximum walking velocity.     \"\"\"     slope_zero = 0     return 6 * np.exp(-3.5 * np.abs(slope_zero + 0.05)) In\u00a0[\u00a0]: Copied! <pre>def normalize_walking_speed(walking_speed_array):\n    \"\"\"\n    Normalize the walking speed array by dividing each element by the maximum velocity.\n    The maximum velocity is dynamically calculated using Tobler's Hiking Function at a slope of zero.\n    Parameters:\n    walking_speed_array (numpy.ndarray): Array of walking speeds to be normalized.\n    Returns:\n    numpy.ndarray: The normalized walking speed array.\n    \"\"\"\n    max_velocity = get_max_velocity()\n    return walking_speed_array / max_velocity\n</pre> def normalize_walking_speed(walking_speed_array):     \"\"\"     Normalize the walking speed array by dividing each element by the maximum velocity.     The maximum velocity is dynamically calculated using Tobler's Hiking Function at a slope of zero.     Parameters:     walking_speed_array (numpy.ndarray): Array of walking speeds to be normalized.     Returns:     numpy.ndarray: The normalized walking speed array.     \"\"\"     max_velocity = get_max_velocity()     return walking_speed_array / max_velocity"},{"location":"source-code/cost-calculation/plotting-utils.html","title":"plotting_utils.py","text":"<p>plotting_utils.py</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport numpy.ma as ma\nimport rasterio\nfrom rasterio.plot import plotting_extent\nfrom typing import Any\nimport geopandas as gpd\nfrom matplotlib.colors import Normalize, ListedColormap, LinearSegmentedColormap\nfrom matplotlib.colors import TwoSlopeNorm\n</pre> import matplotlib.pyplot as plt import matplotlib.patches as mpatches import numpy as np import numpy.ma as ma import rasterio from rasterio.plot import plotting_extent from typing import Any import geopandas as gpd from matplotlib.colors import Normalize, ListedColormap, LinearSegmentedColormap from matplotlib.colors import TwoSlopeNorm In\u00a0[\u00a0]: Copied! <pre>def plot_continuous_raster_with_points(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, colorbar_label: str, output_file_path: str) -&gt; None:\n    \"\"\"\n    Plots a continuous raster with overlaid points.\n\n    Parameters:\n    raster_data (np.ndarray): The raster data to be plotted.\n    extent (Any): The extent of the raster data.\n    points_gdf (gpd.GeoDataFrame): GeoDataFrame containing the points to be overlaid.\n    title (str): The title of the plot.\n    colorbar_label (str): The label for the colorbar.\n    output_file_path (str): The file path where the plot will be saved.\n\n    Returns:\n    None\n    \"\"\"\n    # A4 width is 210mm = 8.27 inches\n    a4_width = 8.27\n    \n    # Create figure with two subplots in 1 row, 2 columns with A4 width\n    # Add extra width to accommodate the colorbar\n    fig, axes = plt.subplots(1, 2, figsize=(a4_width + 1.5, a4_width/2))\n    \n    # If we're plotting a single raster, simply use the first axis and hide the second\n    if not isinstance(raster_data, list):\n        # Display the raster on the first subplot\n        cax = axes[0].imshow(raster_data, extent=extent, cmap='viridis', interpolation='none')\n        axes[0].set_title(title, fontsize=10, fontweight='bold')\n        axes[0].scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")\n        axes[0].legend()\n        \n        # Hide the second subplot\n        axes[1].axis('off')\n        \n        # Add colorbar\n        cbar = fig.colorbar(cax, ax=axes[0], fraction=0.046, pad=0.04)\n        cbar.set_label(colorbar_label)\n    else:\n        # For comparison plots with two rasters\n        # Find common color scale if needed\n        if len(raster_data) == 2:\n            vmin = min(np.nanmin(raster_data[0]), np.nanmin(raster_data[1]))\n            vmax = max(np.nanmax(raster_data[0]), np.nanmax(raster_data[1]))\n            \n            # If we have a list of titles, use them\n            titles = title if isinstance(title, list) and len(title) == 2 else [f\"{title} (1)\", f\"{title} (2)\"]\n            \n            # Determine the global extent to use for both plots\n            if isinstance(extent, list):\n                # If different extents provided, find the union\n                global_extent = [\n                    min(extent[0][0], extent[1][0]),  # xmin\n                    max(extent[0][1], extent[1][1]),  # xmax\n                    min(extent[0][2], extent[1][2]),  # ymin\n                    max(extent[0][3], extent[1][3])   # ymax\n                ]\n            else:\n                # Use the same extent for both\n                global_extent = extent\n            \n            # Plot first raster\n            cax1 = axes[0].imshow(raster_data[0], extent=global_extent, \n                               cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)\n            axes[0].set_title(titles[0], fontsize=12, fontweight='bold')\n            pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[0]\n            axes[0].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")\n            axes[0].legend()\n            \n            # Plot second raster\n            cax2 = axes[1].imshow(raster_data[1], extent=global_extent, \n                               cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)\n            axes[1].set_title(titles[1], fontsize=12, fontweight='bold')\n            pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[1]\n            axes[1].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")\n            axes[1].legend()\n            \n            # Add colorbar on the right side of the second subplot\n            # Use specific positioning to ensure it's outside the plot\n            cbar = fig.colorbar(cax2, ax=axes[1], pad=0.05)\n            cbar.set_label(colorbar_label, fontsize=10)\n            \n            # Make sure both plots have the same aspect ratio\n            for ax in axes:\n                ax.set_aspect('equal')\n                \n                # Improve x-axis tick formatting\n                ax.ticklabel_format(style='plain', axis='x')\n                ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n                \n                # Improve y-axis tick formatting\n                ax.ticklabel_format(style='plain', axis='y')\n                \n                # Adjust tick label font size\n                ax.tick_params(axis='both', labelsize=9)\n                \n                # Ensure there's enough space for tick labels\n                plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n    \n    # Adjust spacing to make room for x-axis labels\n    plt.tight_layout()\n    \n    # Save and show the plot\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_continuous_raster_with_points(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, colorbar_label: str, output_file_path: str) -&gt; None:     \"\"\"     Plots a continuous raster with overlaid points.      Parameters:     raster_data (np.ndarray): The raster data to be plotted.     extent (Any): The extent of the raster data.     points_gdf (gpd.GeoDataFrame): GeoDataFrame containing the points to be overlaid.     title (str): The title of the plot.     colorbar_label (str): The label for the colorbar.     output_file_path (str): The file path where the plot will be saved.      Returns:     None     \"\"\"     # A4 width is 210mm = 8.27 inches     a4_width = 8.27          # Create figure with two subplots in 1 row, 2 columns with A4 width     # Add extra width to accommodate the colorbar     fig, axes = plt.subplots(1, 2, figsize=(a4_width + 1.5, a4_width/2))          # If we're plotting a single raster, simply use the first axis and hide the second     if not isinstance(raster_data, list):         # Display the raster on the first subplot         cax = axes[0].imshow(raster_data, extent=extent, cmap='viridis', interpolation='none')         axes[0].set_title(title, fontsize=10, fontweight='bold')         axes[0].scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")         axes[0].legend()                  # Hide the second subplot         axes[1].axis('off')                  # Add colorbar         cbar = fig.colorbar(cax, ax=axes[0], fraction=0.046, pad=0.04)         cbar.set_label(colorbar_label)     else:         # For comparison plots with two rasters         # Find common color scale if needed         if len(raster_data) == 2:             vmin = min(np.nanmin(raster_data[0]), np.nanmin(raster_data[1]))             vmax = max(np.nanmax(raster_data[0]), np.nanmax(raster_data[1]))                          # If we have a list of titles, use them             titles = title if isinstance(title, list) and len(title) == 2 else [f\"{title} (1)\", f\"{title} (2)\"]                          # Determine the global extent to use for both plots             if isinstance(extent, list):                 # If different extents provided, find the union                 global_extent = [                     min(extent[0][0], extent[1][0]),  # xmin                     max(extent[0][1], extent[1][1]),  # xmax                     min(extent[0][2], extent[1][2]),  # ymin                     max(extent[0][3], extent[1][3])   # ymax                 ]             else:                 # Use the same extent for both                 global_extent = extent                          # Plot first raster             cax1 = axes[0].imshow(raster_data[0], extent=global_extent,                                 cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)             axes[0].set_title(titles[0], fontsize=12, fontweight='bold')             pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[0]             axes[0].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")             axes[0].legend()                          # Plot second raster             cax2 = axes[1].imshow(raster_data[1], extent=global_extent,                                 cmap='viridis', interpolation='none', vmin=vmin, vmax=vmax)             axes[1].set_title(titles[1], fontsize=12, fontweight='bold')             pt_gdf = points_gdf if not isinstance(points_gdf, list) else points_gdf[1]             axes[1].scatter(pt_gdf.geometry.x, pt_gdf.geometry.y, color=\"red\", marker=\"^\", s=50, label=\"Summit\")             axes[1].legend()                          # Add colorbar on the right side of the second subplot             # Use specific positioning to ensure it's outside the plot             cbar = fig.colorbar(cax2, ax=axes[1], pad=0.05)             cbar.set_label(colorbar_label, fontsize=10)                          # Make sure both plots have the same aspect ratio             for ax in axes:                 ax.set_aspect('equal')                                  # Improve x-axis tick formatting                 ax.ticklabel_format(style='plain', axis='x')                 ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))                                  # Improve y-axis tick formatting                 ax.ticklabel_format(style='plain', axis='y')                                  # Adjust tick label font size                 ax.tick_params(axis='both', labelsize=9)                                  # Ensure there's enough space for tick labels                 plt.setp(ax.get_xticklabels(), rotation=45, ha='right')          # Adjust spacing to make room for x-axis labels     plt.tight_layout()          # Save and show the plot     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_normalized_walking_speed(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, output_file_path: str) -&gt; None:\n\n       \n    \"\"\"\n    Plots the normalized walking speed raster with summit points.\n     Parameters:\n        raster_data (np.ndarray): The raster data representing normalized walking speeds.\n        extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).\n        points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.\n        title (str): The title of the plot.\n        output_file_path (str): The file path where the plot will be saved.\n\n        Returns:\n        None\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import ListedColormap\n    import numpy.ma as ma\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    cmap = plt.cm.viridis\n    masked_raster_data = ma.masked_invalid(raster_data)\n    cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')\n    ax.set_facecolor(\"white\")\n    ax.set_title(title, fontsize=10, fontweight='bold')\n    cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    cbar.set_label(\"Speed Conservation Values of Slope\")\n    ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n    ax.legend(loc='upper right')\n    plt.tight_layout()\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_normalized_walking_speed(raster_data: np.ndarray, extent: Any, points_gdf: gpd.GeoDataFrame, title: str, output_file_path: str) -&gt; None:              \"\"\"     Plots the normalized walking speed raster with summit points.      Parameters:         raster_data (np.ndarray): The raster data representing normalized walking speeds.         extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).         points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.         title (str): The title of the plot.         output_file_path (str): The file path where the plot will be saved.          Returns:         None     \"\"\"     import matplotlib.pyplot as plt     from matplotlib.colors import ListedColormap     import numpy.ma as ma      fig, ax = plt.subplots(figsize=(10, 8))     cmap = plt.cm.viridis     masked_raster_data = ma.masked_invalid(raster_data)     cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')     ax.set_facecolor(\"white\")     ax.set_title(title, fontsize=10, fontweight='bold')     cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)     cbar.set_label(\"Speed Conservation Values of Slope\")     ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)     ax.legend(loc='upper right')     plt.tight_layout()     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_adjusted_cost_raster(adjusted_cost_raster, extent, points_gdf, title, output_file_path):\n\n        \n    \"\"\"\n    Plots the adjusted cost raster with summit points.\n    Parameters:\n        adjusted_cost_raster (numpy.ndarray): A 2D array representing the adjusted cost raster data.\n        extent (list or tuple): The bounding box in the form [xmin, xmax, ymin, ymax] for the raster.\n        points_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing the summit points with geometry column.\n        title (str): The title of the plot.\n        output_file_path (str): The file path where the plot image will be saved.\n\n        Returns:\n        None\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.colors import Normalize\n    import numpy.ma as ma\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    cmap = plt.cm.Blues\n    masked_raster_data = ma.masked_invalid(adjusted_cost_raster)\n    cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')\n    ax.set_facecolor(\"white\")\n    ax.set_title(title, fontsize=10, fontweight='bold')\n    cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    cbar.set_label(\"Walking Speed (m/s)\")\n    ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n    ax.legend(loc='upper right')\n    plt.tight_layout()\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_adjusted_cost_raster(adjusted_cost_raster, extent, points_gdf, title, output_file_path):               \"\"\"     Plots the adjusted cost raster with summit points.     Parameters:         adjusted_cost_raster (numpy.ndarray): A 2D array representing the adjusted cost raster data.         extent (list or tuple): The bounding box in the form [xmin, xmax, ymin, ymax] for the raster.         points_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing the summit points with geometry column.         title (str): The title of the plot.         output_file_path (str): The file path where the plot image will be saved.          Returns:         None     \"\"\"     import matplotlib.pyplot as plt     from matplotlib.colors import Normalize     import numpy.ma as ma      fig, ax = plt.subplots(figsize=(10, 8))     cmap = plt.cm.Blues     masked_raster_data = ma.masked_invalid(adjusted_cost_raster)     cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, interpolation='none')     ax.set_facecolor(\"white\")     ax.set_title(title, fontsize=10, fontweight='bold')     cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)     cbar.set_label(\"Walking Speed (m/s)\")     ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)     ax.legend(loc='upper right')     plt.tight_layout()     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_inverted_cost_raster(inverted_cost_raster, extent, points_gdf, title, output_file_path):\n\n     \n    \"\"\"\n    Plots the inverted cost raster with summit points.\n       Parameters:\n        inverted_cost_raster (numpy.ndarray): 2D array representing the inverted cost raster.\n        extent (list or tuple): The bounding box in data coordinates (left, right, bottom, top).\n        points_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the summit points with geometry column.\n        title (str): Title of the plot.\n        output_file_path (str): Path to save the output plot image.\n\n        Returns:\n        None\n    \"\"\"\n\n\n    # Create masks\n    mask_nan = np.isnan(inverted_cost_raster)\n    mask_special = (inverted_cost_raster == 1e6)\n    mask_continuous = (~mask_nan &amp; ~mask_special)\n\n    # Create masked arrays\n    continuous_data = ma.masked_where(~mask_continuous, inverted_cost_raster)\n    special_data = ma.masked_where(~mask_special, inverted_cost_raster)\n\n    # Determine normalization range\n    clean_data = continuous_data[~mask_nan &amp; ~mask_special]\n    vmin, vmax = np.percentile(clean_data, [2, 98])\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    cax = ax.imshow(continuous_data, extent=extent, cmap='Blues', norm=Normalize(vmin=vmin, vmax=vmax))\n    ax.imshow(special_data, extent=extent, cmap=ListedColormap(['blue', 'none']), alpha=0.7)\n    ax.set_facecolor(\"white\")\n    ax.set_title(title, fontsize=16, fontweight='bold')\n    cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n    cbar.set_label('Cost (excluding 1e6)', fontsize=12)\n    ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n    ax.legend(loc='upper right')\n    plt.tight_layout()\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_inverted_cost_raster(inverted_cost_raster, extent, points_gdf, title, output_file_path):            \"\"\"     Plots the inverted cost raster with summit points.        Parameters:         inverted_cost_raster (numpy.ndarray): 2D array representing the inverted cost raster.         extent (list or tuple): The bounding box in data coordinates (left, right, bottom, top).         points_gdf (geopandas.GeoDataFrame): GeoDataFrame containing the summit points with geometry column.         title (str): Title of the plot.         output_file_path (str): Path to save the output plot image.          Returns:         None     \"\"\"       # Create masks     mask_nan = np.isnan(inverted_cost_raster)     mask_special = (inverted_cost_raster == 1e6)     mask_continuous = (~mask_nan &amp; ~mask_special)      # Create masked arrays     continuous_data = ma.masked_where(~mask_continuous, inverted_cost_raster)     special_data = ma.masked_where(~mask_special, inverted_cost_raster)      # Determine normalization range     clean_data = continuous_data[~mask_nan &amp; ~mask_special]     vmin, vmax = np.percentile(clean_data, [2, 98])      fig, ax = plt.subplots(figsize=(10, 8))     cax = ax.imshow(continuous_data, extent=extent, cmap='Blues', norm=Normalize(vmin=vmin, vmax=vmax))     ax.imshow(special_data, extent=extent, cmap=ListedColormap(['blue', 'none']), alpha=0.7)     ax.set_facecolor(\"white\")     ax.set_title(title, fontsize=16, fontweight='bold')     cbar = plt.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)     cbar.set_label('Cost (excluding 1e6)', fontsize=12)     ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)     ax.legend(loc='upper right')     plt.tight_layout()     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_walking_speed_vs_slope(slope_array, walking_speed_array, directions, output_file_path):\n    \"\"\"\n    Plots the walking speed versus slope for all eight directions in a subplot layout.\n\n    Parameters:\n    slope_array (numpy.ndarray): A 3D array representing slope values for eight directions.\n                                  The first dimension represents directions: North, South, East, West,\n                                  North-East, North-West, South-East, South-West.\n    walking_speed_array (numpy.ndarray): A 3D array representing walking speed values for eight directions.\n                                         The first dimension represents directions: North, South, East, West,\n                                         North-East, North-West, South-East, South-West.\n    directions (list): A list of direction labels.\n    output_file_path (str): The file path where the combined plot will be saved.\n\n    Returns:\n    None\n    \"\"\"\n    fig, axes = plt.subplots(4, 2, figsize=(15, 20))\n    fig.suptitle(\"Walking Speed vs Slope for All 8 Directions\", fontsize=16)\n\n    for direction in range(8):\n        # Flatten the slope and walking speed arrays for the current direction\n        slope_values = slope_array[direction].flatten()\n        walking_speed_values = walking_speed_array[direction].flatten()\n\n        # Filter out NaN values from both slope and walking speed arrays\n        valid_mask = ~np.isnan(slope_values) &amp; ~np.isnan(walking_speed_values)\n        slope_values = slope_values[valid_mask]\n        walking_speed_values = walking_speed_values[valid_mask]\n\n        # Get the corresponding axis for the current direction\n        ax = axes[direction // 2, direction % 2]\n        \n        # Create a scatter plot of walking speed vs. slope\n        ax.scatter(slope_values, walking_speed_values, alpha=0.5, s=1)\n        ax.set_title(f\"{directions[direction]} Direction\")\n        ax.set_xlabel(\"Slope\")\n        ax.set_ylabel(\"Walking Speed (m/s)\")\n\n    # Adjust layout to prevent overlapping\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.savefig(output_file_path, dpi=300)\n    plt.show()\n    plt.close(fig)\n</pre> def plot_walking_speed_vs_slope(slope_array, walking_speed_array, directions, output_file_path):     \"\"\"     Plots the walking speed versus slope for all eight directions in a subplot layout.      Parameters:     slope_array (numpy.ndarray): A 3D array representing slope values for eight directions.                                   The first dimension represents directions: North, South, East, West,                                   North-East, North-West, South-East, South-West.     walking_speed_array (numpy.ndarray): A 3D array representing walking speed values for eight directions.                                          The first dimension represents directions: North, South, East, West,                                          North-East, North-West, South-East, South-West.     directions (list): A list of direction labels.     output_file_path (str): The file path where the combined plot will be saved.      Returns:     None     \"\"\"     fig, axes = plt.subplots(4, 2, figsize=(15, 20))     fig.suptitle(\"Walking Speed vs Slope for All 8 Directions\", fontsize=16)      for direction in range(8):         # Flatten the slope and walking speed arrays for the current direction         slope_values = slope_array[direction].flatten()         walking_speed_values = walking_speed_array[direction].flatten()          # Filter out NaN values from both slope and walking speed arrays         valid_mask = ~np.isnan(slope_values) &amp; ~np.isnan(walking_speed_values)         slope_values = slope_values[valid_mask]         walking_speed_values = walking_speed_values[valid_mask]          # Get the corresponding axis for the current direction         ax = axes[direction // 2, direction % 2]                  # Create a scatter plot of walking speed vs. slope         ax.scatter(slope_values, walking_speed_values, alpha=0.5, s=1)         ax.set_title(f\"{directions[direction]} Direction\")         ax.set_xlabel(\"Slope\")         ax.set_ylabel(\"Walking Speed (m/s)\")      # Adjust layout to prevent overlapping     plt.tight_layout(rect=[0, 0.03, 1, 0.95])     plt.savefig(output_file_path, dpi=300)     plt.show()     plt.close(fig) In\u00a0[\u00a0]: Copied! <pre>def plot_north_east_speed_conservation(normalized_walking_speed_array, extent, points_gdf, title, output_file_path):\n    \"\"\"\n    Plots the normalized walking speed raster for North and East directions in a subplot.\n    \n    Parameters:\n        normalized_walking_speed_array (np.ndarray): The 3D array of normalized walking speeds where\n                                                    first dimension represents directions \n                                                    (North: 0, East: 2)\n        extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).\n        points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.\n        title (str): The main title of the plot.\n        output_file_path (str): The file path where the plot will be saved.\n        \n    Returns:\n        None\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy.ma as ma\n    \n    # Create a figure with two subplots\n    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n    fig.suptitle(title, fontsize=14, fontweight='bold')\n    \n    # Direction labels\n    directions = ['North', 'East']\n    indices = [0, 2]  # North is at index 0, East is at index 2 in the array\n    \n    # Create a common color scale for both plots\n    vmin = np.nanmin([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])\n    vmax = np.nanmax([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])\n    cmap = plt.cm.viridis\n    \n    # Loop through the two directions\n    for i, (ax, direction, idx) in enumerate(zip(axes, directions, indices)):\n        # Get the data for this direction\n        raster_data = normalized_walking_speed_array[idx]\n        \n        # Mask invalid data\n        masked_raster_data = ma.masked_invalid(raster_data)\n        \n        # Plot the data\n        cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap, \n                        interpolation='none', vmin=vmin, vmax=vmax)\n        \n        # Set background color for NaN values\n        ax.set_facecolor(\"white\")\n        \n        # Add title for this subplot\n        ax.set_title(f\"{direction} Direction\", fontsize=12, fontweight='bold')\n        \n        # Add summit points\n        ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y, \n                  color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)\n        \n        # Add legend\n        ax.legend(loc='upper right')\n    \n    # Add a colorbar that applies to both subplots\n    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n    cbar = plt.colorbar(cax, cax=cbar_ax)\n    cbar.set_label(\"Speed Conservation Values of Slope\")\n    \n    # Adjust layout\n    plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Adjust layout to give space for colorbar\n    \n    # Save the figure\n    plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n    plt.close(fig)\n</pre> def plot_north_east_speed_conservation(normalized_walking_speed_array, extent, points_gdf, title, output_file_path):     \"\"\"     Plots the normalized walking speed raster for North and East directions in a subplot.          Parameters:         normalized_walking_speed_array (np.ndarray): The 3D array of normalized walking speeds where                                                     first dimension represents directions                                                      (North: 0, East: 2)         extent (Any): The extent of the raster data in the format (xmin, xmax, ymin, ymax).         points_gdf (gpd.GeoDataFrame): A GeoDataFrame containing the summit points to be plotted.         title (str): The main title of the plot.         output_file_path (str): The file path where the plot will be saved.              Returns:         None     \"\"\"     import matplotlib.pyplot as plt     import numpy.ma as ma          # Create a figure with two subplots     fig, axes = plt.subplots(1, 2, figsize=(16, 8))     fig.suptitle(title, fontsize=14, fontweight='bold')          # Direction labels     directions = ['North', 'East']     indices = [0, 2]  # North is at index 0, East is at index 2 in the array          # Create a common color scale for both plots     vmin = np.nanmin([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])     vmax = np.nanmax([normalized_walking_speed_array[0], normalized_walking_speed_array[2]])     cmap = plt.cm.viridis          # Loop through the two directions     for i, (ax, direction, idx) in enumerate(zip(axes, directions, indices)):         # Get the data for this direction         raster_data = normalized_walking_speed_array[idx]                  # Mask invalid data         masked_raster_data = ma.masked_invalid(raster_data)                  # Plot the data         cax = ax.imshow(masked_raster_data, extent=extent, cmap=cmap,                          interpolation='none', vmin=vmin, vmax=vmax)                  # Set background color for NaN values         ax.set_facecolor(\"white\")                  # Add title for this subplot         ax.set_title(f\"{direction} Direction\", fontsize=12, fontweight='bold')                  # Add summit points         ax.scatter(points_gdf.geometry.x, points_gdf.geometry.y,                    color=\"red\", marker=\"*\", s=85, label=\"Summit\", zorder=5)                  # Add legend         ax.legend(loc='upper right')          # Add a colorbar that applies to both subplots     cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]     cbar = plt.colorbar(cax, cax=cbar_ax)     cbar.set_label(\"Speed Conservation Values of Slope\")          # Adjust layout     plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Adjust layout to give space for colorbar          # Save the figure     plt.savefig(output_file_path, format=\"jpg\", dpi=300, bbox_inches=\"tight\")     plt.show()     plt.close(fig)"},{"location":"source-code/evacuation-analysis/analysis.html","title":"analysis.py","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport time\nimport numpy as np\nimport rasterio\nfrom scipy.sparse.csgraph import dijkstra\nfrom grid_utils import process_raster, to_1d\nfrom io_utils import save_raster, load_raster\n</pre> import os import time import numpy as np import rasterio from scipy.sparse.csgraph import dijkstra from grid_utils import process_raster, to_1d from io_utils import save_raster, load_raster In\u00a0[\u00a0]: Copied! <pre>def run_dijkstra_analysis(graph_csr, source_nodes):\n    \"\"\"\n    Run Dijkstra's algorithm for all source nodes.\n    \n    This function computes the shortest paths from a set of source nodes to all other nodes\n    in a given graph using Dijkstra's algorithm. The graph is represented in Compressed Sparse\n    Row (CSR) format.\n    \n    Parameters:\n    -----------\n    graph_csr (scipy.sparse.csr_matrix): The graph in CSR format where each entry represents\n                                        the weight of the edge between nodes.\n    \n    source_nodes (array-like): A list or array of source node indices from which to calculate\n                              the shortest paths.\n    \n    Returns:\n    --------\n    distances (numpy.ndarray): A 2D array where distances[i, j] represents the shortest distance\n                              from source_nodes[i] to node j.\n    \n    predecessors (numpy.ndarray): A 2D array where predecessors[i, j] represents the predecessor\n                                 node on the shortest path from source_nodes[i] to node j.\n                                 A value of -9999 indicates no path exists.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The function uses scipy's implementation of Dijkstra's algorithm.\n    - The graph is treated as directed.\n    \"\"\"\n    print(\"\\nRunning Dijkstra's algorithm...\")\n    start_time = time.time()\n    distances, predecessors = dijkstra(csgraph=graph_csr, directed=True,\n                                      indices=source_nodes, return_predecessors=True)\n    end_time = time.time()\n    print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds\")\n    return distances, predecessors\n</pre> def run_dijkstra_analysis(graph_csr, source_nodes):     \"\"\"     Run Dijkstra's algorithm for all source nodes.          This function computes the shortest paths from a set of source nodes to all other nodes     in a given graph using Dijkstra's algorithm. The graph is represented in Compressed Sparse     Row (CSR) format.          Parameters:     -----------     graph_csr (scipy.sparse.csr_matrix): The graph in CSR format where each entry represents                                         the weight of the edge between nodes.          source_nodes (array-like): A list or array of source node indices from which to calculate                               the shortest paths.          Returns:     --------     distances (numpy.ndarray): A 2D array where distances[i, j] represents the shortest distance                               from source_nodes[i] to node j.          predecessors (numpy.ndarray): A 2D array where predecessors[i, j] represents the predecessor                                  node on the shortest path from source_nodes[i] to node j.                                  A value of -9999 indicates no path exists.          Notes:     ------     - Progress and timing information is printed to standard output.     - The function uses scipy's implementation of Dijkstra's algorithm.     - The graph is treated as directed.     \"\"\"     print(\"\\nRunning Dijkstra's algorithm...\")     start_time = time.time()     distances, predecessors = dijkstra(csgraph=graph_csr, directed=True,                                       indices=source_nodes, return_predecessors=True)     end_time = time.time()     print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds\")     return distances, predecessors In\u00a0[\u00a0]: Copied! <pre>def process_travel_times(base_path, source_name, dataset_key, speed_name, speed_value, meta):\n    \"\"\"\n    Process and save travel time rasters for a given speed value.\n    \n    This function loads a base raster, processes it to calculate travel times \n    based on the specified speed, and saves the resulting raster to disk.\n    \n    Parameters:\n    -----------\n    base_path (str): The file path to the base raster.\n    \n    source_name (str): The name of the source for which travel times are being processed.\n    \n    dataset_key (str): A key identifying the dataset, used in the output filename.\n    \n    speed_name (str): The name of the speed category (e.g., 'slow', 'moderate', 'fast').\n    \n    speed_value (float): The speed value to be used in processing the raster.\n    \n    meta (dict): Metadata for the output raster file.\n    \n    Returns:\n    --------\n    numpy.ndarray: The processed travel time array.\n    \n    Raises:\n    -------\n    FileNotFoundError: If the base raster file does not exist.\n    \n    ValueError: If there is an issue with the raster data or processing.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - Output file is saved as 'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n      in the same directory as the input file.\n    - Output raster uses float32 data type with -1 as the nodata value.\n    \"\"\"\n    print(f\"Processing travel times for {source_name} at {speed_name} speed...\")\n    \n    cost_array_base, cost_meta_base = load_raster(base_path)\n    travel_time_array = process_raster(cost_array_base, speed_value)\n    \n    out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n    out_path = os.path.join(os.path.dirname(base_path), out_filename)\n    save_raster(out_path, travel_time_array, meta, dtype=rasterio.float32, nodata=-1)\n    \n    return travel_time_array\n</pre> def process_travel_times(base_path, source_name, dataset_key, speed_name, speed_value, meta):     \"\"\"     Process and save travel time rasters for a given speed value.          This function loads a base raster, processes it to calculate travel times      based on the specified speed, and saves the resulting raster to disk.          Parameters:     -----------     base_path (str): The file path to the base raster.          source_name (str): The name of the source for which travel times are being processed.          dataset_key (str): A key identifying the dataset, used in the output filename.          speed_name (str): The name of the speed category (e.g., 'slow', 'moderate', 'fast').          speed_value (float): The speed value to be used in processing the raster.          meta (dict): Metadata for the output raster file.          Returns:     --------     numpy.ndarray: The processed travel time array.          Raises:     -------     FileNotFoundError: If the base raster file does not exist.          ValueError: If there is an issue with the raster data or processing.          Notes:     ------     - Progress and timing information is printed to standard output.     - Output file is saved as 'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'       in the same directory as the input file.     - Output raster uses float32 data type with -1 as the nodata value.     \"\"\"     print(f\"Processing travel times for {source_name} at {speed_name} speed...\")          cost_array_base, cost_meta_base = load_raster(base_path)     travel_time_array = process_raster(cost_array_base, speed_value)          out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'     out_path = os.path.join(os.path.dirname(base_path), out_filename)     save_raster(out_path, travel_time_array, meta, dtype=rasterio.float32, nodata=-1)          return travel_time_array In\u00a0[\u00a0]: Copied! <pre>def analyze_safe_zones(distance_from_summit, travel_time_data, safe_zone_distances, source_names):\n    \"\"\"\n    Analyze minimum travel times within specified safe zones from different source locations.\n    \n    Parameters:\n    -----------\n    distance_from_summit (numpy.ndarray): A 2D array representing the distance from the summit.\n    \n    travel_time_data (dict): A dictionary containing travel time data for different speeds and sources.\n                             The structure is expected to be:\n                             {\n                                 'speed_name': {\n                                     'source_name': {\n                                         'cost_array_flat': numpy.ndarray,  # Flattened travel times\n                                         'cost_array': numpy.ndarray        # 2D travel times\n                                     }\n                                 }\n                             }\n    \n    safe_zone_distances (list): A list of distances defining the safe zones in meters.\n    \n    source_names (list): A list of source names corresponding to the travel time data.\n    \n    Returns:\n    --------\n    tuple: A tuple containing two dictionaries:\n           - results: A dictionary with the minimum travel times (in hours) for each speed and safe zone.\n                      The structure is:\n                      {\n                          'speed_name': {\n                              safe_zone_distance: [min_time1, min_time2, ...]\n                          }\n                      }\n                      where the list indices correspond to the order of source_names.\n           \n           - min_coords: A dictionary with the coordinates of the minimum travel times for each speed and safe zone.\n                         The structure is:\n                         {\n                             'speed_name': {\n                                 safe_zone_distance: [(min_r1, min_c1), (min_r2, min_c2), ...]\n                             }\n                         }\n                         where each tuple contains the row and column indices in the original 2D array.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output during execution.\n    - Safe zones are defined as areas where distance_from_summit &gt;= safe_zone_distance.\n    - If no valid times are found within a safe zone, np.nan is used for both time and coordinates.\n    - All travel times are in hours.\n    \"\"\"\n    print(\"\\nAnalyzing safe zones...\")\n    \n    results = {}\n    min_coords = {}\n    \n    for speed_name in travel_time_data.keys():\n        print(f\"\\nProcessing {speed_name} speed...\")\n        results[speed_name] = {}\n        min_coords[speed_name] = {}\n        distance_from_summit_1d = distance_from_summit.ravel()\n        \n        for safe_zone in safe_zone_distances:\n            safe_zone_mask = (distance_from_summit_1d &gt;= safe_zone)\n            min_times_in_safe_zone = []\n            coords_in_safe_zone = []\n            \n            for source_name in source_names:\n                cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']\n                valid_times = cost_array_flat[safe_zone_mask]\n                valid_times = valid_times[~np.isnan(valid_times)]\n                \n                if len(valid_times) &gt; 0:\n                    min_time = np.min(valid_times)\n                    cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']\n                    safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)\n                    valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)\n                    min_idx = np.nanargmin(valid_times_2d)\n                    min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)\n                else:\n                    min_time = np.nan\n                    min_r, min_c = (np.nan, np.nan)\n                    \n                min_times_in_safe_zone.append(min_time)\n                coords_in_safe_zone.append((min_r, min_c))\n                print(f\"{source_name} at {safe_zone}m: min time = {min_time:.2f} hrs\")\n                \n            results[speed_name][safe_zone] = min_times_in_safe_zone\n            min_coords[speed_name][safe_zone] = coords_in_safe_zone\n            \n    return results, min_coords\n</pre> def analyze_safe_zones(distance_from_summit, travel_time_data, safe_zone_distances, source_names):     \"\"\"     Analyze minimum travel times within specified safe zones from different source locations.          Parameters:     -----------     distance_from_summit (numpy.ndarray): A 2D array representing the distance from the summit.          travel_time_data (dict): A dictionary containing travel time data for different speeds and sources.                              The structure is expected to be:                              {                                  'speed_name': {                                      'source_name': {                                          'cost_array_flat': numpy.ndarray,  # Flattened travel times                                          'cost_array': numpy.ndarray        # 2D travel times                                      }                                  }                              }          safe_zone_distances (list): A list of distances defining the safe zones in meters.          source_names (list): A list of source names corresponding to the travel time data.          Returns:     --------     tuple: A tuple containing two dictionaries:            - results: A dictionary with the minimum travel times (in hours) for each speed and safe zone.                       The structure is:                       {                           'speed_name': {                               safe_zone_distance: [min_time1, min_time2, ...]                           }                       }                       where the list indices correspond to the order of source_names.                        - min_coords: A dictionary with the coordinates of the minimum travel times for each speed and safe zone.                          The structure is:                          {                              'speed_name': {                                  safe_zone_distance: [(min_r1, min_c1), (min_r2, min_c2), ...]                              }                          }                          where each tuple contains the row and column indices in the original 2D array.          Notes:     ------     - Progress and timing information is printed to standard output during execution.     - Safe zones are defined as areas where distance_from_summit &gt;= safe_zone_distance.     - If no valid times are found within a safe zone, np.nan is used for both time and coordinates.     - All travel times are in hours.     \"\"\"     print(\"\\nAnalyzing safe zones...\")          results = {}     min_coords = {}          for speed_name in travel_time_data.keys():         print(f\"\\nProcessing {speed_name} speed...\")         results[speed_name] = {}         min_coords[speed_name] = {}         distance_from_summit_1d = distance_from_summit.ravel()                  for safe_zone in safe_zone_distances:             safe_zone_mask = (distance_from_summit_1d &gt;= safe_zone)             min_times_in_safe_zone = []             coords_in_safe_zone = []                          for source_name in source_names:                 cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']                 valid_times = cost_array_flat[safe_zone_mask]                 valid_times = valid_times[~np.isnan(valid_times)]                                  if len(valid_times) &gt; 0:                     min_time = np.min(valid_times)                     cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']                     safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)                     valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)                     min_idx = np.nanargmin(valid_times_2d)                     min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)                 else:                     min_time = np.nan                     min_r, min_c = (np.nan, np.nan)                                      min_times_in_safe_zone.append(min_time)                 coords_in_safe_zone.append((min_r, min_c))                 print(f\"{source_name} at {safe_zone}m: min time = {min_time:.2f} hrs\")                              results[speed_name][safe_zone] = min_times_in_safe_zone             min_coords[speed_name][safe_zone] = coords_in_safe_zone                  return results, min_coords"},{"location":"source-code/evacuation-analysis/config.html","title":"config.py","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nConfiguration settings for volcanic evacuation analysis.\n\nThis module establishes environment variables, defines walking speeds,\nand specifies file paths for various geospatial datasets used in the \nvolcanic evacuation analysis for Mt. Marapi.\n\nConstants:\n----------\nWALKING_SPEEDS : dict\n   Dictionary mapping speed categories to values in meters per second:\n   - 'slow': 0.91 m/s\n   - 'medium': 1.22 m/s\n   - 'fast': 1.52 m/s\n\nDATA_FOLDER : str\n   Base directory for python results.\n\nCOST_PATHS : dict\n   Dictionary mapping cost raster names to their file paths:\n   - 'final': Main cost raster with original hiking path\n   - 'modify_landcover': Cost raster with modified land cover\n   - 'walking_speed': Walking speed cost raster\n   - 'base_cost': Base cost raster\n\nSUMMIT_PATH : str\n   File path to the Mt. Marapi summit location (GPKG format).\n\nCAMP_SPOT_PATH : str\n   File path to camping locations (GPKG format).\n\nHIKING_PATH : str\n   File path to the original hiking path (GPKG format).\n\nLANDCOVER_100M : str\n   File path to land cover data with 100m buffer (TIF format).\n\nDEM_100M : str\n   File path to digital elevation model with 100m buffer (TIF format).\n\nSOURCE_NAMES : list\n   List of source location names for evacuation analysis:\n   ['summit', 'camp1', 'camp2', 'camp3', 'camp4']\n\nSAFE_ZONE_DISTANCES : list\n   List of safe zone distances from 500m to 4500m in 500m increments.\n\nDIRECTIONS : list\n   List of tuples representing the eight possible movement directions\n   for path analysis (cardinal and intercardinal directions).\n\nEnvironment Variables:\n---------------------\nUSE_PATH_FOR_GDAL_PYTHON : str\n   Set to \"True\" to use the system path for GDAL Python bindings.\n\nPROJ_LIB : str\n   Set to pyproj's data directory for projection information.\n\"\"\"\n</pre> \"\"\" Configuration settings for volcanic evacuation analysis.  This module establishes environment variables, defines walking speeds, and specifies file paths for various geospatial datasets used in the  volcanic evacuation analysis for Mt. Marapi.  Constants: ---------- WALKING_SPEEDS : dict    Dictionary mapping speed categories to values in meters per second:    - 'slow': 0.91 m/s    - 'medium': 1.22 m/s    - 'fast': 1.52 m/s  DATA_FOLDER : str    Base directory for python results.  COST_PATHS : dict    Dictionary mapping cost raster names to their file paths:    - 'final': Main cost raster with original hiking path    - 'modify_landcover': Cost raster with modified land cover    - 'walking_speed': Walking speed cost raster    - 'base_cost': Base cost raster  SUMMIT_PATH : str    File path to the Mt. Marapi summit location (GPKG format).  CAMP_SPOT_PATH : str    File path to camping locations (GPKG format).  HIKING_PATH : str    File path to the original hiking path (GPKG format).  LANDCOVER_100M : str    File path to land cover data with 100m buffer (TIF format).  DEM_100M : str    File path to digital elevation model with 100m buffer (TIF format).  SOURCE_NAMES : list    List of source location names for evacuation analysis:    ['summit', 'camp1', 'camp2', 'camp3', 'camp4']  SAFE_ZONE_DISTANCES : list    List of safe zone distances from 500m to 4500m in 500m increments.  DIRECTIONS : list    List of tuples representing the eight possible movement directions    for path analysis (cardinal and intercardinal directions).  Environment Variables: --------------------- USE_PATH_FOR_GDAL_PYTHON : str    Set to \"True\" to use the system path for GDAL Python bindings.  PROJ_LIB : str    Set to pyproj's data directory for projection information. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport pyproj\n# Set environment variables\nos.environ['USE_PATH_FOR_GDAL_PYTHON'] = \"True\"\nos.environ['PROJ_LIB'] = pyproj.datadir.get_data_dir()\n# Walking speeds (m/s)\nWALKING_SPEEDS = {\n   'slow': 0.91,\n   'medium': 1.22,\n   'fast': 1.52\n}\n# Define your paths directly\nDATA_FOLDER = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results'\n# Cost raster datasets\n# Modified to include 4 datasets instead of 2.\n# Cost raster datasets\nCOST_PATHS = {\n    'final': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_original.tif'),\n    'modify_landcover': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_modified.tif'),\n    'walking_speed': os.path.join(DATA_FOLDER, 'inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif'),\n    'base_cost': os.path.join(DATA_FOLDER, 'invert_landcovercost_original_Awu.tif')\n}\n# Input paths\nSUMMIT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\"\nCAMP_SPOT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\"\nHIKING_PATH = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp'\nlandcover_100m = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_LandCover_2019_100m_Buffer_UTM.tif'\nDEM_100m = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_DEM_100m_Buffer_UTM.tif\"\n# Source names\nSOURCE_NAMES = ['summit', 'camp1']\n# Safe zone distances\nSAFE_ZONE_DISTANCES = list(range(500, 5000, 500))\n# Movement directions\nDIRECTIONS = [\n   (-1, 0),   # Up\n   (1, 0),    # Down\n   (0, 1),    # Right\n   (0, -1),   # Left\n   (-1, 1),   # Up-Right\n   (-1, -1),  # Up-Left\n   (1, 1),    # Down-Right\n   (1, -1)    # Down-Left\n]\n</pre> import os import pyproj # Set environment variables os.environ['USE_PATH_FOR_GDAL_PYTHON'] = \"True\" os.environ['PROJ_LIB'] = pyproj.datadir.get_data_dir() # Walking speeds (m/s) WALKING_SPEEDS = {    'slow': 0.91,    'medium': 1.22,    'fast': 1.52 } # Define your paths directly DATA_FOLDER = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\python_results' # Cost raster datasets # Modified to include 4 datasets instead of 2. # Cost raster datasets COST_PATHS = {     'final': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_original.tif'),     'modify_landcover': os.path.join(DATA_FOLDER, 'Awu_inverted_cost_8_directions_modified.tif'),     'walking_speed': os.path.join(DATA_FOLDER, 'inverted_Slopecost_8_directions_Awu_OriginalLandcover.tif'),     'base_cost': os.path.join(DATA_FOLDER, 'invert_landcovercost_original_Awu.tif') } # Input paths SUMMIT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\summit_awu_correct_proj_final.shp\" CAMP_SPOT_PATH = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\campspot_awu_correct_proj_final.shp\" HIKING_PATH = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\projection\\hikingpath_awu_buffer_correctproj_final.shp' landcover_100m = r'C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_LandCover_2019_100m_Buffer_UTM.tif' DEM_100m = r\"C:\\Users\\Mojan\\Desktop\\RA Volcano\\Dataset\\Awu_DEM_100m_Buffer_UTM.tif\" # Source names SOURCE_NAMES = ['summit', 'camp1'] # Safe zone distances SAFE_ZONE_DISTANCES = list(range(500, 5000, 500)) # Movement directions DIRECTIONS = [    (-1, 0),   # Up    (1, 0),    # Down    (0, 1),    # Right    (0, -1),   # Left    (-1, 1),   # Up-Right    (-1, -1),  # Up-Left    (1, 1),    # Down-Right    (1, -1)    # Down-Left ]"},{"location":"source-code/evacuation-analysis/decomposition.html","title":"decomposition.py","text":"<p>decomposition.py</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom config import COST_PATHS, DIRECTIONS\nfrom io_utils import read_raster\nfrom grid_utils import to_1d\n</pre> import numpy as np from config import COST_PATHS, DIRECTIONS from io_utils import read_raster from grid_utils import to_1d In\u00a0[\u00a0]: Copied! <pre>def reconstruct_path(predecessors, target, source):\n    \"\"\"\n    Reconstruct the shortest path from source to target using the predecessor array.\n    \n    This function traces back through a predecessor array produced by a shortest path\n    algorithm (like Dijkstra's) to reconstruct the complete path from source to target.\n    \n    Parameters:\n    -----------\n    predecessors : numpy.ndarray\n        A 1D array where predecessors[i] contains the predecessor node of node i\n        on the shortest path from the source. This array should be for a single\n        source node.\n    \n    target : int\n        The target node index (destination) for which to reconstruct the path.\n    \n    source : int\n        The source node index (starting point) from which the path begins.\n    \n    Returns:\n    --------\n    path : list\n        A list of node indices representing the shortest path from source to target,\n        inclusive of both endpoints. The path is ordered from source to target.\n    \n    Notes:\n    ------\n    - The function assumes that node indices are represented as integers.\n    - A special value (-9999) in the predecessor array indicates \"no predecessor\".\n    - If there is no valid path from source to target, the returned path will\n      only contain the target node.\n    - The path includes both the source and target nodes.\n    \"\"\"\n    path = []\n    node = target\n    # Use a special value (-9999) to indicate \"no predecessor\"\n    while node != source and node != -9999:\n        path.append(node)\n        node = predecessors[node]\n    path.append(source)\n    path.reverse()\n    return path\n</pre> def reconstruct_path(predecessors, target, source):     \"\"\"     Reconstruct the shortest path from source to target using the predecessor array.          This function traces back through a predecessor array produced by a shortest path     algorithm (like Dijkstra's) to reconstruct the complete path from source to target.          Parameters:     -----------     predecessors : numpy.ndarray         A 1D array where predecessors[i] contains the predecessor node of node i         on the shortest path from the source. This array should be for a single         source node.          target : int         The target node index (destination) for which to reconstruct the path.          source : int         The source node index (starting point) from which the path begins.          Returns:     --------     path : list         A list of node indices representing the shortest path from source to target,         inclusive of both endpoints. The path is ordered from source to target.          Notes:     ------     - The function assumes that node indices are represented as integers.     - A special value (-9999) in the predecessor array indicates \"no predecessor\".     - If there is no valid path from source to target, the returned path will       only contain the target node.     - The path includes both the source and target nodes.     \"\"\"     path = []     node = target     # Use a special value (-9999) to indicate \"no predecessor\"     while node != source and node != -9999:         path.append(node)         node = predecessors[node]     path.append(source)     path.reverse()     return path In\u00a0[\u00a0]: Copied! <pre>def expand_single_band(cost_array):\n    \"\"\"\n    Expand a single-band raster into an 8-band array representing different movement directions.\n    \n    This function takes a single-band cost raster and expands it into 8 bands corresponding to\n    the 8 movement directions (4 cardinal and 4 diagonal). The diagonal directions (bands 4-7)\n    are multiplied by sqrt(2) to account for the increased distance when moving diagonally.\n    \n    Parameters:\n    -----------\n    cost_array : numpy.ndarray\n        Input cost raster with shape (1, rows, cols), where the first dimension\n        represents a single band.\n    \n    Returns:\n    --------\n    expanded : numpy.ndarray\n        An 8-band raster with shape (8, rows, cols), where each band represents\n        the cost in one of the 8 movement directions. Bands 0-3 represent cardinal\n        directions, while bands 4-7 represent diagonal directions with costs\n        multiplied by sqrt(2).\n    \n    Notes:\n    ------\n    - The function assumes that the input cost_array has shape (1, rows, cols).\n    - The cardinal directions are ordered as: Up, Down, Right, Left\n    - The diagonal directions are ordered as: Up-Right, Up-Left, Down-Right, Down-Left\n    - The returned array preserves the data type of the input array.\n    \"\"\"\n    single_band = cost_array[0]\n    rows, cols = single_band.shape\n    expanded = np.zeros((8, rows, cols), dtype=single_band.dtype)\n    for i in range(8):\n        expanded[i] = single_band.copy()\n        if i &gt;= 4:  # Diagonal directions\n            expanded[i] *= np.sqrt(2)\n    return expanded\n</pre> def expand_single_band(cost_array):     \"\"\"     Expand a single-band raster into an 8-band array representing different movement directions.          This function takes a single-band cost raster and expands it into 8 bands corresponding to     the 8 movement directions (4 cardinal and 4 diagonal). The diagonal directions (bands 4-7)     are multiplied by sqrt(2) to account for the increased distance when moving diagonally.          Parameters:     -----------     cost_array : numpy.ndarray         Input cost raster with shape (1, rows, cols), where the first dimension         represents a single band.          Returns:     --------     expanded : numpy.ndarray         An 8-band raster with shape (8, rows, cols), where each band represents         the cost in one of the 8 movement directions. Bands 0-3 represent cardinal         directions, while bands 4-7 represent diagonal directions with costs         multiplied by sqrt(2).          Notes:     ------     - The function assumes that the input cost_array has shape (1, rows, cols).     - The cardinal directions are ordered as: Up, Down, Right, Left     - The diagonal directions are ordered as: Up-Right, Up-Left, Down-Right, Down-Left     - The returned array preserves the data type of the input array.     \"\"\"     single_band = cost_array[0]     rows, cols = single_band.shape     expanded = np.zeros((8, rows, cols), dtype=single_band.dtype)     for i in range(8):         expanded[i] = single_band.copy()         if i &gt;= 4:  # Diagonal directions             expanded[i] *= np.sqrt(2)     return expanded In\u00a0[\u00a0]: Copied! <pre>def run_decomposition_analysis(dataset_info, min_coords_all):\n    \"\"\"\n    Perform a decomposition analysis to quantify the relative contributions of slope and land cover\n    to the optimal evacuation paths from the summit.\n    \n    This function analyzes the shortest paths identified for different safe zone thresholds and\n    calculates the percentage contribution of slope (walking speed) and land cover factors to\n    the overall path cost. The analysis uses three cost rasters: the final combined cost,\n    the walking speed (slope) cost, and the base (land cover) cost.\n    \n    Parameters:\n    -----------\n    dataset_info : dict\n        Dictionary containing information about the datasets, including:\n        - 'final': dict containing:\n            - 'pred_summit': ndarray - Predecessor array for the summit source\n            - 'cols': int - Number of columns in the raster\n            - 'summit_raster_coords': tuple or list - Coordinates of the summit in the raster\n    \n    min_coords_all : dict\n        Nested dictionary containing the minimum travel time coordinates for each dataset,\n        speed, and safe zone distance:\n        {\n            dataset_key: {\n                speed_name: {\n                    safe_zone_distance: [(row, col), ...]\n                }\n            }\n        }\n    \n    Returns:\n    --------\n    list of dict\n        A list of dictionaries, each representing a row in the results table with keys:\n        - \"Safe Zone Threshold (m)\": int - The safe zone distance threshold\n        - \"Slope Contribution (%)\": float - Percentage contribution of slope to the path cost\n        - \"Landcover Contribution (%)\": float - Percentage contribution of land cover to the path cost\n    \n    Notes:\n    ------\n    - The function assumes that the summit is at index 0 in the source list.\n    - Contributions are calculated using the logarithmic sum of costs along the path.\n    - The 'medium' speed category is used for analysis.\n    - Safe zone distances from 500m to 4500m in 500m increments are analyzed.\n    - Paths with missing or invalid cost values are skipped.\n    \"\"\"\n    print(\"\\n=== Running Decomposition Analysis ===\")\n    \n    # 1. Read final cost raster\n    final_array, meta_final, transform_final, nodata_final, bounds_final, res_final = read_raster(COST_PATHS['final'])\n    if final_array.shape[0] == 1:\n        print(\"Expanding single-band final raster to 8 directions...\")\n        final_array = expand_single_band(final_array)\n\n    # 2. Read walking speed (slope) cost raster\n    walking_speed_array, meta_ws, transform_ws, nodata_ws, bounds_ws, res_ws = read_raster(COST_PATHS['walking_speed'])\n    if walking_speed_array.shape[0] == 1:\n        print(\"Expanding single-band walking_speed raster to 8 directions...\")\n        walking_speed_array = expand_single_band(walking_speed_array)\n\n    # 3. Read base cost (landcover) raster\n    base_cost_array, meta_base, transform_base, nodata_base, bounds_base, res_base = read_raster(COST_PATHS['base_cost'])\n    if base_cost_array.shape[0] == 1:\n        print(\"Expanding single-band base_cost raster to 8 directions...\")\n        base_cost_array = expand_single_band(base_cost_array)\n\n    # 4. Retrieve the predecessor array from the final dataset (summit source index 0)\n    pred_final = dataset_info['final']['pred_summit']\n    cols = dataset_info['final']['cols']\n\n    # 5. Retrieve the summit raster coords\n    if isinstance(dataset_info['final']['summit_raster_coords'], tuple):\n        summit_idx = dataset_info['final']['summit_raster_coords']\n    else:\n        summit_idx = dataset_info['final']['summit_raster_coords'][0]\n    summit_node = to_1d(summit_idx[0], summit_idx[1], cols)\n\n    # We assume the same safe zone thresholds used previously\n    safe_zone_distances = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]\n\n    # For the final dataset, we pick a walking speed from min_coords_all, e.g. 'medium'\n    selected_speed = 'medium'\n    print(f\"Decomposition analysis for 'final' dataset, source: summit, speed: {selected_speed}\")\n\n    # Prepare a list for table rows\n    table_data = []\n\n    # Create a lookup for directions =&gt; band index\n    direction_indices = {d: i for i, d in enumerate(DIRECTIONS)}\n\n    # 6. Loop over each safe zone threshold, reconstruct path, compute contributions\n    for safe_zone in safe_zone_distances:\n        # Summits are at source index 0 in your code\n        safe_zone_coord = min_coords_all['final'][selected_speed][safe_zone][0]  # (row, col)\n        # Skip if no valid coordinate\n        if np.isnan(safe_zone_coord[0]) or np.isnan(safe_zone_coord[1]):\n            print(f\"Safe zone {safe_zone} m: No valid pixel found.\")\n            continue\n\n        target_node = to_1d(int(safe_zone_coord[0]), int(safe_zone_coord[1]), cols)\n        path_nodes = reconstruct_path(pred_final, target_node, summit_node)\n\n        sum_log_slope = 0.0\n        sum_log_land = 0.0\n        valid_path = True\n\n        for i in range(len(path_nodes) - 1):\n            current = path_nodes[i]\n            nxt = path_nodes[i+1]\n            r, c = divmod(current, cols)\n            nr, nc = divmod(nxt, cols)\n            dr = nr - r\n            dc = nc - c\n            direction = (dr, dc)\n            if direction not in direction_indices:\n                print(f\"Unexpected movement direction {direction} from {current} to {nxt}\")\n                valid_path = False\n                break\n\n            band = direction_indices[direction]\n            slope_val = walking_speed_array[band, r, c]\n            land_val = base_cost_array[band, r, c]\n            # Check positivity for log\n            if slope_val &lt;= 0 or land_val &lt;= 0:\n                print(f\"Invalid cost at {r},{c}, slope={slope_val}, land={land_val}\")\n                valid_path = False\n                break\n\n            sum_log_slope += np.log(slope_val)\n            sum_log_land += np.log(land_val)\n\n        if not valid_path:\n            print(f\"Safe zone {safe_zone} m: path issue, skipping.\")\n            continue\n\n        total_log = sum_log_slope + sum_log_land\n        perc_slope = (sum_log_slope / total_log) * 100\n        perc_land = (sum_log_land / total_log) * 100\n\n        # Store one row for the table\n        row_dict = {\n            \"Safe Zone Threshold (m)\": safe_zone,\n            \"Slope Contribution (%)\": round(perc_slope, 2),\n            \"Landcover Contribution (%)\": round(perc_land, 2)\n        }\n        table_data.append(row_dict)\n\n    return table_data\n</pre> def run_decomposition_analysis(dataset_info, min_coords_all):     \"\"\"     Perform a decomposition analysis to quantify the relative contributions of slope and land cover     to the optimal evacuation paths from the summit.          This function analyzes the shortest paths identified for different safe zone thresholds and     calculates the percentage contribution of slope (walking speed) and land cover factors to     the overall path cost. The analysis uses three cost rasters: the final combined cost,     the walking speed (slope) cost, and the base (land cover) cost.          Parameters:     -----------     dataset_info : dict         Dictionary containing information about the datasets, including:         - 'final': dict containing:             - 'pred_summit': ndarray - Predecessor array for the summit source             - 'cols': int - Number of columns in the raster             - 'summit_raster_coords': tuple or list - Coordinates of the summit in the raster          min_coords_all : dict         Nested dictionary containing the minimum travel time coordinates for each dataset,         speed, and safe zone distance:         {             dataset_key: {                 speed_name: {                     safe_zone_distance: [(row, col), ...]                 }             }         }          Returns:     --------     list of dict         A list of dictionaries, each representing a row in the results table with keys:         - \"Safe Zone Threshold (m)\": int - The safe zone distance threshold         - \"Slope Contribution (%)\": float - Percentage contribution of slope to the path cost         - \"Landcover Contribution (%)\": float - Percentage contribution of land cover to the path cost          Notes:     ------     - The function assumes that the summit is at index 0 in the source list.     - Contributions are calculated using the logarithmic sum of costs along the path.     - The 'medium' speed category is used for analysis.     - Safe zone distances from 500m to 4500m in 500m increments are analyzed.     - Paths with missing or invalid cost values are skipped.     \"\"\"     print(\"\\n=== Running Decomposition Analysis ===\")          # 1. Read final cost raster     final_array, meta_final, transform_final, nodata_final, bounds_final, res_final = read_raster(COST_PATHS['final'])     if final_array.shape[0] == 1:         print(\"Expanding single-band final raster to 8 directions...\")         final_array = expand_single_band(final_array)      # 2. Read walking speed (slope) cost raster     walking_speed_array, meta_ws, transform_ws, nodata_ws, bounds_ws, res_ws = read_raster(COST_PATHS['walking_speed'])     if walking_speed_array.shape[0] == 1:         print(\"Expanding single-band walking_speed raster to 8 directions...\")         walking_speed_array = expand_single_band(walking_speed_array)      # 3. Read base cost (landcover) raster     base_cost_array, meta_base, transform_base, nodata_base, bounds_base, res_base = read_raster(COST_PATHS['base_cost'])     if base_cost_array.shape[0] == 1:         print(\"Expanding single-band base_cost raster to 8 directions...\")         base_cost_array = expand_single_band(base_cost_array)      # 4. Retrieve the predecessor array from the final dataset (summit source index 0)     pred_final = dataset_info['final']['pred_summit']     cols = dataset_info['final']['cols']      # 5. Retrieve the summit raster coords     if isinstance(dataset_info['final']['summit_raster_coords'], tuple):         summit_idx = dataset_info['final']['summit_raster_coords']     else:         summit_idx = dataset_info['final']['summit_raster_coords'][0]     summit_node = to_1d(summit_idx[0], summit_idx[1], cols)      # We assume the same safe zone thresholds used previously     safe_zone_distances = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500]      # For the final dataset, we pick a walking speed from min_coords_all, e.g. 'medium'     selected_speed = 'medium'     print(f\"Decomposition analysis for 'final' dataset, source: summit, speed: {selected_speed}\")      # Prepare a list for table rows     table_data = []      # Create a lookup for directions =&gt; band index     direction_indices = {d: i for i, d in enumerate(DIRECTIONS)}      # 6. Loop over each safe zone threshold, reconstruct path, compute contributions     for safe_zone in safe_zone_distances:         # Summits are at source index 0 in your code         safe_zone_coord = min_coords_all['final'][selected_speed][safe_zone][0]  # (row, col)         # Skip if no valid coordinate         if np.isnan(safe_zone_coord[0]) or np.isnan(safe_zone_coord[1]):             print(f\"Safe zone {safe_zone} m: No valid pixel found.\")             continue          target_node = to_1d(int(safe_zone_coord[0]), int(safe_zone_coord[1]), cols)         path_nodes = reconstruct_path(pred_final, target_node, summit_node)          sum_log_slope = 0.0         sum_log_land = 0.0         valid_path = True          for i in range(len(path_nodes) - 1):             current = path_nodes[i]             nxt = path_nodes[i+1]             r, c = divmod(current, cols)             nr, nc = divmod(nxt, cols)             dr = nr - r             dc = nc - c             direction = (dr, dc)             if direction not in direction_indices:                 print(f\"Unexpected movement direction {direction} from {current} to {nxt}\")                 valid_path = False                 break              band = direction_indices[direction]             slope_val = walking_speed_array[band, r, c]             land_val = base_cost_array[band, r, c]             # Check positivity for log             if slope_val &lt;= 0 or land_val &lt;= 0:                 print(f\"Invalid cost at {r},{c}, slope={slope_val}, land={land_val}\")                 valid_path = False                 break              sum_log_slope += np.log(slope_val)             sum_log_land += np.log(land_val)          if not valid_path:             print(f\"Safe zone {safe_zone} m: path issue, skipping.\")             continue          total_log = sum_log_slope + sum_log_land         perc_slope = (sum_log_slope / total_log) * 100         perc_land = (sum_log_land / total_log) * 100          # Store one row for the table         row_dict = {             \"Safe Zone Threshold (m)\": safe_zone,             \"Slope Contribution (%)\": round(perc_slope, 2),             \"Landcover Contribution (%)\": round(perc_land, 2)         }         table_data.append(row_dict)      return table_data"},{"location":"source-code/evacuation-analysis/grid-utils.html","title":"grid_utils.py","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport time\nfrom rasterio.transform import xy\n</pre> import numpy as np import time from rasterio.transform import xy In\u00a0[\u00a0]: Copied! <pre>def coords_to_raster(gdf, transform, bounds, res):\n    \"\"\"\n    Convert geographic coordinates to raster row and column indices.\n    \n    This function takes a GeoDataFrame containing point geometries and converts\n    the geographic coordinates (x, y) of each point to raster indices (row, col),\n    based on the provided raster transform, bounds, and resolution.\n    \n    Parameters:\n    -----------\n    gdf : geopandas.GeoDataFrame\n        A GeoDataFrame containing point geometries to be converted to raster indices.\n    \n    transform : affine.Affine\n        The affine transform of the raster, defining the relationship between\n        pixel coordinates and geographic coordinates.\n    \n    bounds : rasterio.coords.BoundingBox\n        The geographic bounds of the raster (left, bottom, right, top).\n    \n    res : tuple\n        A tuple (x_res, y_res) containing the pixel resolution in the x and y\n        directions (typically in map units per pixel).\n    \n    Returns:\n    --------\n    raster_coords : list\n        A list of tuples (row, col) representing the raster indices corresponding\n        to each point in the input GeoDataFrame. Points that fall outside the\n        raster bounds are excluded from the returned list.\n    \n    Notes:\n    ------\n    - Points falling outside the raster bounds are reported but not included in the output.\n    - The function assumes that the CRS of the GeoDataFrame matches the CRS of the raster.\n    - Progress and timing information is printed to standard output.\n    - Row indices increase from top to bottom, and column indices increase from left to right.\n    \"\"\"\n    print(\"Converting coordinates to raster indices...\")\n    raster_coords = []\n    for point in gdf.geometry:\n        x, y = point.x, point.y\n        if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):\n            print(f\"Point {x}, {y} is out of raster bounds.\")\n            continue\n        col = int((x - bounds.left) / res[0])\n        row = int((bounds.top - y) / res[1])\n        raster_coords.append((row, col))\n    return raster_coords\n</pre> def coords_to_raster(gdf, transform, bounds, res):     \"\"\"     Convert geographic coordinates to raster row and column indices.          This function takes a GeoDataFrame containing point geometries and converts     the geographic coordinates (x, y) of each point to raster indices (row, col),     based on the provided raster transform, bounds, and resolution.          Parameters:     -----------     gdf : geopandas.GeoDataFrame         A GeoDataFrame containing point geometries to be converted to raster indices.          transform : affine.Affine         The affine transform of the raster, defining the relationship between         pixel coordinates and geographic coordinates.          bounds : rasterio.coords.BoundingBox         The geographic bounds of the raster (left, bottom, right, top).          res : tuple         A tuple (x_res, y_res) containing the pixel resolution in the x and y         directions (typically in map units per pixel).          Returns:     --------     raster_coords : list         A list of tuples (row, col) representing the raster indices corresponding         to each point in the input GeoDataFrame. Points that fall outside the         raster bounds are excluded from the returned list.          Notes:     ------     - Points falling outside the raster bounds are reported but not included in the output.     - The function assumes that the CRS of the GeoDataFrame matches the CRS of the raster.     - Progress and timing information is printed to standard output.     - Row indices increase from top to bottom, and column indices increase from left to right.     \"\"\"     print(\"Converting coordinates to raster indices...\")     raster_coords = []     for point in gdf.geometry:         x, y = point.x, point.y         if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):             print(f\"Point {x}, {y} is out of raster bounds.\")             continue         col = int((x - bounds.left) / res[0])         row = int((bounds.top - y) / res[1])         raster_coords.append((row, col))     return raster_coords In\u00a0[\u00a0]: Copied! <pre>def to_1d(r, c, cols):\n    \"\"\"\n    Convert 2D raster coordinates (row, column) to a 1D array index.\n    \n    This function maps 2D coordinates in a raster or grid to the corresponding\n    1D array index, assuming row-major order (C-style).\n    \n    Parameters:\n    -----------\n    r : int\n        The row index (zero-based).\n    \n    c : int\n        The column index (zero-based).\n    \n    cols : int\n        The total number of columns in the 2D array or grid.\n    \n    Returns:\n    --------\n    int\n        The 1D array index corresponding to the given 2D coordinates.\n    \n    Notes:\n    ------\n    - The function assumes zero-based indexing for both input and output.\n    - The function assumes row-major order (C-style), where elements within a row\n      are stored contiguously.\n    - No bounds checking is performed; it's the caller's responsibility to ensure\n      r and c are valid indices.\n    \n    Examples:\n    ---------\n    &gt;&gt;&gt; to_1d(2, 3, 5)\n    13\n    # In a 5-column grid, the position at row 2, column 3 maps to index 13\n    \"\"\"\n    return r * cols + c\n</pre> def to_1d(r, c, cols):     \"\"\"     Convert 2D raster coordinates (row, column) to a 1D array index.          This function maps 2D coordinates in a raster or grid to the corresponding     1D array index, assuming row-major order (C-style).          Parameters:     -----------     r : int         The row index (zero-based).          c : int         The column index (zero-based).          cols : int         The total number of columns in the 2D array or grid.          Returns:     --------     int         The 1D array index corresponding to the given 2D coordinates.          Notes:     ------     - The function assumes zero-based indexing for both input and output.     - The function assumes row-major order (C-style), where elements within a row       are stored contiguously.     - No bounds checking is performed; it's the caller's responsibility to ensure       r and c are valid indices.          Examples:     ---------     &gt;&gt;&gt; to_1d(2, 3, 5)     13     # In a 5-column grid, the position at row 2, column 3 maps to index 13     \"\"\"     return r * cols + c In\u00a0[\u00a0]: Copied! <pre>def raster_coord_to_map_coords(row, col, transform):\n    \"\"\"\n    Convert raster coordinates (row, column) to geographic map coordinates (x, y).\n    \n    This function transforms pixel coordinates in a raster to their corresponding\n    geographic coordinates using the raster's affine transform.\n    \n    Parameters:\n    -----------\n    row : int\n        The row index in the raster (zero-based).\n    \n    col : int\n        The column index in the raster (zero-based).\n    \n    transform : affine.Affine\n        The affine transform of the raster, defining the relationship between\n        pixel coordinates and geographic coordinates.\n    \n    Returns:\n    --------\n    tuple\n        A tuple (x, y) containing the geographic coordinates corresponding to\n        the center of the specified raster cell.\n    \n    Notes:\n    ------\n    - The function uses the 'xy' function (likely from rasterio.transform) to perform\n      the coordinate transformation.\n    - By default, the coordinates returned correspond to the center of the pixel.\n    - The coordinate reference system (CRS) of the returned coordinates matches\n      the CRS of the input transform.\n    \"\"\"\n    x, y = xy(transform, row, col, offset='center')\n    return x, y\n</pre> def raster_coord_to_map_coords(row, col, transform):     \"\"\"     Convert raster coordinates (row, column) to geographic map coordinates (x, y).          This function transforms pixel coordinates in a raster to their corresponding     geographic coordinates using the raster's affine transform.          Parameters:     -----------     row : int         The row index in the raster (zero-based).          col : int         The column index in the raster (zero-based).          transform : affine.Affine         The affine transform of the raster, defining the relationship between         pixel coordinates and geographic coordinates.          Returns:     --------     tuple         A tuple (x, y) containing the geographic coordinates corresponding to         the center of the specified raster cell.          Notes:     ------     - The function uses the 'xy' function (likely from rasterio.transform) to perform       the coordinate transformation.     - By default, the coordinates returned correspond to the center of the pixel.     - The coordinate reference system (CRS) of the returned coordinates matches       the CRS of the input transform.     \"\"\"     x, y = xy(transform, row, col, offset='center')     return x, y In\u00a0[\u00a0]: Copied! <pre>def process_raster(cost_array, walking_speed):\n    \"\"\"\n    Convert base cost raster to travel time in hours.\n    \n    This function transforms a cost raster into a travel time raster by applying\n    the following steps:\n    1. Multiply by cell size (100m) to convert to distance units\n    2. Divide by walking speed to convert to time in seconds\n    3. Convert seconds to hours\n    4. Replace infinite values with -1 (indicating inaccessible areas)\n    \n    Parameters:\n    -----------\n    cost_array : numpy.ndarray\n        Input cost raster array. Values typically represent the inverse of the\n        travel cost or difficulty of traversing each cell.\n    \n    walking_speed : float\n        Walking speed in meters per second (m/s). Used to convert distance to time.\n    \n    Returns:\n    --------\n    numpy.ndarray\n        Array of travel times in hours. The array has the same shape as the input\n        cost_array, with infinite values replaced by -1.\n    \n    Notes:\n    ------\n    - The function assumes a fixed cell size of 100 meters.\n    - Timing information is printed to standard output.\n    - Infinite values in the output (resulting from division by zero or from input)\n      are replaced with -1 to represent inaccessible areas.\n    - The returned array maintains the same shape and data type as the input array.\n    \"\"\"\n    cost_array = cost_array * 100   # multiply by cell size (100 m)\n    cost_array = cost_array / walking_speed  # convert to seconds (m / (m/s) = s)\n    cost_array = cost_array / 3600  # convert seconds to hours\n    cost_array[np.isinf(cost_array)] = -1\n    return cost_array\n</pre> def process_raster(cost_array, walking_speed):     \"\"\"     Convert base cost raster to travel time in hours.          This function transforms a cost raster into a travel time raster by applying     the following steps:     1. Multiply by cell size (100m) to convert to distance units     2. Divide by walking speed to convert to time in seconds     3. Convert seconds to hours     4. Replace infinite values with -1 (indicating inaccessible areas)          Parameters:     -----------     cost_array : numpy.ndarray         Input cost raster array. Values typically represent the inverse of the         travel cost or difficulty of traversing each cell.          walking_speed : float         Walking speed in meters per second (m/s). Used to convert distance to time.          Returns:     --------     numpy.ndarray         Array of travel times in hours. The array has the same shape as the input         cost_array, with infinite values replaced by -1.          Notes:     ------     - The function assumes a fixed cell size of 100 meters.     - Timing information is printed to standard output.     - Infinite values in the output (resulting from division by zero or from input)       are replaced with -1 to represent inaccessible areas.     - The returned array maintains the same shape and data type as the input array.     \"\"\"     cost_array = cost_array * 100   # multiply by cell size (100 m)     cost_array = cost_array / walking_speed  # convert to seconds (m / (m/s) = s)     cost_array = cost_array / 3600  # convert seconds to hours     cost_array[np.isinf(cost_array)] = -1     return cost_array In\u00a0[\u00a0]: Copied! <pre>def calculate_distance_from_summit(summit_coords, rows, cols, cell_size=100):\n    \"\"\"\n    Compute Euclidean distance (in meters) from the summit for each cell in a raster.\n    \n    This function creates a distance raster where each cell contains the straight-line\n    distance from that cell to the summit location. The distance is calculated using\n    the Euclidean formula and converted to meters using the specified cell size.\n    \n    Parameters:\n    -----------\n    summit_coords : tuple\n        A tuple (row, col) containing the raster coordinates of the summit.\n    \n    rows : int\n        The number of rows in the output raster.\n    \n    cols : int\n        The number of columns in the output raster.\n    \n    cell_size : float, optional\n        The cell size in meters. Default is 100 meters.\n    \n    Returns:\n    --------\n    numpy.ndarray\n        A 2D array of shape (rows, cols) containing the Euclidean distance from\n        each cell to the summit in meters.\n    \n    Notes:\n    ------\n    - The function assumes a regular grid with square cells of uniform size.\n    - The output array is of type numpy.float32.\n    - The calculation is performed using raster coordinates, with the result scaled\n      by the cell size to get distances in meters.\n    - Progress and timing information is printed to standard output.\n    - The computation uses a nested loop approach which may be slow for large rasters.\n      Consider using vectorized operations for better performance.\n    \"\"\"\n    print(\"Calculating distance from summit...\")\n    summit_row, summit_col = summit_coords\n    distance_from_summit = np.zeros((rows, cols), dtype=np.float32)\n    for r in range(rows):\n        for c in range(cols):\n            distance = np.sqrt((r - summit_row) ** 2 + (c - summit_col) ** 2) * cell_size\n            distance_from_summit[r, c] = distance\n    return distance_from_summit\n</pre> def calculate_distance_from_summit(summit_coords, rows, cols, cell_size=100):     \"\"\"     Compute Euclidean distance (in meters) from the summit for each cell in a raster.          This function creates a distance raster where each cell contains the straight-line     distance from that cell to the summit location. The distance is calculated using     the Euclidean formula and converted to meters using the specified cell size.          Parameters:     -----------     summit_coords : tuple         A tuple (row, col) containing the raster coordinates of the summit.          rows : int         The number of rows in the output raster.          cols : int         The number of columns in the output raster.          cell_size : float, optional         The cell size in meters. Default is 100 meters.          Returns:     --------     numpy.ndarray         A 2D array of shape (rows, cols) containing the Euclidean distance from         each cell to the summit in meters.          Notes:     ------     - The function assumes a regular grid with square cells of uniform size.     - The output array is of type numpy.float32.     - The calculation is performed using raster coordinates, with the result scaled       by the cell size to get distances in meters.     - Progress and timing information is printed to standard output.     - The computation uses a nested loop approach which may be slow for large rasters.       Consider using vectorized operations for better performance.     \"\"\"     print(\"Calculating distance from summit...\")     summit_row, summit_col = summit_coords     distance_from_summit = np.zeros((rows, cols), dtype=np.float32)     for r in range(rows):         for c in range(cols):             distance = np.sqrt((r - summit_row) ** 2 + (c - summit_col) ** 2) * cell_size             distance_from_summit[r, c] = distance     return distance_from_summit"},{"location":"source-code/evacuation-analysis/io-utils.html","title":"io_utils.py","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport csv\nimport time\nimport fiona\nimport geopandas as gpd\nimport rasterio\nimport numpy as np\nimport pandas as pd\n</pre> import os import csv import time import fiona import geopandas as gpd import rasterio import numpy as np import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def read_shapefile(path, crs_epsg=32651):\n    \"\"\"\n    Read a shapefile into a GeoDataFrame with proper coordinate reference system handling.\n    \n    This function reads a shapefile from the specified path using fiona and GeoPandas,\n    and ensures that a coordinate reference system (CRS) is assigned. If the shapefile\n    does not have a defined CRS, it will assign the specified EPSG code.\n    \n    Parameters:\n    -----------\n    path : str\n        The file path to the shapefile to be read.\n    \n    crs_epsg : int, optional\n        The EPSG code to assign if the shapefile lacks a CRS. Default is 32651\n        (WGS 84 / UTM zone 51N), which is appropriate for Indonesia/Sumatra region.\n    \n    Returns:\n    --------\n    geopandas.GeoDataFrame\n        A GeoDataFrame containing the features from the shapefile with the proper CRS.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The function uses fiona directly to open the file and then creates a GeoDataFrame\n      from the features.\n    - If the shapefile has a defined CRS, it will be preserved; otherwise, the specified\n      EPSG code will be assigned.\n    - EPSG 32651 (the default) is appropriate for the Mt. Awu region in Indonesia.\n    \n    Raises:\n    -------\n    FileNotFoundError\n        If the shapefile does not exist at the specified path.\n    fiona.errors.DriverError\n        If the file exists but cannot be read as a shapefile.\n    \"\"\"\n    print(f\"Reading shapefile: {os.path.basename(path)}\")\n    with fiona.open(path) as src:\n        gdf = gpd.GeoDataFrame.from_features(src)\n        if gdf.crs is None:\n            gdf.set_crs(epsg=crs_epsg, inplace=True)\n    return gdf\n</pre> def read_shapefile(path, crs_epsg=32651):     \"\"\"     Read a shapefile into a GeoDataFrame with proper coordinate reference system handling.          This function reads a shapefile from the specified path using fiona and GeoPandas,     and ensures that a coordinate reference system (CRS) is assigned. If the shapefile     does not have a defined CRS, it will assign the specified EPSG code.          Parameters:     -----------     path : str         The file path to the shapefile to be read.          crs_epsg : int, optional         The EPSG code to assign if the shapefile lacks a CRS. Default is 32651         (WGS 84 / UTM zone 51N), which is appropriate for Indonesia/Sumatra region.          Returns:     --------     geopandas.GeoDataFrame         A GeoDataFrame containing the features from the shapefile with the proper CRS.          Notes:     ------     - Progress and timing information is printed to standard output.     - The function uses fiona directly to open the file and then creates a GeoDataFrame       from the features.     - If the shapefile has a defined CRS, it will be preserved; otherwise, the specified       EPSG code will be assigned.     - EPSG 32651 (the default) is appropriate for the Mt. Awu region in Indonesia.          Raises:     -------     FileNotFoundError         If the shapefile does not exist at the specified path.     fiona.errors.DriverError         If the file exists but cannot be read as a shapefile.     \"\"\"     print(f\"Reading shapefile: {os.path.basename(path)}\")     with fiona.open(path) as src:         gdf = gpd.GeoDataFrame.from_features(src)         if gdf.crs is None:             gdf.set_crs(epsg=crs_epsg, inplace=True)     return gdf In\u00a0[\u00a0]: Copied! <pre>def read_raster(path):\n    \"\"\"\n    Read a raster file and return its data and associated metadata.\n    \n    This function opens a raster file using rasterio and extracts the raster data,\n    metadata, spatial reference information, and other key properties needed for\n    geospatial analysis.\n    \n    Parameters:\n    -----------\n    path : str\n        The file path to the raster file to be read.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - data (numpy.ndarray): The raster data with shape (bands, rows, cols)\n        - meta (dict): A copy of the raster metadata\n        - transform (affine.Affine): The affine transform defining the relationship\n          between pixel coordinates and geographic coordinates\n        - nodata (float or int): The value used to represent no data or null values\n        - bounds (rasterio.coords.BoundingBox): The geographic bounds of the raster\n          (left, bottom, right, top)\n        - resolution (tuple): A tuple (x_res, y_res) containing the pixel size in\n          the x and y directions\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The function ensures the raster file is properly closed after reading by using\n      a context manager.\n    - The metadata dictionary returned includes information such as driver, CRS,\n      dimensions, data type, etc.\n    \n    Raises:\n    -------\n    FileNotFoundError\n        If the raster file does not exist at the specified path.\n    rasterio.errors.RasterioIOError\n        If the file exists but cannot be read as a raster.\n    \"\"\"\n    print(f\"Reading raster: {os.path.basename(path)}\")\n    with rasterio.open(path) as src:\n        data = src.read()\n        meta = src.meta.copy()\n        transform = src.transform\n        nodata = src.nodata\n        bounds = src.bounds\n        resolution = src.res\n    return data, meta, transform, nodata, bounds, resolution\n</pre> def read_raster(path):     \"\"\"     Read a raster file and return its data and associated metadata.          This function opens a raster file using rasterio and extracts the raster data,     metadata, spatial reference information, and other key properties needed for     geospatial analysis.          Parameters:     -----------     path : str         The file path to the raster file to be read.          Returns:     --------     tuple         A tuple containing:         - data (numpy.ndarray): The raster data with shape (bands, rows, cols)         - meta (dict): A copy of the raster metadata         - transform (affine.Affine): The affine transform defining the relationship           between pixel coordinates and geographic coordinates         - nodata (float or int): The value used to represent no data or null values         - bounds (rasterio.coords.BoundingBox): The geographic bounds of the raster           (left, bottom, right, top)         - resolution (tuple): A tuple (x_res, y_res) containing the pixel size in           the x and y directions          Notes:     ------     - Progress and timing information is printed to standard output.     - The function ensures the raster file is properly closed after reading by using       a context manager.     - The metadata dictionary returned includes information such as driver, CRS,       dimensions, data type, etc.          Raises:     -------     FileNotFoundError         If the raster file does not exist at the specified path.     rasterio.errors.RasterioIOError         If the file exists but cannot be read as a raster.     \"\"\"     print(f\"Reading raster: {os.path.basename(path)}\")     with rasterio.open(path) as src:         data = src.read()         meta = src.meta.copy()         transform = src.transform         nodata = src.nodata         bounds = src.bounds         resolution = src.res     return data, meta, transform, nodata, bounds, resolution In\u00a0[\u00a0]: Copied! <pre>def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):\n    \"\"\"\n    Save a numpy array as a raster file with specified metadata.\n    \n    This function writes a numpy array to disk as a geospatial raster file with\n    appropriate metadata, including coordinate reference system, transformation,\n    and no-data value.\n    \n    Parameters:\n    -----------\n    output_path : str\n        The file path where the raster will be saved.\n    \n    data : numpy.ndarray\n        The raster data to be saved. If 3D, the first dimension should be bands.\n        If 2D, it will be treated as a single-band raster.\n    \n    meta : dict\n        Metadata dictionary containing information such as width, height, transform,\n        and CRS. This is typically obtained from another raster.\n    \n    dtype : rasterio data type, optional\n        The data type to use for the output raster. Default is rasterio.float32.\n    \n    nodata : int or float, optional\n        The value to use for no-data or missing values in the output raster.\n        Default is -1.\n    \n    Returns:\n    --------\n    None\n        The function does not return a value, but writes the raster to disk.\n    \n    Notes:\n    ------\n    - The function updates the provided metadata with the specified dtype, band count,\n      compression method, and nodata value.\n    - LZW compression is applied to the output raster for efficient storage.\n    - The function assumes that if the input data is 2D, it represents a single band.\n    - Existing files at the output path will be overwritten without warning.\n    \n    Raises:\n    -------\n    rasterio.errors.RasterioIOError\n        If the raster cannot be written to the specified location.\n    \"\"\"\n    meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)\n    with rasterio.open(output_path, 'w', **meta) as dst:\n        dst.write(data, 1)\n</pre> def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):     \"\"\"     Save a numpy array as a raster file with specified metadata.          This function writes a numpy array to disk as a geospatial raster file with     appropriate metadata, including coordinate reference system, transformation,     and no-data value.          Parameters:     -----------     output_path : str         The file path where the raster will be saved.          data : numpy.ndarray         The raster data to be saved. If 3D, the first dimension should be bands.         If 2D, it will be treated as a single-band raster.          meta : dict         Metadata dictionary containing information such as width, height, transform,         and CRS. This is typically obtained from another raster.          dtype : rasterio data type, optional         The data type to use for the output raster. Default is rasterio.float32.          nodata : int or float, optional         The value to use for no-data or missing values in the output raster.         Default is -1.          Returns:     --------     None         The function does not return a value, but writes the raster to disk.          Notes:     ------     - The function updates the provided metadata with the specified dtype, band count,       compression method, and nodata value.     - LZW compression is applied to the output raster for efficient storage.     - The function assumes that if the input data is 2D, it represents a single band.     - Existing files at the output path will be overwritten without warning.          Raises:     -------     rasterio.errors.RasterioIOError         If the raster cannot be written to the specified location.     \"\"\"     meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)     with rasterio.open(output_path, 'w', **meta) as dst:         dst.write(data, 1) In\u00a0[\u00a0]: Copied! <pre>def load_raster(file_path):\n    \"\"\"\n    Load a single-band raster file and return its data and metadata.\n    \n    This function opens a raster file using rasterio and extracts the first band\n    of data along with its metadata. Unlike the read_raster function which reads\n    all bands, this function is optimized for single-band rasters.\n    \n    Parameters:\n    -----------\n    file_path : str\n        The file path to the raster file to be loaded.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - data (numpy.ndarray): The raster data from the first band with shape (rows, cols)\n        - meta (dict): A copy of the raster metadata\n    \n    Notes:\n    ------\n    - The function reads only the first band (band index 1) of the raster.\n    - The metadata dictionary returned includes information such as driver, CRS,\n      dimensions, data type, transform, etc.\n    - The function ensures the raster file is properly closed after reading by using\n      a context manager.\n    \n    Raises:\n    -------\n    FileNotFoundError\n        If the raster file does not exist at the specified path.\n    rasterio.errors.RasterioIOError\n        If the file exists but cannot be read as a raster.\n    \"\"\"\n    with rasterio.open(file_path) as src:\n        data = src.read(1)\n        meta = src.meta.copy()\n    return data, meta\n</pre> def load_raster(file_path):     \"\"\"     Load a single-band raster file and return its data and metadata.          This function opens a raster file using rasterio and extracts the first band     of data along with its metadata. Unlike the read_raster function which reads     all bands, this function is optimized for single-band rasters.          Parameters:     -----------     file_path : str         The file path to the raster file to be loaded.          Returns:     --------     tuple         A tuple containing:         - data (numpy.ndarray): The raster data from the first band with shape (rows, cols)         - meta (dict): A copy of the raster metadata          Notes:     ------     - The function reads only the first band (band index 1) of the raster.     - The metadata dictionary returned includes information such as driver, CRS,       dimensions, data type, transform, etc.     - The function ensures the raster file is properly closed after reading by using       a context manager.          Raises:     -------     FileNotFoundError         If the raster file does not exist at the specified path.     rasterio.errors.RasterioIOError         If the file exists but cannot be read as a raster.     \"\"\"     with rasterio.open(file_path) as src:         data = src.read(1)         meta = src.meta.copy()     return data, meta In\u00a0[\u00a0]: Copied! <pre>def save_analysis_report(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):\n    \"\"\"\n    Save evacuation analysis results to a formatted text report file.\n    \n    This function generates a human-readable text report summarizing the minimum \n    travel times from various sources to safe zones for different walking speeds.\n    \n    Parameters:\n    -----------\n    output_path : str\n        The file path where the report will be saved.\n    \n    results : dict\n        A nested dictionary containing travel time results:\n        {speed_name: {safe_zone_distance: [times_per_source]}}\n        where times_per_source is a list of travel times (in hours) for each source.\n    \n    min_coords : dict\n        A nested dictionary containing the coordinates of minimum travel times:\n        {speed_name: {safe_zone_distance: [coords_per_source]}}\n        where coords_per_source is a list of (row, col) tuples for each source.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n    \n    walking_speeds : dict\n        Dictionary mapping speed names (str) to speed values (float) in meters per second.\n    \n    safe_zone_distances : list\n        List of safe zone distances used in the analysis.\n    \n    Returns:\n    --------\n    None\n        The function does not return a value, but writes results to a text file.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The report is organized hierarchically by walking speed, safe zone distance, \n      and source location.\n    - Travel times are formatted to 2 decimal places.\n    \"\"\"\n    with open(output_path, 'w') as f:\n        f.write(\"Safe Zone Travel Time Analysis Report\\n\")\n        f.write(\"======================================\\n\\n\")\n        for speed_name in walking_speeds.keys():\n            f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")\n            f.write(\"-\" * 40 + \"\\n\")\n            for safe_zone in safe_zone_distances:\n                f.write(f\"\\nSafe Zone: {safe_zone} m\\n\")\n                for idx, source_name in enumerate(source_names):\n                    tt = results[speed_name][safe_zone][idx]\n                    coords = min_coords[speed_name][safe_zone][idx]\n                    f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\")\n</pre> def save_analysis_report(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):     \"\"\"     Save evacuation analysis results to a formatted text report file.          This function generates a human-readable text report summarizing the minimum      travel times from various sources to safe zones for different walking speeds.          Parameters:     -----------     output_path : str         The file path where the report will be saved.          results : dict         A nested dictionary containing travel time results:         {speed_name: {safe_zone_distance: [times_per_source]}}         where times_per_source is a list of travel times (in hours) for each source.          min_coords : dict         A nested dictionary containing the coordinates of minimum travel times:         {speed_name: {safe_zone_distance: [coords_per_source]}}         where coords_per_source is a list of (row, col) tuples for each source.          source_names : list         A list of source location names corresponding to indices in the results arrays.          walking_speeds : dict         Dictionary mapping speed names (str) to speed values (float) in meters per second.          safe_zone_distances : list         List of safe zone distances used in the analysis.          Returns:     --------     None         The function does not return a value, but writes results to a text file.          Notes:     ------     - Progress and timing information is printed to standard output.     - The report is organized hierarchically by walking speed, safe zone distance,        and source location.     - Travel times are formatted to 2 decimal places.     \"\"\"     with open(output_path, 'w') as f:         f.write(\"Safe Zone Travel Time Analysis Report\\n\")         f.write(\"======================================\\n\\n\")         for speed_name in walking_speeds.keys():             f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")             f.write(\"-\" * 40 + \"\\n\")             for safe_zone in safe_zone_distances:                 f.write(f\"\\nSafe Zone: {safe_zone} m\\n\")                 for idx, source_name in enumerate(source_names):                     tt = results[speed_name][safe_zone][idx]                     coords = min_coords[speed_name][safe_zone][idx]                     f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\") In\u00a0[\u00a0]: Copied! <pre>def save_metrics_csv(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):\n    \"\"\"\n    Save evacuation analysis metrics to a CSV file for further processing.\n    \n    This function exports the analysis results in a tabular format suitable for\n    import into spreadsheet software or data analysis tools. Each row represents\n    a unique combination of walking speed, safe zone distance, and source location.\n    \n    Parameters:\n    -----------\n    output_path : str\n        The file path where the CSV will be saved.\n    \n    results : dict\n        A nested dictionary containing travel time results:\n        {speed_name: {safe_zone_distance: [times_per_source]}}\n        where times_per_source is a list of travel times (in hours) for each source.\n    \n    min_coords : dict\n        A nested dictionary containing the coordinates of minimum travel times:\n        {speed_name: {safe_zone_distance: [coords_per_source]}}\n        where coords_per_source is a list of (row, col) tuples for each source.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n    \n    walking_speeds : dict\n        Dictionary mapping speed names (str) to speed values (float) in meters per second.\n    \n    safe_zone_distances : list\n        List of safe zone distances used in the analysis.\n    \n    Returns:\n    --------\n    None\n        The function does not return a value, but writes results to a CSV file.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - The CSV includes columns for Walking_Speed, Safe_Zone, Source, \n      Min_Travel_Time (hrs), and Min_Coords (row,col).\n    - All combinations of walking speed, safe zone, and source are included.\n    \"\"\"\n    with open(output_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Walking_Speed', 'Safe_Zone', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])\n        for speed_name in walking_speeds.keys():\n            for safe_zone in safe_zone_distances:\n                for idx, source_name in enumerate(source_names):\n                    writer.writerow([\n                        speed_name,\n                        safe_zone,\n                        source_name,\n                        results[speed_name][safe_zone][idx],\n                        min_coords[speed_name][safe_zone][idx]\n                    ])\n</pre> def save_metrics_csv(output_path, results, min_coords, source_names, walking_speeds, safe_zone_distances):     \"\"\"     Save evacuation analysis metrics to a CSV file for further processing.          This function exports the analysis results in a tabular format suitable for     import into spreadsheet software or data analysis tools. Each row represents     a unique combination of walking speed, safe zone distance, and source location.          Parameters:     -----------     output_path : str         The file path where the CSV will be saved.          results : dict         A nested dictionary containing travel time results:         {speed_name: {safe_zone_distance: [times_per_source]}}         where times_per_source is a list of travel times (in hours) for each source.          min_coords : dict         A nested dictionary containing the coordinates of minimum travel times:         {speed_name: {safe_zone_distance: [coords_per_source]}}         where coords_per_source is a list of (row, col) tuples for each source.          source_names : list         A list of source location names corresponding to indices in the results arrays.          walking_speeds : dict         Dictionary mapping speed names (str) to speed values (float) in meters per second.          safe_zone_distances : list         List of safe zone distances used in the analysis.          Returns:     --------     None         The function does not return a value, but writes results to a CSV file.          Notes:     ------     - Progress and timing information is printed to standard output.     - The CSV includes columns for Walking_Speed, Safe_Zone, Source,        Min_Travel_Time (hrs), and Min_Coords (row,col).     - All combinations of walking speed, safe zone, and source are included.     \"\"\"     with open(output_path, 'w', newline='') as f:         writer = csv.writer(f)         writer.writerow(['Walking_Speed', 'Safe_Zone', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])         for speed_name in walking_speeds.keys():             for safe_zone in safe_zone_distances:                 for idx, source_name in enumerate(source_names):                     writer.writerow([                         speed_name,                         safe_zone,                         source_name,                         results[speed_name][safe_zone][idx],                         min_coords[speed_name][safe_zone][idx]                     ])"},{"location":"source-code/evacuation-analysis/path-utils.html","title":"path_utils.py","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport time\nfrom numpy import sqrt\nfrom scipy.sparse import lil_matrix\nfrom grid_utils import to_1d\nfrom tqdm import tqdm\n</pre> import numpy as np import time from numpy import sqrt from scipy.sparse import lil_matrix from grid_utils import to_1d from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):\n    \"\"\"\n    Calculate metrics for the shortest path between source and target nodes.\n    \n    This function reconstructs the shortest path using the predecessor array and\n    computes various metrics including the number of pixels in the path, the cost\n    of each step, and the total path cost.\n    \n    Parameters:\n    -----------\n    predecessors : numpy.ndarray\n        A 1D array where predecessors[i] contains the predecessor node of node i\n        on the shortest path from the source.\n    \n    source_node : int\n        The source node index (starting point) of the path.\n    \n    target_node : int or float\n        The target node index (destination) of the path. Can be -9999 or NaN to\n        indicate an invalid target.\n    \n    graph_csr : scipy.sparse.csr_matrix\n        The graph in CSR format where each entry (i,j) represents the cost/weight\n        of the edge from node i to node j.\n    \n    rows : int\n        The number of rows in the original raster grid.\n    \n    cols : int\n        The number of columns in the original raster grid.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - pixel_count (int): The number of pixels in the path.\n        - cell_costs (list): A list of the costs for each step in the path.\n        - total_cost (float): The sum of all cell costs along the path.\n    \n    Notes:\n    ------\n    - If target_node is -9999 or NaN, the function returns (0, [], 0) indicating\n      no valid path.\n    - The path is reconstructed from the target to the source and then reversed.\n    - A value of -9999 in the predecessors array indicates \"no predecessor\".\n    - Progress and timing information is printed to standard output.\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return 0, [], 0\n    \n    path = []\n    current = target_node\n    while current != source_node and current != -9999:\n        path.append(current)\n        current = predecessors[current]\n    if current != -9999:\n        path.append(source_node)\n    path.reverse()\n    \n    pixel_count = len(path)\n    cell_costs = []\n    total_cost = 0\n    for i in range(len(path)-1):\n        cell_cost = graph_csr[path[i], path[i+1]]\n        cell_costs.append(cell_cost)\n        total_cost += cell_cost\n    \n    return pixel_count, cell_costs, total_cost\n</pre> def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):     \"\"\"     Calculate metrics for the shortest path between source and target nodes.          This function reconstructs the shortest path using the predecessor array and     computes various metrics including the number of pixels in the path, the cost     of each step, and the total path cost.          Parameters:     -----------     predecessors : numpy.ndarray         A 1D array where predecessors[i] contains the predecessor node of node i         on the shortest path from the source.          source_node : int         The source node index (starting point) of the path.          target_node : int or float         The target node index (destination) of the path. Can be -9999 or NaN to         indicate an invalid target.          graph_csr : scipy.sparse.csr_matrix         The graph in CSR format where each entry (i,j) represents the cost/weight         of the edge from node i to node j.          rows : int         The number of rows in the original raster grid.          cols : int         The number of columns in the original raster grid.          Returns:     --------     tuple         A tuple containing:         - pixel_count (int): The number of pixels in the path.         - cell_costs (list): A list of the costs for each step in the path.         - total_cost (float): The sum of all cell costs along the path.          Notes:     ------     - If target_node is -9999 or NaN, the function returns (0, [], 0) indicating       no valid path.     - The path is reconstructed from the target to the source and then reversed.     - A value of -9999 in the predecessors array indicates \"no predecessor\".     - Progress and timing information is printed to standard output.     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return 0, [], 0          path = []     current = target_node     while current != source_node and current != -9999:         path.append(current)         current = predecessors[current]     if current != -9999:         path.append(source_node)     path.reverse()          pixel_count = len(path)     cell_costs = []     total_cost = 0     for i in range(len(path)-1):         cell_cost = graph_csr[path[i], path[i+1]]         cell_costs.append(cell_cost)         total_cost += cell_cost          return pixel_count, cell_costs, total_cost In\u00a0[\u00a0]: Copied! <pre>def reconstruct_path(pred, source_node, target_node, cols):\n    \"\"\"\n    Reconstruct the shortest path from source to target and convert to 2D coordinates.\n    \n    This function traces back through a predecessor array to reconstruct the path\n    from source to target, then converts the 1D node indices to 2D grid coordinates.\n    \n    Parameters:\n    -----------\n    pred : numpy.ndarray\n        A 1D array where pred[i] contains the predecessor node of node i\n        on the shortest path from the source.\n    \n    source_node : int\n        The source node index (starting point) of the path.\n    \n    target_node : int or float\n        The target node index (destination) of the path. Can be -9999 or NaN to\n        indicate an invalid target.\n    \n    cols : int\n        The number of columns in the grid, used to convert 1D indices to 2D coordinates.\n    \n    Returns:\n    --------\n    list\n        A list of tuples (row, col) representing the 2D coordinates of each point\n        along the path from source to target. Returns an empty list if no valid path exists.\n    \n    Notes:\n    ------\n    - If target_node is -9999 or NaN, the function returns an empty list.\n    - The path is reconstructed from target to source and then reversed.\n    - A value of -9999 in the predecessor array indicates \"no predecessor\".\n    - 1D indices are converted to 2D coordinates using integer division and modulo operations.\n    - Progress and timing information is printed to standard output.\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return []\n        \n    path_nodes = []\n    current = target_node\n    while current != source_node and current != -9999:\n        path_nodes.append(current)\n        current = pred[current]\n    path_nodes.append(source_node)\n    path_nodes.reverse()\n    \n    # Convert each 1D index to (row, col)\n    path_coords = [(node // cols, node % cols) for node in path_nodes]\n    \n    return path_coords\n</pre> def reconstruct_path(pred, source_node, target_node, cols):     \"\"\"     Reconstruct the shortest path from source to target and convert to 2D coordinates.          This function traces back through a predecessor array to reconstruct the path     from source to target, then converts the 1D node indices to 2D grid coordinates.          Parameters:     -----------     pred : numpy.ndarray         A 1D array where pred[i] contains the predecessor node of node i         on the shortest path from the source.          source_node : int         The source node index (starting point) of the path.          target_node : int or float         The target node index (destination) of the path. Can be -9999 or NaN to         indicate an invalid target.          cols : int         The number of columns in the grid, used to convert 1D indices to 2D coordinates.          Returns:     --------     list         A list of tuples (row, col) representing the 2D coordinates of each point         along the path from source to target. Returns an empty list if no valid path exists.          Notes:     ------     - If target_node is -9999 or NaN, the function returns an empty list.     - The path is reconstructed from target to source and then reversed.     - A value of -9999 in the predecessor array indicates \"no predecessor\".     - 1D indices are converted to 2D coordinates using integer division and modulo operations.     - Progress and timing information is printed to standard output.     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return []              path_nodes = []     current = target_node     while current != source_node and current != -9999:         path_nodes.append(current)         current = pred[current]     path_nodes.append(source_node)     path_nodes.reverse()          # Convert each 1D index to (row, col)     path_coords = [(node // cols, node % cols) for node in path_nodes]          return path_coords In\u00a0[\u00a0]: Copied! <pre>def build_adjacency_matrix(cost_array, rows, cols, directions):\n    \"\"\"\n    Build a sparse adjacency matrix representing the graph for path finding.\n    \n    This function creates a graph representation of the raster grid where nodes\n    are grid cells and edges represent possible movements between adjacent cells.\n    Edge weights are derived from the cost array, with diagonal movements adjusted\n    by a factor of sqrt(2) to account for the increased distance.\n    \n    Parameters:\n    -----------\n    cost_array : numpy.ndarray\n        A 3D array of shape (n_directions, rows, cols) containing the cost values\n        for moving in each direction from each cell. The first dimension corresponds\n        to the directions defined in the directions parameter.\n    \n    rows : int\n        The number of rows in the grid.\n    \n    cols : int\n        The number of columns in the grid.\n    \n    directions : list of tuples\n        A list of (dr, dc) tuples defining the possible movement directions.\n        Typically includes the 8 neighboring directions (cardinal and diagonal).\n    \n    Returns:\n    --------\n    scipy.sparse.csr_matrix\n        A sparse CSR matrix representation of the graph, where each entry (i, j)\n        represents the cost of moving from node i to node j. The matrix has\n        dimensions (rows*cols, rows*cols).\n    \n    Notes:\n    ------\n    - Progress information is displayed using tqdm and timing information is printed.\n    - Invalid costs (NaN, infinite, negative, or zero) are skipped.\n    - Diagonal movements have their costs multiplied by sqrt(2) to account for the\n      increased distance.\n    - The function first builds the matrix in LIL format for efficient construction,\n      then converts to CSR format for efficient operations.\n    - Nodes are indexed in row-major order using the to_1d function.\n    \"\"\"\n    print(\"\\nBuilding adjacency matrix...\")\n    start_time = time.time()\n    \n    graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)\n    direction_indices = {dir: idx for idx, dir in enumerate(directions)}\n    \n    # Use tqdm for progress bar\n    for r in tqdm(range(rows), desc=\"Processing rows\"):\n        for c in range(cols):\n            current_node = to_1d(r, c, cols)\n            for dr, dc in directions:\n                nr, nc = r + dr, c + dc\n                if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:\n                    neighbor_node = to_1d(nr, nc, cols)\n                    idx = direction_indices[(dr, dc)]\n                    cost_val = cost_array[idx, r, c]\n                    if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:\n                        continue\n                    if abs(dr) == 1 and abs(dc) == 1:\n                        cost_val *= sqrt(2)\n                    graph[current_node, neighbor_node] = cost_val\n    \n    graph_csr = graph.tocsr()  # Convert to CSR format for efficient operations\n    end_time = time.time()\n    print(f\"Adjacency matrix built in {end_time - start_time:.2f} seconds\")\n    print(f\"Matrix shape: {graph_csr.shape}\")\n    print(f\"Number of non-zero elements: {graph_csr.nnz}\")\n    return graph_csr\n</pre> def build_adjacency_matrix(cost_array, rows, cols, directions):     \"\"\"     Build a sparse adjacency matrix representing the graph for path finding.          This function creates a graph representation of the raster grid where nodes     are grid cells and edges represent possible movements between adjacent cells.     Edge weights are derived from the cost array, with diagonal movements adjusted     by a factor of sqrt(2) to account for the increased distance.          Parameters:     -----------     cost_array : numpy.ndarray         A 3D array of shape (n_directions, rows, cols) containing the cost values         for moving in each direction from each cell. The first dimension corresponds         to the directions defined in the directions parameter.          rows : int         The number of rows in the grid.          cols : int         The number of columns in the grid.          directions : list of tuples         A list of (dr, dc) tuples defining the possible movement directions.         Typically includes the 8 neighboring directions (cardinal and diagonal).          Returns:     --------     scipy.sparse.csr_matrix         A sparse CSR matrix representation of the graph, where each entry (i, j)         represents the cost of moving from node i to node j. The matrix has         dimensions (rows*cols, rows*cols).          Notes:     ------     - Progress information is displayed using tqdm and timing information is printed.     - Invalid costs (NaN, infinite, negative, or zero) are skipped.     - Diagonal movements have their costs multiplied by sqrt(2) to account for the       increased distance.     - The function first builds the matrix in LIL format for efficient construction,       then converts to CSR format for efficient operations.     - Nodes are indexed in row-major order using the to_1d function.     \"\"\"     print(\"\\nBuilding adjacency matrix...\")     start_time = time.time()          graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)     direction_indices = {dir: idx for idx, dir in enumerate(directions)}          # Use tqdm for progress bar     for r in tqdm(range(rows), desc=\"Processing rows\"):         for c in range(cols):             current_node = to_1d(r, c, cols)             for dr, dc in directions:                 nr, nc = r + dr, c + dc                 if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:                     neighbor_node = to_1d(nr, nc, cols)                     idx = direction_indices[(dr, dc)]                     cost_val = cost_array[idx, r, c]                     if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:                         continue                     if abs(dr) == 1 and abs(dc) == 1:                         cost_val *= sqrt(2)                     graph[current_node, neighbor_node] = cost_val          graph_csr = graph.tocsr()  # Convert to CSR format for efficient operations     end_time = time.time()     print(f\"Adjacency matrix built in {end_time - start_time:.2f} seconds\")     print(f\"Matrix shape: {graph_csr.shape}\")     print(f\"Number of non-zero elements: {graph_csr.nnz}\")     return graph_csr"},{"location":"source-code/evacuation-analysis/visualization.html","title":"visualization.py","text":"In\u00a0[\u00a0]: Copied! <pre>import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom matplotlib.patheffects import withStroke\nimport os\nimport rasterio\nimport pandas as pd\n</pre> import time import numpy as np import matplotlib.pyplot as plt import matplotlib.colors as mcolors from matplotlib.patheffects import withStroke import os import rasterio import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def load_raster(path):\n    \"\"\"\n    Load a single-band raster file and return its data and metadata.\n    \n    This helper function opens a raster file using rasterio and extracts\n    the first band of data along with essential metadata for visualization.\n    \n    Parameters:\n    -----------\n    path : str\n        The file path to the raster file to be loaded.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - array (numpy.ndarray): The raster data from the first band with shape (rows, cols)\n        - metadata (dict): A dictionary containing:\n            - 'transform' (affine.Affine): The affine transform\n            - 'crs' (CRS): The coordinate reference system\n            - 'nodata' (float or int): The no-data value\n    \n    Notes:\n    ------\n    - This function is streamlined for visualization purposes, reading only the\n      first band and a subset of metadata.\n    - The raster file is properly closed after reading using a context manager.\n    \"\"\"\n    with rasterio.open(path) as src:\n        array = src.read(1)  # Read the first band\n        metadata = {\n            'transform': src.transform,\n            'crs': src.crs,\n            'nodata': src.nodata\n        }\n    return array, metadata\n</pre> def load_raster(path):     \"\"\"     Load a single-band raster file and return its data and metadata.          This helper function opens a raster file using rasterio and extracts     the first band of data along with essential metadata for visualization.          Parameters:     -----------     path : str         The file path to the raster file to be loaded.          Returns:     --------     tuple         A tuple containing:         - array (numpy.ndarray): The raster data from the first band with shape (rows, cols)         - metadata (dict): A dictionary containing:             - 'transform' (affine.Affine): The affine transform             - 'crs' (CRS): The coordinate reference system             - 'nodata' (float or int): The no-data value          Notes:     ------     - This function is streamlined for visualization purposes, reading only the       first band and a subset of metadata.     - The raster file is properly closed after reading using a context manager.     \"\"\"     with rasterio.open(path) as src:         array = src.read(1)  # Read the first band         metadata = {             'transform': src.transform,             'crs': src.crs,             'nodata': src.nodata         }     return array, metadata In\u00a0[\u00a0]: Copied! <pre>def raster_coord_to_map_coords(row, col, transform):\n    \"\"\"\n    Convert raster coordinates (row, column) to map coordinates (x, y).\n    \n    This function transforms pixel coordinates in a raster to their corresponding\n    geographic coordinates using the raster's affine transform.\n    \n    Parameters:\n    -----------\n    row : int\n        The row index in the raster (zero-based).\n    \n    col : int\n        The column index in the raster (zero-based).\n    \n    transform : affine.Affine or list/tuple\n        The affine transform of the raster, defining the relationship between\n        pixel coordinates and geographic coordinates. Can be provided as an\n        Affine object or as a 6-element tuple/list in the form \n        [a, b, c, d, e, f] where:\n        - a: width of a pixel\n        - b: row rotation (typically 0)\n        - c: x-coordinate of the upper-left corner\n        - d: column rotation (typically 0)\n        - e: height of a pixel (typically negative)\n        - f: y-coordinate of the upper-left corner\n    \n    Returns:\n    --------\n    tuple\n        A tuple (x, y) containing the geographic coordinates corresponding to\n        the specified raster cell.\n    \n    Notes:\n    ------\n    - The function calculates coordinates based on the transform components\n      rather than using the rasterio.transform.xy function, enabling use with\n      transform arrays as well as Affine objects.\n    \"\"\"\n    x = transform[2] + col * transform[0]\n    y = transform[5] + row * transform[4]\n    return x, y\n</pre> def raster_coord_to_map_coords(row, col, transform):     \"\"\"     Convert raster coordinates (row, column) to map coordinates (x, y).          This function transforms pixel coordinates in a raster to their corresponding     geographic coordinates using the raster's affine transform.          Parameters:     -----------     row : int         The row index in the raster (zero-based).          col : int         The column index in the raster (zero-based).          transform : affine.Affine or list/tuple         The affine transform of the raster, defining the relationship between         pixel coordinates and geographic coordinates. Can be provided as an         Affine object or as a 6-element tuple/list in the form          [a, b, c, d, e, f] where:         - a: width of a pixel         - b: row rotation (typically 0)         - c: x-coordinate of the upper-left corner         - d: column rotation (typically 0)         - e: height of a pixel (typically negative)         - f: y-coordinate of the upper-left corner          Returns:     --------     tuple         A tuple (x, y) containing the geographic coordinates corresponding to         the specified raster cell.          Notes:     ------     - The function calculates coordinates based on the transform components       rather than using the rasterio.transform.xy function, enabling use with       transform arrays as well as Affine objects.     \"\"\"     x = transform[2] + col * transform[0]     y = transform[5] + row * transform[4]     return x, y In\u00a0[\u00a0]: Copied! <pre>def plot_travel_time_comparison(all_results, safe_zone_distances, source_names, speed_colors):\n    \"\"\"\n    Create a comparison plot of minimum travel times for different evacuation scenarios.\n    \n    This function generates a figure with three subplots showing travel time curves\n    for summit, camp1, and camp2 sources. Each subplot compares travel times for\n    different walking speeds (slow, medium, fast) and terrain scenarios (original \n    and penalized landcover).\n    \n    Parameters:\n    -----------\n    all_results : dict\n        A nested dictionary containing travel time results for different datasets:\n        {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}\n        where times_per_source is a list of travel times (in hours) for each source.\n        Must include 'final' and 'modify_landcover' datasets.\n    \n    safe_zone_distances : list\n        List of safe zone distances used in the analysis.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n        Must include 'summit', 'camp1', and 'camp2'.\n    \n    speed_colors : dict\n        Dictionary mapping speed names (str) to color values for plotting.\n    \n    Returns:\n    --------\n    matplotlib.figure.Figure\n        The figure object containing the comparison plots, designed for A4 landscape format.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - Plots use solid lines for original landcover and dashed lines for penalized landcover.\n    - The figure includes a shared legend in the rightmost subplot.\n    - Each subplot is labeled with a letter (A, B, C) in the top-left corner.\n    \"\"\"\n    print(\"\\nCreating travel time comparison plot...\")\n    start_time = time.time()\n    \n    # Define specific sources to plot\n    selected_sources = ['summit', 'camp1']\n    n_sources = len(selected_sources)\n    \n    # Create figure in A4 landscape format (11.69 x 8.27 inches)\n    fig, axes = plt.subplots(1, 2, figsize=(11.69, 4), sharey=True)\n    \n    # Store lines and labels for the legend\n    lines = []\n    labels = []\n    first_plot = True\n    \n    # For each source\n    for src_idx, source_name in enumerate(selected_sources):\n        ax = axes[src_idx]\n        \n        # Plot all walking speeds for both original and penalized\n        for speed_name, speed_color in speed_colors.items():\n            # Get data for original and penalized\n            original = [\n                all_results['final'][speed_name][sz][source_names.index(source_name)]\n                for sz in safe_zone_distances\n            ]\n            penalized = [\n                all_results['modify_landcover'][speed_name][sz][source_names.index(source_name)]\n                for sz in safe_zone_distances\n            ]\n            \n            # Plot with consistent line styles\n            line1 = ax.plot(safe_zone_distances, original, '-', color=speed_color)\n            line2 = ax.plot(safe_zone_distances, penalized, '--', color=speed_color)\n            \n            # Only store legend entries once\n            if first_plot:\n                lines.extend([line1[0], line2[0]])\n                labels.extend([\n                    f'{speed_name.capitalize()} - Original', \n                    f'{speed_name.capitalize()} - Penalized'\n                ])\n        \n        # Add letter label in top left corner\n        ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes, \n                fontsize=12, fontweight='bold', va='top')\n        \n        ax.set_xlabel(\"Safe Zone Radius (m)\", fontsize=10)\n        if src_idx == 0:  # Add ylabel only for leftmost plot\n            ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)\n        ax.grid(True, alpha=0.3)\n        \n        # Add legend to the right side of the third subplot\n        if src_idx == 2:  # For the last subplot\n            box = ax.get_position()\n            ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n            ax.legend(lines, labels, fontsize=9, \n                      loc='center left', \n                      bbox_to_anchor=(1.05, 0.5))\n        \n        first_plot = False\n    \n    plt.tight_layout()\n    \n    # Comment out plt.show() to avoid opening a new window:\n    plt.show()\n    \n    end_time = time.time()\n    print(f\"Travel time comparison plot created in {end_time - start_time:.2f} seconds\")\n    return fig\n</pre> def plot_travel_time_comparison(all_results, safe_zone_distances, source_names, speed_colors):     \"\"\"     Create a comparison plot of minimum travel times for different evacuation scenarios.          This function generates a figure with three subplots showing travel time curves     for summit, camp1, and camp2 sources. Each subplot compares travel times for     different walking speeds (slow, medium, fast) and terrain scenarios (original      and penalized landcover).          Parameters:     -----------     all_results : dict         A nested dictionary containing travel time results for different datasets:         {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}         where times_per_source is a list of travel times (in hours) for each source.         Must include 'final' and 'modify_landcover' datasets.          safe_zone_distances : list         List of safe zone distances used in the analysis.          source_names : list         A list of source location names corresponding to indices in the results arrays.         Must include 'summit', 'camp1', and 'camp2'.          speed_colors : dict         Dictionary mapping speed names (str) to color values for plotting.          Returns:     --------     matplotlib.figure.Figure         The figure object containing the comparison plots, designed for A4 landscape format.          Notes:     ------     - Progress and timing information is printed to standard output.     - Plots use solid lines for original landcover and dashed lines for penalized landcover.     - The figure includes a shared legend in the rightmost subplot.     - Each subplot is labeled with a letter (A, B, C) in the top-left corner.     \"\"\"     print(\"\\nCreating travel time comparison plot...\")     start_time = time.time()          # Define specific sources to plot     selected_sources = ['summit', 'camp1']     n_sources = len(selected_sources)          # Create figure in A4 landscape format (11.69 x 8.27 inches)     fig, axes = plt.subplots(1, 2, figsize=(11.69, 4), sharey=True)          # Store lines and labels for the legend     lines = []     labels = []     first_plot = True          # For each source     for src_idx, source_name in enumerate(selected_sources):         ax = axes[src_idx]                  # Plot all walking speeds for both original and penalized         for speed_name, speed_color in speed_colors.items():             # Get data for original and penalized             original = [                 all_results['final'][speed_name][sz][source_names.index(source_name)]                 for sz in safe_zone_distances             ]             penalized = [                 all_results['modify_landcover'][speed_name][sz][source_names.index(source_name)]                 for sz in safe_zone_distances             ]                          # Plot with consistent line styles             line1 = ax.plot(safe_zone_distances, original, '-', color=speed_color)             line2 = ax.plot(safe_zone_distances, penalized, '--', color=speed_color)                          # Only store legend entries once             if first_plot:                 lines.extend([line1[0], line2[0]])                 labels.extend([                     f'{speed_name.capitalize()} - Original',                      f'{speed_name.capitalize()} - Penalized'                 ])                  # Add letter label in top left corner         ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes,                  fontsize=12, fontweight='bold', va='top')                  ax.set_xlabel(\"Safe Zone Radius (m)\", fontsize=10)         if src_idx == 0:  # Add ylabel only for leftmost plot             ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)         ax.grid(True, alpha=0.3)                  # Add legend to the right side of the third subplot         if src_idx == 2:  # For the last subplot             box = ax.get_position()             ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])             ax.legend(lines, labels, fontsize=9,                        loc='center left',                        bbox_to_anchor=(1.05, 0.5))                  first_plot = False          plt.tight_layout()          # Comment out plt.show() to avoid opening a new window:     plt.show()          end_time = time.time()     print(f\"Travel time comparison plot created in {end_time - start_time:.2f} seconds\")     return fig In\u00a0[\u00a0]: Copied! <pre>def create_cost_surface_subplots(dataset_info, cost_arrays, transforms, evacuation_paths, \n                               summit_coords, safe_zone_distances, hiking_gdf, output_path):\n    \"\"\"\n    Create visualizations of cost surfaces with evacuation paths for two terrain scenarios.\n    \n    This function generates a figure with two cost surface maps showing optimal evacuation\n    paths from the summit to different safe zone distances. The maps include distance contours,\n    hiking paths, and the summit location. Cost surfaces use a logarithmic color scale to\n    better visualize travel time differences.\n    \n    Parameters:\n    -----------\n    dataset_info : dict\n        Dictionary containing information about each dataset, including:\n        - keys: Dataset names (e.g., 'final', 'modify_landcover')\n        - values: Dictionaries with dataset metadata\n    \n    cost_arrays : list\n        List of numpy arrays containing the cost surfaces for each dataset.\n    \n    transforms : list\n        List of affine transforms corresponding to each cost array.\n    \n    evacuation_paths : dict\n        Dictionary of evacuation paths for each dataset and safe zone:\n        {dataset_key: {safe_zone_distance: [path_coordinates]}}\n        where path_coordinates is a list of (row, col) tuples.\n    \n    summit_coords : dict\n        Dictionary mapping dataset keys to summit coordinates as (row, col) tuples.\n    \n    safe_zone_distances : list\n        List of safe zone distances to plot paths for.\n    \n    hiking_gdf : geopandas.GeoDataFrame\n        GeoDataFrame containing hiking trail geometries to overlay on the maps.\n    \n    output_path : str\n        File path where the figure will be saved.\n    \n    Returns:\n    --------\n    None\n        The function saves the figure to the specified output path but does not return a value.\n    \n    Notes:\n    ------\n    - Progress and timing information is printed to standard output.\n    - A logarithmic transformation is applied to the cost surfaces to enhance visualization.\n    - The figure includes a color bar indicating the travel time.\n    - Evacuation paths are color-coded by safe zone distance.\n    - Contour lines show distance from the summit.\n    - Each subplot is labeled with a letter (A, B) in the top-left corner.\n    \"\"\"\n    print(\"\\nCreating cost surface subplots...\")\n    start_time = time.time()\n    \n    # Create figure with larger size \n    fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4\n    \n    # Create subplot grid with space for legend and colorbar\n    gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    \n    # Create a common colormap and normalization\n    cmap = plt.cm.RdYlBu_r.copy()\n    cmap.set_bad('white', alpha=0)\n    \n    epsilon = 0.1\n    max_log_cost = np.log1p(10 + epsilon)\n    \n    # Find global vmin and vmax for consistent color scaling\n    vmin = float('inf')\n    vmax = float('-inf')\n    for cost_array in cost_arrays:\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))\n        vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))\n    vcenter = vmin + (vmax - vmin) / 2\n    \n    norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n    \n    # Colors for paths\n    colors_paths = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange', 'red', 'blue', 'purple']\n    \n    # Plot both subplots\n    for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):\n        cost_array = cost_arrays[idx]\n        transform = transforms[idx]\n        \n        # Process cost array\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        \n        # Plot cost surface\n        im = ax.imshow(\n            cost_array_masked, cmap=cmap, norm=norm,\n            extent=[\n                transform[2], \n                transform[2] + transform[0] * cost_array.shape[1],\n                transform[5] + transform[4] * cost_array.shape[0],\n                transform[5]\n            ]\n        )\n        \n        # Plot evacuation paths\n        for i, distance in enumerate(safe_zone_distances):\n            if distance in evacuation_paths[ds_key]:\n                path = evacuation_paths[ds_key][distance]\n                if path:\n                    path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]\n                    xs, ys = zip(*path_coords)\n                    ax.plot(xs, ys, '-', color=colors_paths[i], \n                            linewidth=3, label=f'{distance}m safe zone',\n                            alpha=1.0, zorder=5)\n        \n        # Plot hiking trail with outline effect\n        hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)\n        ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')\n        plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])\n        \n        # Plot summit\n        summit_x, summit_y = raster_coord_to_map_coords(\n            summit_coords[ds_key][0], summit_coords[ds_key][1], transform\n        )\n        ax.plot(summit_x, summit_y, '*', color='yellow', markersize=15, label='Summit', zorder=6)\n        \n        # Add distance contours\n        x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])\n        y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])\n        X, Y = np.meshgrid(x, y)\n        distances = np.sqrt((X - summit_x)**2 + (Y - summit_y)**2)\n        \n        contours = ax.contour(X, Y, distances, levels=safe_zone_distances,\n                              colors='black', linestyles='--', alpha=0.4,\n                              linewidths=1.5, zorder=3)\n        ax.clabel(contours, inline=True, fmt='%1.0fm', fontsize=8)\n        \n        # Set plot limits\n        ax.set_xlim(summit_x - 5000, summit_x + 5000)\n        ax.set_ylim(summit_y - 5000, summit_y + 5000)\n        \n        # Add simple A/B labels\n        ax.text(0.02, 0.98, f'{chr(65+idx)}',\n                transform=ax.transAxes, fontsize=10, fontweight='bold',\n                verticalalignment='top')\n        \n        # Add axis labels\n        ax.set_xlabel('Easting (m)')\n        ax.set_ylabel('Northing (m)')\n    \n    # Add legend\n    handles, labels = ax1.get_legend_handles_labels()\n    unique_labels = []\n    unique_handles = []\n    seen_labels = set()\n    for handle, label in zip(handles, labels):\n        if label not in seen_labels:\n            seen_labels.add(label)\n            unique_labels.append(label)\n            unique_handles.append(handle)\n    \n    ax2.legend(unique_handles, unique_labels, \n               loc='center left',\n               bbox_to_anchor=(1.0, 0.5))\n    \n    # Add horizontal colorbar\n    cbar_ax = fig.add_subplot(gs[1, :])\n    cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n    cbar.set_label('Accumulated Cost', labelpad=10)\n    \n    cbar.set_ticks(np.linspace(vmin, vmax, 6))\n    cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])\n    \n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.2)\n    \n    # Comment out the plt.show() call:\n    plt.show()\n    \n    # Save the plot\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    end_time = time.time()\n    print(f\"Cost surface subplots created in {end_time - start_time:.2f} seconds\")\n</pre> def create_cost_surface_subplots(dataset_info, cost_arrays, transforms, evacuation_paths,                                 summit_coords, safe_zone_distances, hiking_gdf, output_path):     \"\"\"     Create visualizations of cost surfaces with evacuation paths for two terrain scenarios.          This function generates a figure with two cost surface maps showing optimal evacuation     paths from the summit to different safe zone distances. The maps include distance contours,     hiking paths, and the summit location. Cost surfaces use a logarithmic color scale to     better visualize travel time differences.          Parameters:     -----------     dataset_info : dict         Dictionary containing information about each dataset, including:         - keys: Dataset names (e.g., 'final', 'modify_landcover')         - values: Dictionaries with dataset metadata          cost_arrays : list         List of numpy arrays containing the cost surfaces for each dataset.          transforms : list         List of affine transforms corresponding to each cost array.          evacuation_paths : dict         Dictionary of evacuation paths for each dataset and safe zone:         {dataset_key: {safe_zone_distance: [path_coordinates]}}         where path_coordinates is a list of (row, col) tuples.          summit_coords : dict         Dictionary mapping dataset keys to summit coordinates as (row, col) tuples.          safe_zone_distances : list         List of safe zone distances to plot paths for.          hiking_gdf : geopandas.GeoDataFrame         GeoDataFrame containing hiking trail geometries to overlay on the maps.          output_path : str         File path where the figure will be saved.          Returns:     --------     None         The function saves the figure to the specified output path but does not return a value.          Notes:     ------     - Progress and timing information is printed to standard output.     - A logarithmic transformation is applied to the cost surfaces to enhance visualization.     - The figure includes a color bar indicating the travel time.     - Evacuation paths are color-coded by safe zone distance.     - Contour lines show distance from the summit.     - Each subplot is labeled with a letter (A, B) in the top-left corner.     \"\"\"     print(\"\\nCreating cost surface subplots...\")     start_time = time.time()          # Create figure with larger size      fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4          # Create subplot grid with space for legend and colorbar     gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)     ax1 = fig.add_subplot(gs[0, 0])     ax2 = fig.add_subplot(gs[0, 1])          # Create a common colormap and normalization     cmap = plt.cm.RdYlBu_r.copy()     cmap.set_bad('white', alpha=0)          epsilon = 0.1     max_log_cost = np.log1p(10 + epsilon)          # Find global vmin and vmax for consistent color scaling     vmin = float('inf')     vmax = float('-inf')     for cost_array in cost_arrays:         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)         vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))         vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))     vcenter = vmin + (vmax - vmin) / 2          norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)          # Colors for paths     colors_paths = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange', 'red', 'blue', 'purple']          # Plot both subplots     for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):         cost_array = cost_arrays[idx]         transform = transforms[idx]                  # Process cost array         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)                  # Plot cost surface         im = ax.imshow(             cost_array_masked, cmap=cmap, norm=norm,             extent=[                 transform[2],                  transform[2] + transform[0] * cost_array.shape[1],                 transform[5] + transform[4] * cost_array.shape[0],                 transform[5]             ]         )                  # Plot evacuation paths         for i, distance in enumerate(safe_zone_distances):             if distance in evacuation_paths[ds_key]:                 path = evacuation_paths[ds_key][distance]                 if path:                     path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]                     xs, ys = zip(*path_coords)                     ax.plot(xs, ys, '-', color=colors_paths[i],                              linewidth=3, label=f'{distance}m safe zone',                             alpha=1.0, zorder=5)                  # Plot hiking trail with outline effect         hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)         ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')         plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])                  # Plot summit         summit_x, summit_y = raster_coord_to_map_coords(             summit_coords[ds_key][0], summit_coords[ds_key][1], transform         )         ax.plot(summit_x, summit_y, '*', color='yellow', markersize=15, label='Summit', zorder=6)                  # Add distance contours         x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])         y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])         X, Y = np.meshgrid(x, y)         distances = np.sqrt((X - summit_x)**2 + (Y - summit_y)**2)                  contours = ax.contour(X, Y, distances, levels=safe_zone_distances,                               colors='black', linestyles='--', alpha=0.4,                               linewidths=1.5, zorder=3)         ax.clabel(contours, inline=True, fmt='%1.0fm', fontsize=8)                  # Set plot limits         ax.set_xlim(summit_x - 5000, summit_x + 5000)         ax.set_ylim(summit_y - 5000, summit_y + 5000)                  # Add simple A/B labels         ax.text(0.02, 0.98, f'{chr(65+idx)}',                 transform=ax.transAxes, fontsize=10, fontweight='bold',                 verticalalignment='top')                  # Add axis labels         ax.set_xlabel('Easting (m)')         ax.set_ylabel('Northing (m)')          # Add legend     handles, labels = ax1.get_legend_handles_labels()     unique_labels = []     unique_handles = []     seen_labels = set()     for handle, label in zip(handles, labels):         if label not in seen_labels:             seen_labels.add(label)             unique_labels.append(label)             unique_handles.append(handle)          ax2.legend(unique_handles, unique_labels,                 loc='center left',                bbox_to_anchor=(1.0, 0.5))          # Add horizontal colorbar     cbar_ax = fig.add_subplot(gs[1, :])     cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')     cbar.set_label('Accumulated Cost', labelpad=10)          cbar.set_ticks(np.linspace(vmin, vmax, 6))     cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])          plt.tight_layout()     plt.subplots_adjust(hspace=0.2)          # Comment out the plt.show() call:     plt.show()          # Save the plot     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.close()          end_time = time.time()     print(f\"Cost surface subplots created in {end_time - start_time:.2f} seconds\") In\u00a0[\u00a0]: Copied! <pre>def create_decomposition_table(decomp_data, output_path):\n    \"\"\"\n    Create a visualization table showing the relative contributions of slope and landcover factors.\n    \n    This function takes decomposition analysis results and creates a formatted table\n    showing the percentage contribution of slope and landcover factors to the\n    evacuation path costs for different safe zone thresholds.\n    \n    Parameters:\n    -----------\n    decomp_data : list of dict\n        A list of dictionaries, each containing decomposition results for a safe zone:\n        {\n            \"Safe Zone Threshold (m)\": int,\n            \"Slope Contribution (%)\": float,\n            \"Landcover Contribution (%)\": float\n        }\n    \n    output_path : str\n        File path where the table visualization will be saved as an image.\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        A DataFrame containing the decomposition data, useful for further analysis.\n    \n    Notes:\n    ------\n    - The function creates a visualization of the table using matplotlib.\n    - The table has a formatted header row and is styled for readability.\n    - The table is saved as an image at the specified output path.\n    \"\"\"\n    df = pd.DataFrame(decomp_data)\n    \n    # Create a figure for the table\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.axis('off')\n    \n    # Create the table from the DataFrame\n    table = ax.table(\n        cellText=df.values,\n        colLabels=df.columns,\n        cellLoc='center',\n        loc='center'\n    )\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.4)\n    \n    # Optionally style header row\n    for key, cell in table.get_celld().items():\n        row, col = key\n        if row == 0:\n            cell.set_text_props(weight='bold')\n            cell.set_facecolor('#e6e6e6')\n    \n    plt.tight_layout()\n    plt.show()\n    # Save the figure\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close(fig)\n    \n    return df\n</pre> def create_decomposition_table(decomp_data, output_path):     \"\"\"     Create a visualization table showing the relative contributions of slope and landcover factors.          This function takes decomposition analysis results and creates a formatted table     showing the percentage contribution of slope and landcover factors to the     evacuation path costs for different safe zone thresholds.          Parameters:     -----------     decomp_data : list of dict         A list of dictionaries, each containing decomposition results for a safe zone:         {             \"Safe Zone Threshold (m)\": int,             \"Slope Contribution (%)\": float,             \"Landcover Contribution (%)\": float         }          output_path : str         File path where the table visualization will be saved as an image.          Returns:     --------     pandas.DataFrame         A DataFrame containing the decomposition data, useful for further analysis.          Notes:     ------     - The function creates a visualization of the table using matplotlib.     - The table has a formatted header row and is styled for readability.     - The table is saved as an image at the specified output path.     \"\"\"     df = pd.DataFrame(decomp_data)          # Create a figure for the table     fig, ax = plt.subplots(figsize=(6, 4))     ax.axis('off')          # Create the table from the DataFrame     table = ax.table(         cellText=df.values,         colLabels=df.columns,         cellLoc='center',         loc='center'     )     table.auto_set_font_size(False)     table.set_fontsize(9)     table.scale(1.2, 1.4)          # Optionally style header row     for key, cell in table.get_celld().items():         row, col = key         if row == 0:             cell.set_text_props(weight='bold')             cell.set_facecolor('#e6e6e6')          plt.tight_layout()     plt.show()     # Save the figure     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.close(fig)          return df In\u00a0[\u00a0]: Copied! <pre>def create_final_evacuation_table(all_results, source_names, output_path):\n    \"\"\"\n    Create a comprehensive evacuation time table for different sources and walking speeds.\n    \n    This function generates a formatted table visualization showing evacuation times\n    for different combinations of safe zone distances, source locations, and walking speeds.\n    The table uses a multi-level column organization for clear presentation.\n    \n    Parameters:\n    -----------\n    all_results : dict\n        A nested dictionary containing travel time results:\n        {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}\n        The function uses only the 'final' dataset from this dictionary.\n    \n    source_names : list\n        A list of source location names corresponding to indices in the results arrays.\n        Must include 'summit', 'camp1', and 'camp2'.\n    \n    output_path : str\n        File path where the table visualization will be saved as an image.\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        A DataFrame with multi-level columns containing the evacuation time data.\n    \n    Notes:\n    ------\n    - The function selects only specific safe zone distances: 500, 1500, 2500, 3500, 4500m.\n    - The table uses a three-level header structure:\n      1. \"Safe Zone (m)\" / \"Evacuation Time (hours)\"\n      2. Source names (Summit, Camp1, Camp2)\n      3. Walking speeds (Slow, Moderate, Fast)\n    - NaN values are handled appropriately in the output.\n    - The table is styled with a bold header row and saved as an image.\n    \"\"\"\n    \n    # We only want to display these safe zones and these sources/speeds\n    safe_zones = [500, 1500, 2500, 3500, 4500]\n    sources_to_show = ['summit', 'camp1']\n    speeds = ['slow', 'medium', 'fast']\n    \n    # Prepare the table rows\n    table_data = []\n    for sz in safe_zones:\n        # First column is the safe zone distance\n        row = [sz]\n        for src in sources_to_show:\n            for spd in speeds:\n                # Find the index of this source in source_names\n                src_idx = source_names.index(src)\n                # Get the travel time from all_results['final']\n                val = all_results['final'][spd][sz][src_idx]\n                if np.isnan(val):\n                    row.append(\"Nan\")\n                else:\n                    row.append(f\"{val:.2f}\")\n        table_data.append(row)\n    \n    # We will have 1 column for safe zone + 3 sources \u00d7 3 speeds = 1 + 9 = 10 columns total.\n    # Build multi-level columns:\n    #  Top row: \"Safe Zone (m)\" repeated once, then \"Evacuation Time (hours)\" repeated 9 times\n    #  Middle row: \"\", \"Summit\" \u00d7 3, \"Camp1\" \u00d7 3, \"Camp2\" \u00d7 3\n    #  Bottom row: \"\", \"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\" repeated 3 times\n    \n    arrays = [\n        [\"Safe Zone (m)\"] + [\"Evacuation Time (hours)\"] * 9,\n        [\"\"]\n        + [\"Summit\"]*3\n        + [\"Camp1\"]*3,\n        [\"\"]\n        + [\"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\"]*3\n    ]\n    # Convert these into a MultiIndex\n    tuples = list(zip(*arrays))  # Transpose\n    columns = pd.MultiIndex.from_tuples(tuples)\n    \n    # Create a DataFrame\n    df = pd.DataFrame(table_data, columns=columns)\n    \n    # Now plot the DataFrame as a matplotlib table\n    fig, ax = plt.subplots(figsize=(12, 3))  # Wider figure for more columns\n    ax.axis('off')\n    \n    # Convert df to a 2D list (cellText) plus colLabels\n    # However, we have a MultiIndex. We'll create a table with two header rows:\n    # an approach is to flatten the columns for colLabels, but let's do a quick approach:\n    \n    # We can manually set up the table by using the DataFrame's .values for cellText\n    # and the multi-level columns as a header.\n    # For a nicely formatted multi-level header in matplotlib, it's a bit tricky,\n    # so a simpler approach is to just let DataFrame do the \"pretty printing\" into a single row.\n    \n    # Easiest is to do:\n    table = ax.table(\n        cellText=df.values,\n        colLabels=df.columns.to_flat_index(),  # Flatten the multiindex for a single row of headers\n        cellLoc='center',\n        loc='center'\n    )\n    \n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.4)\n    \n    # Optionally style the top row\n    for (row, col), cell in table.get_celld().items():\n        if row == 0:\n            cell.set_text_props(weight='bold')\n            cell.set_facecolor('#e6e6e6')\n    \n    # Show the table inline\n    plt.tight_layout()\n    plt.show()\n    \n    # Save the figure\n    fig.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close(fig)\n    \n    return df\n</pre> def create_final_evacuation_table(all_results, source_names, output_path):     \"\"\"     Create a comprehensive evacuation time table for different sources and walking speeds.          This function generates a formatted table visualization showing evacuation times     for different combinations of safe zone distances, source locations, and walking speeds.     The table uses a multi-level column organization for clear presentation.          Parameters:     -----------     all_results : dict         A nested dictionary containing travel time results:         {dataset_key: {speed_name: {safe_zone_distance: [times_per_source]}}}         The function uses only the 'final' dataset from this dictionary.          source_names : list         A list of source location names corresponding to indices in the results arrays.         Must include 'summit', 'camp1', and 'camp2'.          output_path : str         File path where the table visualization will be saved as an image.          Returns:     --------     pandas.DataFrame         A DataFrame with multi-level columns containing the evacuation time data.          Notes:     ------     - The function selects only specific safe zone distances: 500, 1500, 2500, 3500, 4500m.     - The table uses a three-level header structure:       1. \"Safe Zone (m)\" / \"Evacuation Time (hours)\"       2. Source names (Summit, Camp1, Camp2)       3. Walking speeds (Slow, Moderate, Fast)     - NaN values are handled appropriately in the output.     - The table is styled with a bold header row and saved as an image.     \"\"\"          # We only want to display these safe zones and these sources/speeds     safe_zones = [500, 1500, 2500, 3500, 4500]     sources_to_show = ['summit', 'camp1']     speeds = ['slow', 'medium', 'fast']          # Prepare the table rows     table_data = []     for sz in safe_zones:         # First column is the safe zone distance         row = [sz]         for src in sources_to_show:             for spd in speeds:                 # Find the index of this source in source_names                 src_idx = source_names.index(src)                 # Get the travel time from all_results['final']                 val = all_results['final'][spd][sz][src_idx]                 if np.isnan(val):                     row.append(\"Nan\")                 else:                     row.append(f\"{val:.2f}\")         table_data.append(row)          # We will have 1 column for safe zone + 3 sources \u00d7 3 speeds = 1 + 9 = 10 columns total.     # Build multi-level columns:     #  Top row: \"Safe Zone (m)\" repeated once, then \"Evacuation Time (hours)\" repeated 9 times     #  Middle row: \"\", \"Summit\" \u00d7 3, \"Camp1\" \u00d7 3, \"Camp2\" \u00d7 3     #  Bottom row: \"\", \"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\" repeated 3 times          arrays = [         [\"Safe Zone (m)\"] + [\"Evacuation Time (hours)\"] * 9,         [\"\"]         + [\"Summit\"]*3         + [\"Camp1\"]*3,         [\"\"]         + [\"Slow walking speed\", \"Moderate walking speed\", \"Fast walking speed\"]*3     ]     # Convert these into a MultiIndex     tuples = list(zip(*arrays))  # Transpose     columns = pd.MultiIndex.from_tuples(tuples)          # Create a DataFrame     df = pd.DataFrame(table_data, columns=columns)          # Now plot the DataFrame as a matplotlib table     fig, ax = plt.subplots(figsize=(12, 3))  # Wider figure for more columns     ax.axis('off')          # Convert df to a 2D list (cellText) plus colLabels     # However, we have a MultiIndex. We'll create a table with two header rows:     # an approach is to flatten the columns for colLabels, but let's do a quick approach:          # We can manually set up the table by using the DataFrame's .values for cellText     # and the multi-level columns as a header.     # For a nicely formatted multi-level header in matplotlib, it's a bit tricky,     # so a simpler approach is to just let DataFrame do the \"pretty printing\" into a single row.          # Easiest is to do:     table = ax.table(         cellText=df.values,         colLabels=df.columns.to_flat_index(),  # Flatten the multiindex for a single row of headers         cellLoc='center',         loc='center'     )          table.auto_set_font_size(False)     table.set_fontsize(9)     table.scale(1.2, 1.4)          # Optionally style the top row     for (row, col), cell in table.get_celld().items():         if row == 0:             cell.set_text_props(weight='bold')             cell.set_facecolor('#e6e6e6')          # Show the table inline     plt.tight_layout()     plt.show()          # Save the figure     fig.savefig(output_path, dpi=300, bbox_inches='tight')     plt.close(fig)          return df"},{"location":"source-code/probability-analysis/data-utils.html","title":"data_utils.py","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nData loading and file operation utilities for the volcanic evacuation analysis.\n\"\"\"\n\nimport os\nimport numpy as np\nimport rasterio\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nimport csv\n\n\ndef read_shapefile(path):\n    \"\"\"\n    Read a shapefile into a GeoDataFrame.\n    \n    Args:\n        path (str): Path to the shapefile\n        \n    Returns:\n        gpd.GeoDataFrame: GeoDataFrame containing the shapefile data\n    \"\"\"\n    with fiona.open(path) as src:\n        return gpd.GeoDataFrame.from_features(src)\n\n\ndef read_raster(path):\n    \"\"\"\n    Read a raster file and return its data, metadata, and properties.\n    \n    Args:\n        path (str): Path to the raster file\n        \n    Returns:\n        tuple: (data, meta, transform, nodata, bounds, resolution)\n    \"\"\"\n    with rasterio.open(path) as src:\n        data = src.read()\n        meta = src.meta.copy()\n        transform = src.transform\n        nodata = src.nodata\n        bounds = src.bounds\n        resolution = src.res\n    return data, meta, transform, nodata, bounds, resolution\n\n\ndef load_raster(file_path):\n    \"\"\"\n    Load a raster file and return its data and metadata.\n    \n    Args:\n        file_path (str): Path to the raster file\n        \n    Returns:\n        tuple: (data, meta)\n    \"\"\"\n    with rasterio.open(file_path) as src:\n        data = src.read(1)  # Read the first band\n        meta = src.meta.copy()\n    return data, meta\n\n\ndef save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):\n    \"\"\"\n    Save a numpy array as a raster file.\n    \n    Args:\n        output_path (str): Path to save the raster\n        data (numpy.ndarray): The raster data\n        meta (dict): The raster metadata\n        dtype: The data type of the output raster\n        nodata: The NoData value\n    \"\"\"\n    meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)\n    with rasterio.open(output_path, 'w', **meta) as dst:\n        dst.write(data, 1)\n\n\ndef save_analysis_report(results, min_coords, source_names, thresholds, walking_speeds, dataset_key, output_dir):\n    \"\"\"\n    Save analysis results to a text report and CSV file.\n    \n    Args:\n        results (dict): Dictionary of results\n        min_coords (dict): Dictionary of coordinates\n        source_names (list): List of source names\n        thresholds (list): List of probability thresholds\n        walking_speeds (dict): Dictionary of walking speeds\n        dataset_key (str): Key for the dataset\n        output_dir (str): Directory to save outputs\n    \"\"\"\n    # Save text report\n    output_report_path = os.path.join(output_dir, f\"eruption_probability_travel_time_report_{dataset_key}.txt\")\n    with open(output_report_path, 'w') as f:\n        f.write(\"Eruption Probability Travel Time Analysis Report\\n\")\n        f.write(\"=================================================\\n\\n\")\n        for speed_name in walking_speeds.keys():\n            f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")\n            f.write(\"-\" * 40 + \"\\n\")\n            for thresh in thresholds:\n                f.write(f\"\\nEruption Probability Threshold: {thresh}\\n\")\n                for idx, source_name in enumerate(source_names):\n                    tt = results[speed_name][thresh][idx]\n                    coords = min_coords[speed_name][thresh][idx]\n                    f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\")\n    \n    # Save CSV file\n    output_csv_path = os.path.join(output_dir, f\"eruption_probability_travel_time_metrics_{dataset_key}.csv\")\n    with open(output_csv_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Walking_Speed', 'Probability_Threshold', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])\n        for speed_name in walking_speeds.keys():\n            for thresh in thresholds:\n                for idx, source_name in enumerate(source_names):\n                    writer.writerow([\n                        speed_name,\n                        thresh,\n                        source_name,\n                        results[speed_name][thresh][idx],\n                        min_coords[speed_name][thresh][idx]\n                    ])\n    \n    return output_report_path, output_csv_path\n\n\ndef create_statistics_table(results, source_names, walking_speeds, thresholds, dataset_key, output_dir):\n    \"\"\"\n    Create a statistics table and save it as PNG and CSV.\n    \n    Args:\n        results (dict): Dictionary of results\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        thresholds (list): List of probability thresholds\n        dataset_key (str): Key for the dataset\n        output_dir (str): Directory to save outputs\n        \n    Returns:\n        tuple: Paths to the saved PNG and CSV files\n    \"\"\"\n    import matplotlib.pyplot as plt\n    \n    stats_data = []\n    for source_name in source_names:\n        i = source_names.index(source_name)\n        for metric in ['Min', 'Max', 'Mean']:\n            row_data = {\n                'Source': source_name,\n                'Metric': f'{metric} (hours)'\n            }\n            for speed_name, speed_val in walking_speeds.items():\n                times = [results[speed_name][thresh][i] for thresh in thresholds]\n                times = [t for t in times if not np.isnan(t)]\n                if len(times) == 0:\n                    value = np.nan\n                else:\n                    if metric == 'Min':\n                        value = min(times)\n                    elif metric == 'Max':\n                        value = max(times)\n                    else:\n                        value = np.mean(times)\n                row_data[f'{speed_name.capitalize()} ({speed_val} m/s)'] = f\"{value:.2f}\" if not np.isnan(value) else \"NaN\"\n            stats_data.append(row_data)\n    \n    df_stats = pd.DataFrame(stats_data)\n    plt.figure(figsize=(12, 8))\n    plt.axis('tight')\n    plt.axis('off')\n    table = plt.table(\n        cellText=df_stats.values,\n        colLabels=df_stats.columns,\n        cellLoc='center',\n        loc='center',\n        colColours=['#e6e6e6']*len(df_stats.columns)\n    )\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.5)\n    plt.title(f'Travel Time Statistics by Source and Walking Speed\\n[Dataset: {dataset_key}]', pad=3, y=0.85)\n    \n    # Save as PNG\n    table_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.png\")\n    plt.savefig(table_output_path, dpi=300, bbox_inches='tight', pad_inches=0.2)\n    plt.close()\n    \n    # Save as CSV\n    csv_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.csv\")\n    df_stats.to_csv(csv_output_path, index=False)\n    \n    return table_output_path, csv_output_path\n</pre> \"\"\" Data loading and file operation utilities for the volcanic evacuation analysis. \"\"\"  import os import numpy as np import rasterio import geopandas as gpd import fiona import pandas as pd import csv   def read_shapefile(path):     \"\"\"     Read a shapefile into a GeoDataFrame.          Args:         path (str): Path to the shapefile              Returns:         gpd.GeoDataFrame: GeoDataFrame containing the shapefile data     \"\"\"     with fiona.open(path) as src:         return gpd.GeoDataFrame.from_features(src)   def read_raster(path):     \"\"\"     Read a raster file and return its data, metadata, and properties.          Args:         path (str): Path to the raster file              Returns:         tuple: (data, meta, transform, nodata, bounds, resolution)     \"\"\"     with rasterio.open(path) as src:         data = src.read()         meta = src.meta.copy()         transform = src.transform         nodata = src.nodata         bounds = src.bounds         resolution = src.res     return data, meta, transform, nodata, bounds, resolution   def load_raster(file_path):     \"\"\"     Load a raster file and return its data and metadata.          Args:         file_path (str): Path to the raster file              Returns:         tuple: (data, meta)     \"\"\"     with rasterio.open(file_path) as src:         data = src.read(1)  # Read the first band         meta = src.meta.copy()     return data, meta   def save_raster(output_path, data, meta, dtype=rasterio.float32, nodata=-1):     \"\"\"     Save a numpy array as a raster file.          Args:         output_path (str): Path to save the raster         data (numpy.ndarray): The raster data         meta (dict): The raster metadata         dtype: The data type of the output raster         nodata: The NoData value     \"\"\"     meta.update(dtype=dtype, count=1, compress='lzw', nodata=nodata)     with rasterio.open(output_path, 'w', **meta) as dst:         dst.write(data, 1)   def save_analysis_report(results, min_coords, source_names, thresholds, walking_speeds, dataset_key, output_dir):     \"\"\"     Save analysis results to a text report and CSV file.          Args:         results (dict): Dictionary of results         min_coords (dict): Dictionary of coordinates         source_names (list): List of source names         thresholds (list): List of probability thresholds         walking_speeds (dict): Dictionary of walking speeds         dataset_key (str): Key for the dataset         output_dir (str): Directory to save outputs     \"\"\"     # Save text report     output_report_path = os.path.join(output_dir, f\"eruption_probability_travel_time_report_{dataset_key}.txt\")     with open(output_report_path, 'w') as f:         f.write(\"Eruption Probability Travel Time Analysis Report\\n\")         f.write(\"=================================================\\n\\n\")         for speed_name in walking_speeds.keys():             f.write(f\"\\nWalking Speed: {speed_name} ({walking_speeds[speed_name]} m/s)\\n\")             f.write(\"-\" * 40 + \"\\n\")             for thresh in thresholds:                 f.write(f\"\\nEruption Probability Threshold: {thresh}\\n\")                 for idx, source_name in enumerate(source_names):                     tt = results[speed_name][thresh][idx]                     coords = min_coords[speed_name][thresh][idx]                     f.write(f\"{source_name}: Travel time = {tt:.2f} hrs at cell {coords}\\n\")          # Save CSV file     output_csv_path = os.path.join(output_dir, f\"eruption_probability_travel_time_metrics_{dataset_key}.csv\")     with open(output_csv_path, 'w', newline='') as f:         writer = csv.writer(f)         writer.writerow(['Walking_Speed', 'Probability_Threshold', 'Source', 'Min_Travel_Time (hrs)', 'Min_Coords (row,col)'])         for speed_name in walking_speeds.keys():             for thresh in thresholds:                 for idx, source_name in enumerate(source_names):                     writer.writerow([                         speed_name,                         thresh,                         source_name,                         results[speed_name][thresh][idx],                         min_coords[speed_name][thresh][idx]                     ])          return output_report_path, output_csv_path   def create_statistics_table(results, source_names, walking_speeds, thresholds, dataset_key, output_dir):     \"\"\"     Create a statistics table and save it as PNG and CSV.          Args:         results (dict): Dictionary of results         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         thresholds (list): List of probability thresholds         dataset_key (str): Key for the dataset         output_dir (str): Directory to save outputs              Returns:         tuple: Paths to the saved PNG and CSV files     \"\"\"     import matplotlib.pyplot as plt          stats_data = []     for source_name in source_names:         i = source_names.index(source_name)         for metric in ['Min', 'Max', 'Mean']:             row_data = {                 'Source': source_name,                 'Metric': f'{metric} (hours)'             }             for speed_name, speed_val in walking_speeds.items():                 times = [results[speed_name][thresh][i] for thresh in thresholds]                 times = [t for t in times if not np.isnan(t)]                 if len(times) == 0:                     value = np.nan                 else:                     if metric == 'Min':                         value = min(times)                     elif metric == 'Max':                         value = max(times)                     else:                         value = np.mean(times)                 row_data[f'{speed_name.capitalize()} ({speed_val} m/s)'] = f\"{value:.2f}\" if not np.isnan(value) else \"NaN\"             stats_data.append(row_data)          df_stats = pd.DataFrame(stats_data)     plt.figure(figsize=(12, 8))     plt.axis('tight')     plt.axis('off')     table = plt.table(         cellText=df_stats.values,         colLabels=df_stats.columns,         cellLoc='center',         loc='center',         colColours=['#e6e6e6']*len(df_stats.columns)     )     table.auto_set_font_size(False)     table.set_fontsize(9)     table.scale(1.2, 1.5)     plt.title(f'Travel Time Statistics by Source and Walking Speed\\n[Dataset: {dataset_key}]', pad=3, y=0.85)          # Save as PNG     table_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.png\")     plt.savefig(table_output_path, dpi=300, bbox_inches='tight', pad_inches=0.2)     plt.close()          # Save as CSV     csv_output_path = os.path.join(output_dir, f\"travel_time_statistics_by_source_{dataset_key}.csv\")     df_stats.to_csv(csv_output_path, index=False)          return table_output_path, csv_output_path"},{"location":"source-code/probability-analysis/graph-utils.html","title":"graph_utils.py","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nGraph construction, shortest path calculation, and path analysis utilities.\n\"\"\"\n\nimport time\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csr_matrix\nfrom scipy.sparse.csgraph import dijkstra\nfrom tqdm import tqdm\nfrom numpy import sqrt\n\nfrom raster_utils import to_1d\n\n\ndef build_adjacency_matrix(cost_array, rows, cols):\n    \"\"\"\n    Build an adjacency matrix (graph) from a cost raster.\n    \n    Args:\n        cost_array (numpy.ndarray): Cost array (bands, rows, cols)\n        rows (int): Number of rows\n        cols (int): Number of columns\n        \n    Returns:\n        scipy.sparse.csr_matrix: CSR format adjacency matrix\n    \"\"\"\n    print(\"Building adjacency matrix...\")\n    graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)\n    \n    # Define 8 movement directions and map them to band indices\n    directions = [\n        (-1, 0),   # Up\n        (1, 0),    # Down\n        (0, 1),    # Right\n        (0, -1),   # Left\n        (-1, 1),   # Up-Right\n        (-1, -1),  # Up-Left\n        (1, 1),    # Down-Right\n        (1, -1)    # Down-Left\n    ]\n    direction_indices = {dir: idx for idx, dir in enumerate(directions)}\n    \n    for r in tqdm(range(rows), desc=\"Processing rows\"):\n        for c in range(cols):\n            current_node = to_1d(r, c, cols)\n            for dr, dc in directions:\n                nr, nc = r + dr, c + dc\n                if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:\n                    neighbor_node = to_1d(nr, nc, cols)\n                    idx = direction_indices[(dr, dc)]\n                    cost_val = cost_array[idx, r, c]\n                    \n                    # Skip invalid costs\n                    if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:\n                        continue\n                    \n                    # Apply sqrt(2) factor for diagonal movement\n                    if abs(dr) == 1 and abs(dc) == 1:\n                        cost_val *= sqrt(2)\n                    \n                    graph[current_node, neighbor_node] = cost_val\n    \n    # Convert to CSR format for faster computation\n    graph_csr = graph.tocsr()\n    print(\"Adjacency matrix created.\")\n    \n    return graph_csr\n\n\ndef compute_shortest_paths(graph_csr, source_nodes):\n    \"\"\"\n    Compute shortest paths from source nodes using Dijkstra's algorithm.\n    \n    Args:\n        graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix\n        source_nodes (list): List of source node indices\n        \n    Returns:\n        tuple: (distances, predecessors)\n    \"\"\"\n    print(\"Running Dijkstra's algorithm for all sources...\")\n    start_time = time.time()\n    distances, predecessors = dijkstra(\n        csgraph=graph_csr, \n        directed=True, \n        indices=source_nodes, \n        return_predecessors=True\n    )\n    end_time = time.time()\n    print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds.\")\n    \n    return distances, predecessors\n\n\ndef reconstruct_path(predecessors, source_node, target_node, cols):\n    \"\"\"\n    Reconstruct the path from source_node to target_node using the predecessor array.\n    \n    Args:\n        predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm\n        source_node (int): Source node index\n        target_node (int): Target node index\n        cols (int): Number of columns in the raster\n        \n    Returns:\n        list: List of (row, col) coordinates for the path\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return []\n    \n    path_nodes = []\n    current = target_node\n    \n    while current != source_node and current != -9999:\n        path_nodes.append(current)\n        current = predecessors[current]\n    \n    path_nodes.append(source_node)\n    path_nodes.reverse()\n    \n    # Convert each 1D index to (row, col)\n    return [(node // cols, node % cols) for node in path_nodes]\n\n\ndef calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):\n    \"\"\"\n    Calculate metrics for a path between source and target nodes.\n    \n    Args:\n        predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm\n        source_node (int): Source node index\n        target_node (int): Target node index\n        graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix\n        rows (int): Number of rows in the raster\n        cols (int): Number of columns in the raster\n        \n    Returns:\n        tuple: (pixel_count, cell_costs, total_cost)\n    \"\"\"\n    if target_node == -9999 or np.isnan(target_node):\n        return 0, [], 0\n    \n    path = []\n    current = target_node\n    \n    while current != source_node and current != -9999:\n        path.append(current)\n        current = predecessors[current]\n    \n    if current != -9999:\n        path.append(source_node)\n    \n    path.reverse()\n    pixel_count = len(path)\n    \n    cell_costs = []\n    total_cost = 0\n    \n    for i in range(len(path)-1):\n        cell_cost = graph_csr[path[i], path[i+1]]\n        cell_costs.append(cell_cost)\n        total_cost += cell_cost\n    \n    return pixel_count, cell_costs, total_cost\n</pre> \"\"\" Graph construction, shortest path calculation, and path analysis utilities. \"\"\"  import time import numpy as np from scipy.sparse import lil_matrix, csr_matrix from scipy.sparse.csgraph import dijkstra from tqdm import tqdm from numpy import sqrt  from raster_utils import to_1d   def build_adjacency_matrix(cost_array, rows, cols):     \"\"\"     Build an adjacency matrix (graph) from a cost raster.          Args:         cost_array (numpy.ndarray): Cost array (bands, rows, cols)         rows (int): Number of rows         cols (int): Number of columns              Returns:         scipy.sparse.csr_matrix: CSR format adjacency matrix     \"\"\"     print(\"Building adjacency matrix...\")     graph = lil_matrix((rows * cols, rows * cols), dtype=np.float32)          # Define 8 movement directions and map them to band indices     directions = [         (-1, 0),   # Up         (1, 0),    # Down         (0, 1),    # Right         (0, -1),   # Left         (-1, 1),   # Up-Right         (-1, -1),  # Up-Left         (1, 1),    # Down-Right         (1, -1)    # Down-Left     ]     direction_indices = {dir: idx for idx, dir in enumerate(directions)}          for r in tqdm(range(rows), desc=\"Processing rows\"):         for c in range(cols):             current_node = to_1d(r, c, cols)             for dr, dc in directions:                 nr, nc = r + dr, c + dc                 if 0 &lt;= nr &lt; rows and 0 &lt;= nc &lt; cols:                     neighbor_node = to_1d(nr, nc, cols)                     idx = direction_indices[(dr, dc)]                     cost_val = cost_array[idx, r, c]                                          # Skip invalid costs                     if np.isnan(cost_val) or np.isinf(cost_val) or cost_val &lt;= 0:                         continue                                          # Apply sqrt(2) factor for diagonal movement                     if abs(dr) == 1 and abs(dc) == 1:                         cost_val *= sqrt(2)                                          graph[current_node, neighbor_node] = cost_val          # Convert to CSR format for faster computation     graph_csr = graph.tocsr()     print(\"Adjacency matrix created.\")          return graph_csr   def compute_shortest_paths(graph_csr, source_nodes):     \"\"\"     Compute shortest paths from source nodes using Dijkstra's algorithm.          Args:         graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix         source_nodes (list): List of source node indices              Returns:         tuple: (distances, predecessors)     \"\"\"     print(\"Running Dijkstra's algorithm for all sources...\")     start_time = time.time()     distances, predecessors = dijkstra(         csgraph=graph_csr,          directed=True,          indices=source_nodes,          return_predecessors=True     )     end_time = time.time()     print(f\"Dijkstra's algorithm completed in {end_time - start_time:.2f} seconds.\")          return distances, predecessors   def reconstruct_path(predecessors, source_node, target_node, cols):     \"\"\"     Reconstruct the path from source_node to target_node using the predecessor array.          Args:         predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm         source_node (int): Source node index         target_node (int): Target node index         cols (int): Number of columns in the raster              Returns:         list: List of (row, col) coordinates for the path     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return []          path_nodes = []     current = target_node          while current != source_node and current != -9999:         path_nodes.append(current)         current = predecessors[current]          path_nodes.append(source_node)     path_nodes.reverse()          # Convert each 1D index to (row, col)     return [(node // cols, node % cols) for node in path_nodes]   def calculate_path_metrics(predecessors, source_node, target_node, graph_csr, rows, cols):     \"\"\"     Calculate metrics for a path between source and target nodes.          Args:         predecessors (numpy.ndarray): Predecessor array from Dijkstra's algorithm         source_node (int): Source node index         target_node (int): Target node index         graph_csr (scipy.sparse.csr_matrix): CSR format adjacency matrix         rows (int): Number of rows in the raster         cols (int): Number of columns in the raster              Returns:         tuple: (pixel_count, cell_costs, total_cost)     \"\"\"     if target_node == -9999 or np.isnan(target_node):         return 0, [], 0          path = []     current = target_node          while current != source_node and current != -9999:         path.append(current)         current = predecessors[current]          if current != -9999:         path.append(source_node)          path.reverse()     pixel_count = len(path)          cell_costs = []     total_cost = 0          for i in range(len(path)-1):         cell_cost = graph_csr[path[i], path[i+1]]         cell_costs.append(cell_cost)         total_cost += cell_cost          return pixel_count, cell_costs, total_cost"},{"location":"source-code/probability-analysis/prob_visualization.html","title":"visualization.py","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nModified plotting and visualization functions for the volcanic evacuation analysis.\nHandles multiple VEI levels and creates separate contour maps.\n\"\"\"\n</pre> \"\"\" Modified plotting and visualization functions for the volcanic evacuation analysis. Handles multiple VEI levels and creates separate contour maps. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors as mcolors\nfrom matplotlib.patheffects import withStroke\nimport rasterio\nfrom rasterio.warp import reproject, Resampling\nfrom pyproj import CRS, Transformer\nimport contextily as ctx\nfrom shapely.geometry import box\nimport geopandas as gpd\n</pre> import os import numpy as np import matplotlib.pyplot as plt from matplotlib import colors as mcolors from matplotlib.patheffects import withStroke import rasterio from rasterio.warp import reproject, Resampling from pyproj import CRS, Transformer import contextily as ctx from shapely.geometry import box import geopandas as gpd In\u00a0[\u00a0]: Copied! <pre>from raster_utils import raster_coord_to_map_coords\nfrom data_utils import load_raster, read_shapefile\n</pre> from raster_utils import raster_coord_to_map_coords from data_utils import load_raster, read_shapefile In\u00a0[\u00a0]: Copied! <pre>def plot_travel_time_comparison(all_results, source_names, thresholds, walking_speeds, output_dir, filename=\"comparison_travel_time_by_source.png\"):\n    \"\"\"\n    Plot a comparison of travel times between datasets.\n    \n    Args:\n        all_results (dict): Dictionary of results from all datasets\n        source_names (list): List of source names\n        thresholds (list): List of probability thresholds\n        walking_speeds (dict): Dictionary of walking speeds\n        output_dir (str): Directory to save the plot\n        filename (str): Filename for the plot\n        \n    Returns:\n        str: Path to the saved plot\n    \"\"\"\n    dataset_styles = {\n        'final': {'label': 'Original', 'linestyle': '-', 'alpha': 0.9},\n        'modify_landcover': {'label': 'Penalized', 'linestyle': '--', 'alpha': 0.9}\n    }\n    \n    speed_markers = {'slow': 'o', 'medium': 's', 'fast': '^'}\n    speed_colors = {'slow': 'red', 'medium': 'blue', 'fast': 'green'}\n    \n    # Create figure with one row, two columns\n    n_sources = min(len(source_names), 2)  # Limit to first two sources\n    fig, axes = plt.subplots(1, n_sources, figsize=(16, 6), sharey=True)\n    \n    # Handle case of single subplot\n    if n_sources == 1:\n        axes = [axes]\n    \n    # Store lines and labels for legend\n    lines = []\n    labels = []\n    first_plot = True\n    \n    # Plot for each source\n    for src_idx, source_name in enumerate(source_names[:n_sources]):\n        ax = axes[src_idx]\n        \n        # Plot all walking speeds for both original and modified landcover\n        for speed_name, speed_color in speed_colors.items():\n            # Get data for both datasets\n            original_times = [all_results['final'][speed_name][thresh][src_idx] for thresh in thresholds]\n            modified_times = [all_results['modify_landcover'][speed_name][thresh][src_idx] for thresh in thresholds]\n            \n            # Plot with consistent line styles\n            line1 = ax.plot(thresholds, original_times, '-', color=speed_color)\n            line2 = ax.plot(thresholds, modified_times, '--', color=speed_color)\n            \n            # Store legend entries only once\n            if first_plot:\n                lines.extend([line1[0], line2[0]])\n                labels.extend([f'{speed_name.capitalize()} - Original', \n                             f'{speed_name.capitalize()} - Penalized'])\n        \n        # Add label in the top left corner\n        ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes, \n                fontsize=14, fontweight='bold', va='top')\n        \n        # Invert the x-axis to show high probabilities first\n        ax.invert_xaxis()\n        \n        ax.set_xlabel(\"Eruption Probability Threshold\", fontsize=10)\n        if src_idx == 0:  # Add ylabel only for leftmost plot\n            ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)\n        ax.grid(True, alpha=0.3)\n        \n        # Set y-axis to go from 0 to 2.5\n        ax.set_ylim(bottom=0, top=2.5)\n        \n        first_plot = False\n    \n    # Add legend to the right of the second subplot\n    fig.legend(lines, labels, fontsize=9, loc='center left', \n              bbox_to_anchor=(1.02, 0.5))\n    \n    plt.tight_layout()\n    \n    # Save the plot\n    output_path = os.path.join(output_dir, filename)\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show() \n    plt.close()\n    return output_path\n</pre> def plot_travel_time_comparison(all_results, source_names, thresholds, walking_speeds, output_dir, filename=\"comparison_travel_time_by_source.png\"):     \"\"\"     Plot a comparison of travel times between datasets.          Args:         all_results (dict): Dictionary of results from all datasets         source_names (list): List of source names         thresholds (list): List of probability thresholds         walking_speeds (dict): Dictionary of walking speeds         output_dir (str): Directory to save the plot         filename (str): Filename for the plot              Returns:         str: Path to the saved plot     \"\"\"     dataset_styles = {         'final': {'label': 'Original', 'linestyle': '-', 'alpha': 0.9},         'modify_landcover': {'label': 'Penalized', 'linestyle': '--', 'alpha': 0.9}     }          speed_markers = {'slow': 'o', 'medium': 's', 'fast': '^'}     speed_colors = {'slow': 'red', 'medium': 'blue', 'fast': 'green'}          # Create figure with one row, two columns     n_sources = min(len(source_names), 2)  # Limit to first two sources     fig, axes = plt.subplots(1, n_sources, figsize=(16, 6), sharey=True)          # Handle case of single subplot     if n_sources == 1:         axes = [axes]          # Store lines and labels for legend     lines = []     labels = []     first_plot = True          # Plot for each source     for src_idx, source_name in enumerate(source_names[:n_sources]):         ax = axes[src_idx]                  # Plot all walking speeds for both original and modified landcover         for speed_name, speed_color in speed_colors.items():             # Get data for both datasets             original_times = [all_results['final'][speed_name][thresh][src_idx] for thresh in thresholds]             modified_times = [all_results['modify_landcover'][speed_name][thresh][src_idx] for thresh in thresholds]                          # Plot with consistent line styles             line1 = ax.plot(thresholds, original_times, '-', color=speed_color)             line2 = ax.plot(thresholds, modified_times, '--', color=speed_color)                          # Store legend entries only once             if first_plot:                 lines.extend([line1[0], line2[0]])                 labels.extend([f'{speed_name.capitalize()} - Original',                               f'{speed_name.capitalize()} - Penalized'])                  # Add label in the top left corner         ax.text(0.05, 0.95, chr(65 + src_idx), transform=ax.transAxes,                  fontsize=14, fontweight='bold', va='top')                  # Invert the x-axis to show high probabilities first         ax.invert_xaxis()                  ax.set_xlabel(\"Eruption Probability Threshold\", fontsize=10)         if src_idx == 0:  # Add ylabel only for leftmost plot             ax.set_ylabel(\"Minimum Travel Time (hrs)\", fontsize=10)         ax.grid(True, alpha=0.3)                  # Set y-axis to go from 0 to 2.5         ax.set_ylim(bottom=0, top=2.5)                  first_plot = False          # Add legend to the right of the second subplot     fig.legend(lines, labels, fontsize=9, loc='center left',                bbox_to_anchor=(1.02, 0.5))          plt.tight_layout()          # Save the plot     output_path = os.path.join(output_dir, filename)     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.show()      plt.close()     return output_path In\u00a0[\u00a0]: Copied! <pre>def plot_cost_surface_with_paths(dataset_info, evacuation_paths, eruption_probability_path, \n                                  hiking_path, selected_speed, thresholds, output_dir, vei_label=\"VEI4\"):\n    \"\"\"\n    Plot cost surface with evacuation paths.\n    \n    Args:\n        dataset_info (dict): Dictionary of dataset information\n        evacuation_paths (dict): Dictionary of evacuation paths\n        eruption_probability_path (str): Path to the eruption probability raster\n        hiking_path (str): Path to the hiking path shapefile\n        selected_speed (str): Selected walking speed\n        thresholds (list): List of probability thresholds\n        output_dir (str): Directory to save the plot\n        vei_label (str): VEI label for the title\n        \n    Returns:\n        str: Path to the saved plot\n    \"\"\"\n    print(f\"\\nCreating cost surface subplots with evacuation paths for {vei_label}...\")\n    \n    # Set up colors for each probability threshold\n    threshold_colors = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange']\n    \n    # Load hiking trail shapefile\n    hiking_gdf = read_shapefile(hiking_path)\n    \n    # Load cost rasters for visualization\n    cost_arrays = []\n    transforms = []\n    for ds_key in dataset_info.keys():\n        cost_raster_path = os.path.join(output_dir, f'cost_distance_summit_{ds_key}_{selected_speed}_hours.tif')\n        cost_array, cost_meta = load_raster(cost_raster_path)\n        cost_arrays.append(cost_array)\n        transforms.append(cost_meta['transform'])\n    \n    # Create figure with larger size \n    fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4\n    \n    # Create subplot grid with space for legend and colorbar\n    gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    \n    # Create a common colormap and normalization\n    cmap = plt.cm.RdYlBu_r.copy()\n    cmap.set_bad('white', alpha=0)\n    \n    epsilon = 0.1\n    max_log_cost = np.log1p(10 + epsilon)\n    \n    # Find global vmin and vmax for consistent color scaling\n    vmin = float('inf')\n    vmax = float('-inf')\n    for cost_array in cost_arrays:\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))\n        vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))\n    vcenter = vmin + (vmax - vmin) / 2\n    \n    norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)\n    \n    # Add title for the overall figure\n    fig.suptitle(f'Contour Map Showing Summit Location and Hiking Trail for {vei_label}', fontsize=12)\n    \n    # Plot both subplots\n    for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):\n        cost_array = cost_arrays[idx]\n        transform = transforms[idx]\n        \n        # Process cost array\n        cost_array_log = np.log1p(cost_array + epsilon)\n        cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost\n        cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)\n        \n        # Plot cost surface\n        im = ax.imshow(\n            cost_array_masked, cmap=cmap, norm=norm,\n            extent=[\n                transform[2], \n                transform[2] + transform[0] * cost_array.shape[1],\n                transform[5] + transform[4] * cost_array.shape[0],\n                transform[5]\n            ]\n        )\n        \n        # Plot evacuation paths\n        for i, thresh in enumerate(thresholds):\n            if thresh in evacuation_paths[ds_key]:\n                path = evacuation_paths[ds_key][thresh]\n                if path:\n                    path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]\n                    xs, ys = zip(*path_coords)\n                    ax.plot(xs, ys, '-', color=threshold_colors[i % len(threshold_colors)], \n                            linewidth=3, label=f'Prob \u2264 {thresh}',\n                            alpha=1.0, zorder=5)\n        \n        # Plot hiking trail with outline effect\n        hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)\n        ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')\n        plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])\n        \n        # Plot summit\n        summit_rc = dataset_info[ds_key][\"summit_raster_coords\"]\n        summit_x, summit_y = raster_coord_to_map_coords(\n            summit_rc[0], summit_rc[1], transform\n        )\n        ax.plot(summit_x, summit_y, '*', color='blue', markersize=15, label='Summit', zorder=6)\n        \n        # Add probability contours\n        x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])\n        y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])\n        X, Y = np.meshgrid(x, y)\n        \n        # Load eruption probability raster\n        with rasterio.open(eruption_probability_path) as src:\n            prob_array = src.read(1)\n            prob_transform = src.transform\n        \n        # Ensure probability array matches cost array dimensions\n        if prob_array.shape != cost_array.shape:\n            print(f\"Reshaping probability array for {vei_label} to match cost array dimensions...\")\n            new_prob_array = np.empty(cost_array.shape, dtype=np.float32)\n            reproject(\n                source=prob_array,\n                destination=new_prob_array,\n                src_transform=prob_transform,\n                dst_transform=transform,\n                src_crs=CRS.from_epsg(32751),\n                dst_crs=CRS.from_epsg(32751),\n                resampling=Resampling.bilinear\n            )\n            prob_array = new_prob_array\n        \n        # Define contour levels in ASCENDING order (critical)\n        contour_levels = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]  # Sorted in ascending order\n        \n        # Create contours for probability thresholds\n        contours = ax.contour(X, Y, prob_array, levels=contour_levels,\n                            colors='black', linestyles='--', alpha=0.7,\n                            linewidths=1.5, zorder=3)\n        ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)\n        \n        # Set plot limits based on summit location (adjust as needed for different VEI levels)\n        # For VEI3, use a smaller radius, for VEI5 use a larger radius\n        if vei_label == 'VEI3':\n            radius = 3500\n        elif vei_label == 'VEI5':\n            radius = 7000\n        else:  # VEI4 default\n            radius = 5000\n            \n        ax.set_xlim(summit_x - radius, summit_x + radius)\n        ax.set_ylim(summit_y - radius, summit_y + radius)\n        \n        # Add legend with blue Summit star and red Hiking Path\n        ax.legend(loc='upper right', fontsize=8)\n        \n        # Add axis labels\n        ax.set_xlabel('Easting (m)')\n        ax.set_ylabel('Northing (m)')\n    \n    # Add horizontal colorbar\n    cbar_ax = fig.add_subplot(gs[1, :])\n    cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n    cbar.set_label('Accumulated Cost', labelpad=10)\n    \n    cbar.set_ticks(np.linspace(vmin, vmax, 6))\n    cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])\n    \n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.2, top=0.92)  # Make room for suptitle\n    \n    # Save the plot\n    output_path = os.path.join(output_dir, f'contour_map_showing_summit_location_and_hiking_trail_for_{vei_label}.png')\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show() \n    plt.close()\n    print(f\"Cost surface visualization with evacuation paths for {vei_label} saved to: {output_path}\")\n    return output_path\n</pre> def plot_cost_surface_with_paths(dataset_info, evacuation_paths, eruption_probability_path,                                    hiking_path, selected_speed, thresholds, output_dir, vei_label=\"VEI4\"):     \"\"\"     Plot cost surface with evacuation paths.          Args:         dataset_info (dict): Dictionary of dataset information         evacuation_paths (dict): Dictionary of evacuation paths         eruption_probability_path (str): Path to the eruption probability raster         hiking_path (str): Path to the hiking path shapefile         selected_speed (str): Selected walking speed         thresholds (list): List of probability thresholds         output_dir (str): Directory to save the plot         vei_label (str): VEI label for the title              Returns:         str: Path to the saved plot     \"\"\"     print(f\"\\nCreating cost surface subplots with evacuation paths for {vei_label}...\")          # Set up colors for each probability threshold     threshold_colors = ['lime', 'cyan', 'yellow', 'silver', 'pink', 'orange']          # Load hiking trail shapefile     hiking_gdf = read_shapefile(hiking_path)          # Load cost rasters for visualization     cost_arrays = []     transforms = []     for ds_key in dataset_info.keys():         cost_raster_path = os.path.join(output_dir, f'cost_distance_summit_{ds_key}_{selected_speed}_hours.tif')         cost_array, cost_meta = load_raster(cost_raster_path)         cost_arrays.append(cost_array)         transforms.append(cost_meta['transform'])          # Create figure with larger size      fig = plt.figure(figsize=(11.69, 8.27))  # Landscape A4          # Create subplot grid with space for legend and colorbar     gs = plt.GridSpec(2, 2, height_ratios=[0.9, 0.1], hspace=0.1)     ax1 = fig.add_subplot(gs[0, 0])     ax2 = fig.add_subplot(gs[0, 1])          # Create a common colormap and normalization     cmap = plt.cm.RdYlBu_r.copy()     cmap.set_bad('white', alpha=0)          epsilon = 0.1     max_log_cost = np.log1p(10 + epsilon)          # Find global vmin and vmax for consistent color scaling     vmin = float('inf')     vmax = float('-inf')     for cost_array in cost_arrays:         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)         vmin = min(vmin, np.min(cost_array_log[~cost_array_masked.mask]))         vmax = max(vmax, np.max(cost_array_log[~cost_array_masked.mask]))     vcenter = vmin + (vmax - vmin) / 2          norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=vcenter, vmax=vmax)          # Add title for the overall figure     fig.suptitle(f'Contour Map Showing Summit Location and Hiking Trail for {vei_label}', fontsize=12)          # Plot both subplots     for idx, (ax, ds_key) in enumerate(zip([ax1, ax2], dataset_info.keys())):         cost_array = cost_arrays[idx]         transform = transforms[idx]                  # Process cost array         cost_array_log = np.log1p(cost_array + epsilon)         cost_array_log[cost_array_log &gt; max_log_cost] = max_log_cost         cost_array_masked = np.ma.masked_where(cost_array &lt;= 0, cost_array_log)                  # Plot cost surface         im = ax.imshow(             cost_array_masked, cmap=cmap, norm=norm,             extent=[                 transform[2],                  transform[2] + transform[0] * cost_array.shape[1],                 transform[5] + transform[4] * cost_array.shape[0],                 transform[5]             ]         )                  # Plot evacuation paths         for i, thresh in enumerate(thresholds):             if thresh in evacuation_paths[ds_key]:                 path = evacuation_paths[ds_key][thresh]                 if path:                     path_coords = [raster_coord_to_map_coords(row, col, transform) for row, col in path]                     xs, ys = zip(*path_coords)                     ax.plot(xs, ys, '-', color=threshold_colors[i % len(threshold_colors)],                              linewidth=3, label=f'Prob \u2264 {thresh}',                             alpha=1.0, zorder=5)                  # Plot hiking trail with outline effect         hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.5, zorder=4)         ax.plot([], [], color='red', linewidth=2.5, label='Hiking Path')         plt.setp(hiking_trail, path_effects=[withStroke(linewidth=4, foreground='gray')])                  # Plot summit         summit_rc = dataset_info[ds_key][\"summit_raster_coords\"]         summit_x, summit_y = raster_coord_to_map_coords(             summit_rc[0], summit_rc[1], transform         )         ax.plot(summit_x, summit_y, '*', color='blue', markersize=15, label='Summit', zorder=6)                  # Add probability contours         x = np.linspace(transform[2], transform[2] + transform[0] * cost_array.shape[1], cost_array.shape[1])         y = np.linspace(transform[5] + transform[4] * cost_array.shape[0], transform[5], cost_array.shape[0])         X, Y = np.meshgrid(x, y)                  # Load eruption probability raster         with rasterio.open(eruption_probability_path) as src:             prob_array = src.read(1)             prob_transform = src.transform                  # Ensure probability array matches cost array dimensions         if prob_array.shape != cost_array.shape:             print(f\"Reshaping probability array for {vei_label} to match cost array dimensions...\")             new_prob_array = np.empty(cost_array.shape, dtype=np.float32)             reproject(                 source=prob_array,                 destination=new_prob_array,                 src_transform=prob_transform,                 dst_transform=transform,                 src_crs=CRS.from_epsg(32751),                 dst_crs=CRS.from_epsg(32751),                 resampling=Resampling.bilinear             )             prob_array = new_prob_array                  # Define contour levels in ASCENDING order (critical)         contour_levels = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]  # Sorted in ascending order                  # Create contours for probability thresholds         contours = ax.contour(X, Y, prob_array, levels=contour_levels,                             colors='black', linestyles='--', alpha=0.7,                             linewidths=1.5, zorder=3)         ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)                  # Set plot limits based on summit location (adjust as needed for different VEI levels)         # For VEI3, use a smaller radius, for VEI5 use a larger radius         if vei_label == 'VEI3':             radius = 3500         elif vei_label == 'VEI5':             radius = 7000         else:  # VEI4 default             radius = 5000                      ax.set_xlim(summit_x - radius, summit_x + radius)         ax.set_ylim(summit_y - radius, summit_y + radius)                  # Add legend with blue Summit star and red Hiking Path         ax.legend(loc='upper right', fontsize=8)                  # Add axis labels         ax.set_xlabel('Easting (m)')         ax.set_ylabel('Northing (m)')          # Add horizontal colorbar     cbar_ax = fig.add_subplot(gs[1, :])     cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')     cbar.set_label('Accumulated Cost', labelpad=10)          cbar.set_ticks(np.linspace(vmin, vmax, 6))     cbar.set_ticklabels([f'{np.expm1(v):.1f}' for v in np.linspace(vmin, vmax, 6)])          plt.tight_layout()     plt.subplots_adjust(hspace=0.2, top=0.92)  # Make room for suptitle          # Save the plot     output_path = os.path.join(output_dir, f'contour_map_showing_summit_location_and_hiking_trail_for_{vei_label}.png')     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.show()      plt.close()     print(f\"Cost surface visualization with evacuation paths for {vei_label} saved to: {output_path}\")     return output_path In\u00a0[\u00a0]: Copied! <pre>def create_vei_comparison_plot(eruption_probability_paths, hiking_path, summit_path, output_dir):\n    \"\"\"\n    Create a comparison plot of contour maps for different VEI levels side by side.\n    \n    Args:\n        eruption_probability_paths (dict): Dictionary of VEI levels and their file paths\n        hiking_path (str): Path to the hiking path shapefile\n        summit_path (str): Path to the summit point shapefile\n        output_dir (str): Directory to save the plot\n        \n    Returns:\n        str: Path to the saved plot\n    \"\"\"\n    print(\"\\nCreating VEI comparison plot...\")\n    \n    # Load hiking trail shapefile\n    hiking_gdf = read_shapefile(hiking_path)\n    \n    # Load summit points\n    summit_gdf = read_shapefile(summit_path)\n    \n    # Create figure with subplots for each VEI level\n    fig, axes = plt.subplots(1, len(eruption_probability_paths), figsize=(18, 6), sharey=True)\n    \n    # If only one VEI level, convert axes to list\n    if len(eruption_probability_paths) == 1:\n        axes = [axes]\n    \n    # Define eruption probability thresholds - CRITICAL: Must be in ASCENDING order for matplotlib contour\n    thresholds = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\n    \n    # Process each VEI level\n    for i, (vei, path) in enumerate(eruption_probability_paths.items()):\n        ax = axes[i]\n        \n        # Load eruption probability raster\n        with rasterio.open(path) as src:\n            prob_array = src.read(1)\n            transform = src.transform\n            bounds = src.bounds\n            \n            # Get CRS information for the raster\n            raster_crs = src.crs\n            \n            # Create coordinate grid\n            x = np.linspace(bounds.left, bounds.right, prob_array.shape[1])\n            y = np.linspace(bounds.bottom, bounds.top, prob_array.shape[0])\n            X, Y = np.meshgrid(x, y)\n            \n            # Get summit coordinates\n            summit_x = summit_gdf.geometry.x.values[0]\n            summit_y = summit_gdf.geometry.y.values[0]\n            \n            # Set radius based on VEI level\n            if vei == 'VEI3':\n                radius = 3500\n            elif vei == 'VEI5':\n                radius = 7000\n            else:  # VEI4 default\n                radius = 5000\n            \n            # Create a bounding box for the area around the summit\n            bbox = box(summit_x - radius, summit_y - radius, \n                      summit_x + radius, summit_y + radius)\n            \n            # Create a GeoDataFrame from the bounding box\n            bbox_gdf = gpd.GeoDataFrame({'geometry': [bbox]}, crs=raster_crs)\n            \n            # Create a light blue background for the map\n            ax.imshow(np.ones(prob_array.shape), \n                     extent=[bounds.left, bounds.right, bounds.bottom, bounds.top],\n                     cmap='Blues', alpha=0.1, vmin=0, vmax=1)\n            \n            # Plot the OpenStreetMap tiles (wrapped in try/except in case of connection issues)\n            try:\n                ctx.add_basemap(ax, crs=raster_crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n                print(f\"Successfully added OpenStreetMap background for {vei}\")\n            except Exception as e:\n                print(f\"Warning: Could not add OpenStreetMap basemap for {vei}. Error: {e}\")\n                # Continue without the basemap\n            \n            # Create contours for probability thresholds (already in ascending order)\n            contours = ax.contour(X, Y, prob_array, levels=thresholds,\n                                colors='black', linestyles='--', alpha=0.7,\n                                linewidths=1.0)\n            ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)\n            \n            # Plot hiking trail\n            hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.0)\n            \n            # Plot summit\n            ax.plot(summit_x, summit_y, '*', color='blue', markersize=12)\n            \n            # Set title\n            ax.set_title(f'Contour Map Showing Summit Location and Hiking Trail for {vei}', fontsize=10)\n            \n            # Set plot limits\n            ax.set_xlim(summit_x - radius, summit_x + radius)\n            ax.set_ylim(summit_y - radius, summit_y + radius)\n            \n            # Add axis labels\n            ax.set_xlabel('Easting (m)')\n            if i == 0:  # Only add y-label for the first subplot\n                ax.set_ylabel('Northing (m)')\n            \n            # Add legend\n            ax.plot([], [], '*', color='blue', markersize=12, label='Summit')\n            ax.plot([], [], '-', color='red', linewidth=2.0, label='Hiking Path')\n            ax.legend(loc='upper right', fontsize=8)\n    \n    plt.tight_layout()\n    \n    # Save the plot\n    output_path = os.path.join(output_dir, 'vei_comparison_contour_maps.png')\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()\n    print(f\"VEI comparison plot saved to: {output_path}\")\n    return output_path\n</pre> def create_vei_comparison_plot(eruption_probability_paths, hiking_path, summit_path, output_dir):     \"\"\"     Create a comparison plot of contour maps for different VEI levels side by side.          Args:         eruption_probability_paths (dict): Dictionary of VEI levels and their file paths         hiking_path (str): Path to the hiking path shapefile         summit_path (str): Path to the summit point shapefile         output_dir (str): Directory to save the plot              Returns:         str: Path to the saved plot     \"\"\"     print(\"\\nCreating VEI comparison plot...\")          # Load hiking trail shapefile     hiking_gdf = read_shapefile(hiking_path)          # Load summit points     summit_gdf = read_shapefile(summit_path)          # Create figure with subplots for each VEI level     fig, axes = plt.subplots(1, len(eruption_probability_paths), figsize=(18, 6), sharey=True)          # If only one VEI level, convert axes to list     if len(eruption_probability_paths) == 1:         axes = [axes]          # Define eruption probability thresholds - CRITICAL: Must be in ASCENDING order for matplotlib contour     thresholds = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9]          # Process each VEI level     for i, (vei, path) in enumerate(eruption_probability_paths.items()):         ax = axes[i]                  # Load eruption probability raster         with rasterio.open(path) as src:             prob_array = src.read(1)             transform = src.transform             bounds = src.bounds                          # Get CRS information for the raster             raster_crs = src.crs                          # Create coordinate grid             x = np.linspace(bounds.left, bounds.right, prob_array.shape[1])             y = np.linspace(bounds.bottom, bounds.top, prob_array.shape[0])             X, Y = np.meshgrid(x, y)                          # Get summit coordinates             summit_x = summit_gdf.geometry.x.values[0]             summit_y = summit_gdf.geometry.y.values[0]                          # Set radius based on VEI level             if vei == 'VEI3':                 radius = 3500             elif vei == 'VEI5':                 radius = 7000             else:  # VEI4 default                 radius = 5000                          # Create a bounding box for the area around the summit             bbox = box(summit_x - radius, summit_y - radius,                        summit_x + radius, summit_y + radius)                          # Create a GeoDataFrame from the bounding box             bbox_gdf = gpd.GeoDataFrame({'geometry': [bbox]}, crs=raster_crs)                          # Create a light blue background for the map             ax.imshow(np.ones(prob_array.shape),                       extent=[bounds.left, bounds.right, bounds.bottom, bounds.top],                      cmap='Blues', alpha=0.1, vmin=0, vmax=1)                          # Plot the OpenStreetMap tiles (wrapped in try/except in case of connection issues)             try:                 ctx.add_basemap(ax, crs=raster_crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)                 print(f\"Successfully added OpenStreetMap background for {vei}\")             except Exception as e:                 print(f\"Warning: Could not add OpenStreetMap basemap for {vei}. Error: {e}\")                 # Continue without the basemap                          # Create contours for probability thresholds (already in ascending order)             contours = ax.contour(X, Y, prob_array, levels=thresholds,                                 colors='black', linestyles='--', alpha=0.7,                                 linewidths=1.0)             ax.clabel(contours, inline=True, fmt='%1.2f', fontsize=8)                          # Plot hiking trail             hiking_trail = hiking_gdf.plot(ax=ax, color='red', linewidth=2.0)                          # Plot summit             ax.plot(summit_x, summit_y, '*', color='blue', markersize=12)                          # Set title             ax.set_title(f'Contour Map Showing Summit Location and Hiking Trail for {vei}', fontsize=10)                          # Set plot limits             ax.set_xlim(summit_x - radius, summit_x + radius)             ax.set_ylim(summit_y - radius, summit_y + radius)                          # Add axis labels             ax.set_xlabel('Easting (m)')             if i == 0:  # Only add y-label for the first subplot                 ax.set_ylabel('Northing (m)')                          # Add legend             ax.plot([], [], '*', color='blue', markersize=12, label='Summit')             ax.plot([], [], '-', color='red', linewidth=2.0, label='Hiking Path')             ax.legend(loc='upper right', fontsize=8)          plt.tight_layout()          # Save the plot     output_path = os.path.join(output_dir, 'vei_comparison_contour_maps.png')     plt.savefig(output_path, dpi=300, bbox_inches='tight')     plt.show()     plt.close()     print(f\"VEI comparison plot saved to: {output_path}\")     return output_path"},{"location":"source-code/probability-analysis/probability_analysis.html","title":"analysis.py","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nEvacuation path analysis, safe zone calculations, and related functionality.\n\"\"\"\n\nimport os\nimport numpy as np\nimport rasterio\n\nfrom data_utils import load_raster, save_raster, save_analysis_report, create_statistics_table\nfrom raster_utils import process_raster, to_1d\nfrom graph_utils import build_adjacency_matrix, compute_shortest_paths\n\n\ndef perform_evacuation_analysis(cost_paths, source_coords, source_names, walking_speeds, output_dir):\n    \"\"\"\n    Perform evacuation analysis for multiple cost datasets and walking speeds.\n    \n    Args:\n        cost_paths (dict): Dictionary of cost raster paths\n        source_coords (list): List of source (row, col) coordinates\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        output_dir (str): Directory to save outputs\n        \n    Returns:\n        tuple: (all_results, dataset_info)\n    \"\"\"\n    # Dictionary to store results\n    all_results = {}\n    dataset_info = {}\n    \n    # Process each cost dataset\n    for dataset_key, current_cost_path in cost_paths.items():\n        print(f\"\\nProcessing cost dataset: {dataset_key}\")\n        \n        # Read the cost raster\n        cost_array, cost_meta, transform, cost_nodata, raster_bounds, resolution = read_raster(current_cost_path)\n        print(f\"Raster shape: {cost_array.shape}\")\n        \n        # Get the dimensions of the raster\n        bands, rows, cols = cost_array.shape\n        \n        # Convert 2D source coordinates to 1D node indices\n        source_nodes = [to_1d(r, c, cols) for (r, c) in source_coords]\n        print(\"Source Nodes (1D):\", source_nodes)\n        \n        # Build the adjacency matrix\n        graph_csr = build_adjacency_matrix(cost_array, rows, cols)\n        \n        # Compute shortest paths\n        distances, predecessors = compute_shortest_paths(graph_csr, source_nodes)\n        \n        # Save the base cost distance rasters\n        for i, source_name in enumerate(source_names):\n            source_distance = distances[i, :].reshape(rows, cols)\n            source_distance[np.isinf(source_distance)] = -1\n            out_filename = f'cost_distance_{source_name}_{dataset_key}.tif'\n            out_path = os.path.join(output_dir, out_filename)\n            save_raster(out_path, source_distance, cost_meta, dtype=rasterio.float32, nodata=-1)\n            print(f\"Saved base cost distance raster for {source_name}: {out_filename}\")\n        \n        # Convert to travel time for each walking speed\n        for speed_name, speed_value in walking_speeds.items():\n            print(f\"\\nProcessing travel time rasters for walking speed '{speed_name}' ({speed_value} m/s)...\")\n            \n            for source_name in source_names:\n                base_raster_filename = f'cost_distance_{source_name}_{dataset_key}.tif'\n                base_raster_path = os.path.join(output_dir, base_raster_filename)\n                \n                cost_array_base, cost_meta_base = load_raster(base_raster_path)\n                travel_time_array = process_raster(cost_array_base, speed_value)\n                \n                out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n                out_path = os.path.join(output_dir, out_filename)\n                \n                save_raster(out_path, travel_time_array, cost_meta_base, dtype=rasterio.float32, nodata=-1)\n                print(f\"Saved travel time raster for {source_name} at speed '{speed_name}': {out_filename}\")\n        \n        # Store info for this dataset\n        summit_idx = 0  # Assuming summit is always the first source\n        dataset_info[dataset_key] = {\n            \"pred_summit\": predecessors[summit_idx],\n            \"rows\": rows,\n            \"cols\": cols,\n            \"transform\": transform,\n            \"summit_raster_coords\": source_coords[summit_idx]\n        }\n        \n        print(f\"Processing for dataset '{dataset_key}' complete!\")\n    \n    return all_results, dataset_info\n\n\ndef analyze_safe_zones(probability_path, travel_time_data, thresholds, source_names, walking_speeds, dataset_key, output_dir):\n    \"\"\"\n    Analyze safe zones based on eruption probability thresholds.\n    \n    Args:\n        probability_path (str): Path to the eruption probability raster\n        travel_time_data (dict): Dictionary of travel time data\n        thresholds (list): List of probability thresholds\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        dataset_key (str): Key for the dataset\n        output_dir (str): Directory to save outputs\n        \n    Returns:\n        tuple: (results, min_coords)\n    \"\"\"\n    print(\"\\nPerforming safe zone analysis based on eruption probability thresholds...\")\n    \n    # Load the eruption probability raster\n    with rasterio.open(probability_path) as src:\n        probability_array = src.read(1)\n    \n    # Initialize result dictionaries\n    results = {speed_name: {} for speed_name in walking_speeds.keys()}\n    min_coords = {speed_name: {} for speed_name in walking_speeds.keys()}\n    \n    # Analyze each walking speed\n    for speed_name in walking_speeds.keys():\n        print(f\"\\n--- Walking speed: {speed_name} ---\")\n        results[speed_name] = {}\n        \n        # Analyze each probability threshold\n        for thresh in thresholds:\n            print(f\"\\nEruption Probability Threshold: {thresh}\")\n            \n            # Flatten probability array for easier masking\n            probability_flat = probability_array.ravel()\n            \n            # Create safe zone mask where probability &lt;= threshold\n            safe_zone_mask = (probability_flat &lt;= thresh)\n            \n            min_times_in_zone = []\n            coords_in_zone = []\n            \n            # Find minimum travel time for each source\n            for source_name in source_names:\n                cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']\n                \n                # Find valid times within the safe zone\n                valid_times = cost_array_flat[safe_zone_mask]\n                valid_times = valid_times[~np.isnan(valid_times)]\n                \n                if len(valid_times) &gt; 0:\n                    min_time = np.min(valid_times)\n                    cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']\n                    safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)\n                    valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)\n                    min_idx = np.nanargmin(valid_times_2d)\n                    min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)\n                else:\n                    min_time = np.nan\n                    min_r, min_c = (np.nan, np.nan)\n                \n                min_times_in_zone.append(min_time)\n                coords_in_zone.append((min_r, min_c))\n                print(f\"{source_name}: min travel time = {min_time:.2f} hrs at cell ({min_r}, {min_c})\")\n            \n            results[speed_name][thresh] = min_times_in_zone\n            min_coords[speed_name][thresh] = coords_in_zone\n    \n    # Save analysis reports\n    report_path, csv_path = save_analysis_report(\n        results, min_coords, source_names, thresholds, \n        walking_speeds, dataset_key, output_dir\n    )\n    \n    # Create and save statistics table\n    table_path, stats_csv_path = create_statistics_table(\n        results, source_names, walking_speeds, \n        thresholds, dataset_key, output_dir\n    )\n    \n    print(f\"Saved analysis report: {report_path}\")\n    print(f\"Saved metrics CSV: {csv_path}\")\n    print(f\"Saved statistics table: {table_path}\")\n    print(f\"Saved statistics CSV: {stats_csv_path}\")\n    \n    return results, min_coords\n\n\ndef load_travel_time_data(dataset_key, source_names, walking_speeds, output_dir):\n    \"\"\"\n    Load travel time rasters for further analysis.\n    \n    Args:\n        dataset_key (str): Key for the dataset\n        source_names (list): List of source names\n        walking_speeds (dict): Dictionary of walking speeds\n        output_dir (str): Directory where rasters are stored\n        \n    Returns:\n        dict: Dictionary of travel time data\n    \"\"\"\n    travel_time_data = {speed_name: {} for speed_name in walking_speeds.keys()}\n    \n    for speed_name in walking_speeds.keys():\n        for source_name in source_names:\n            raster_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'\n            raster_path = os.path.join(output_dir, raster_filename)\n            \n            cost_array_hours, meta_hours = load_raster(raster_path)\n            rows_rt, cols_rt = cost_array_hours.shape\n            cost_array_hours = np.where(cost_array_hours == meta_hours['nodata'], np.nan, cost_array_hours)\n            \n            travel_time_data[speed_name][source_name] = {\n                'cost_array': cost_array_hours,\n                'meta': meta_hours,\n                'shape': (rows_rt, cols_rt),\n                'cost_array_flat': cost_array_hours.ravel()\n            }\n        \n        print(f\"Loaded travel time data for speed '{speed_name}'\")\n    \n    return travel_time_data\n\n\ndef read_raster(path):\n    \"\"\"\n    Read a raster file and return its data, metadata, and properties.\n    \n    Args:\n        path (str): Path to the raster file\n        \n    Returns:\n        tuple: (data, meta, transform, nodata, bounds, resolution)\n    \"\"\"\n    with rasterio.open(path) as src:\n        data = src.read()\n        meta = src.meta.copy()\n        transform = src.transform\n        nodata = src.nodata\n        bounds = src.bounds\n        resolution = src.res\n    return data, meta, transform, nodata, bounds, resolution\n</pre> \"\"\" Evacuation path analysis, safe zone calculations, and related functionality. \"\"\"  import os import numpy as np import rasterio  from data_utils import load_raster, save_raster, save_analysis_report, create_statistics_table from raster_utils import process_raster, to_1d from graph_utils import build_adjacency_matrix, compute_shortest_paths   def perform_evacuation_analysis(cost_paths, source_coords, source_names, walking_speeds, output_dir):     \"\"\"     Perform evacuation analysis for multiple cost datasets and walking speeds.          Args:         cost_paths (dict): Dictionary of cost raster paths         source_coords (list): List of source (row, col) coordinates         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         output_dir (str): Directory to save outputs              Returns:         tuple: (all_results, dataset_info)     \"\"\"     # Dictionary to store results     all_results = {}     dataset_info = {}          # Process each cost dataset     for dataset_key, current_cost_path in cost_paths.items():         print(f\"\\nProcessing cost dataset: {dataset_key}\")                  # Read the cost raster         cost_array, cost_meta, transform, cost_nodata, raster_bounds, resolution = read_raster(current_cost_path)         print(f\"Raster shape: {cost_array.shape}\")                  # Get the dimensions of the raster         bands, rows, cols = cost_array.shape                  # Convert 2D source coordinates to 1D node indices         source_nodes = [to_1d(r, c, cols) for (r, c) in source_coords]         print(\"Source Nodes (1D):\", source_nodes)                  # Build the adjacency matrix         graph_csr = build_adjacency_matrix(cost_array, rows, cols)                  # Compute shortest paths         distances, predecessors = compute_shortest_paths(graph_csr, source_nodes)                  # Save the base cost distance rasters         for i, source_name in enumerate(source_names):             source_distance = distances[i, :].reshape(rows, cols)             source_distance[np.isinf(source_distance)] = -1             out_filename = f'cost_distance_{source_name}_{dataset_key}.tif'             out_path = os.path.join(output_dir, out_filename)             save_raster(out_path, source_distance, cost_meta, dtype=rasterio.float32, nodata=-1)             print(f\"Saved base cost distance raster for {source_name}: {out_filename}\")                  # Convert to travel time for each walking speed         for speed_name, speed_value in walking_speeds.items():             print(f\"\\nProcessing travel time rasters for walking speed '{speed_name}' ({speed_value} m/s)...\")                          for source_name in source_names:                 base_raster_filename = f'cost_distance_{source_name}_{dataset_key}.tif'                 base_raster_path = os.path.join(output_dir, base_raster_filename)                                  cost_array_base, cost_meta_base = load_raster(base_raster_path)                 travel_time_array = process_raster(cost_array_base, speed_value)                                  out_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'                 out_path = os.path.join(output_dir, out_filename)                                  save_raster(out_path, travel_time_array, cost_meta_base, dtype=rasterio.float32, nodata=-1)                 print(f\"Saved travel time raster for {source_name} at speed '{speed_name}': {out_filename}\")                  # Store info for this dataset         summit_idx = 0  # Assuming summit is always the first source         dataset_info[dataset_key] = {             \"pred_summit\": predecessors[summit_idx],             \"rows\": rows,             \"cols\": cols,             \"transform\": transform,             \"summit_raster_coords\": source_coords[summit_idx]         }                  print(f\"Processing for dataset '{dataset_key}' complete!\")          return all_results, dataset_info   def analyze_safe_zones(probability_path, travel_time_data, thresholds, source_names, walking_speeds, dataset_key, output_dir):     \"\"\"     Analyze safe zones based on eruption probability thresholds.          Args:         probability_path (str): Path to the eruption probability raster         travel_time_data (dict): Dictionary of travel time data         thresholds (list): List of probability thresholds         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         dataset_key (str): Key for the dataset         output_dir (str): Directory to save outputs              Returns:         tuple: (results, min_coords)     \"\"\"     print(\"\\nPerforming safe zone analysis based on eruption probability thresholds...\")          # Load the eruption probability raster     with rasterio.open(probability_path) as src:         probability_array = src.read(1)          # Initialize result dictionaries     results = {speed_name: {} for speed_name in walking_speeds.keys()}     min_coords = {speed_name: {} for speed_name in walking_speeds.keys()}          # Analyze each walking speed     for speed_name in walking_speeds.keys():         print(f\"\\n--- Walking speed: {speed_name} ---\")         results[speed_name] = {}                  # Analyze each probability threshold         for thresh in thresholds:             print(f\"\\nEruption Probability Threshold: {thresh}\")                          # Flatten probability array for easier masking             probability_flat = probability_array.ravel()                          # Create safe zone mask where probability &lt;= threshold             safe_zone_mask = (probability_flat &lt;= thresh)                          min_times_in_zone = []             coords_in_zone = []                          # Find minimum travel time for each source             for source_name in source_names:                 cost_array_flat = travel_time_data[speed_name][source_name]['cost_array_flat']                                  # Find valid times within the safe zone                 valid_times = cost_array_flat[safe_zone_mask]                 valid_times = valid_times[~np.isnan(valid_times)]                                  if len(valid_times) &gt; 0:                     min_time = np.min(valid_times)                     cost_array_2d = travel_time_data[speed_name][source_name]['cost_array']                     safe_zone_mask_2d = safe_zone_mask.reshape(cost_array_2d.shape)                     valid_times_2d = np.where(safe_zone_mask_2d, cost_array_2d, np.nan)                     min_idx = np.nanargmin(valid_times_2d)                     min_r, min_c = np.unravel_index(min_idx, cost_array_2d.shape)                 else:                     min_time = np.nan                     min_r, min_c = (np.nan, np.nan)                                  min_times_in_zone.append(min_time)                 coords_in_zone.append((min_r, min_c))                 print(f\"{source_name}: min travel time = {min_time:.2f} hrs at cell ({min_r}, {min_c})\")                          results[speed_name][thresh] = min_times_in_zone             min_coords[speed_name][thresh] = coords_in_zone          # Save analysis reports     report_path, csv_path = save_analysis_report(         results, min_coords, source_names, thresholds,          walking_speeds, dataset_key, output_dir     )          # Create and save statistics table     table_path, stats_csv_path = create_statistics_table(         results, source_names, walking_speeds,          thresholds, dataset_key, output_dir     )          print(f\"Saved analysis report: {report_path}\")     print(f\"Saved metrics CSV: {csv_path}\")     print(f\"Saved statistics table: {table_path}\")     print(f\"Saved statistics CSV: {stats_csv_path}\")          return results, min_coords   def load_travel_time_data(dataset_key, source_names, walking_speeds, output_dir):     \"\"\"     Load travel time rasters for further analysis.          Args:         dataset_key (str): Key for the dataset         source_names (list): List of source names         walking_speeds (dict): Dictionary of walking speeds         output_dir (str): Directory where rasters are stored              Returns:         dict: Dictionary of travel time data     \"\"\"     travel_time_data = {speed_name: {} for speed_name in walking_speeds.keys()}          for speed_name in walking_speeds.keys():         for source_name in source_names:             raster_filename = f'cost_distance_{source_name}_{dataset_key}_{speed_name}_hours.tif'             raster_path = os.path.join(output_dir, raster_filename)                          cost_array_hours, meta_hours = load_raster(raster_path)             rows_rt, cols_rt = cost_array_hours.shape             cost_array_hours = np.where(cost_array_hours == meta_hours['nodata'], np.nan, cost_array_hours)                          travel_time_data[speed_name][source_name] = {                 'cost_array': cost_array_hours,                 'meta': meta_hours,                 'shape': (rows_rt, cols_rt),                 'cost_array_flat': cost_array_hours.ravel()             }                  print(f\"Loaded travel time data for speed '{speed_name}'\")          return travel_time_data   def read_raster(path):     \"\"\"     Read a raster file and return its data, metadata, and properties.          Args:         path (str): Path to the raster file              Returns:         tuple: (data, meta, transform, nodata, bounds, resolution)     \"\"\"     with rasterio.open(path) as src:         data = src.read()         meta = src.meta.copy()         transform = src.transform         nodata = src.nodata         bounds = src.bounds         resolution = src.res     return data, meta, transform, nodata, bounds, resolution"},{"location":"source-code/probability-analysis/raster-utils.html","title":"raster_utils.py","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nRaster operations and coordinate conversion utilities.\n\"\"\"\n\nimport numpy as np\nimport rasterio\nimport rasterio.warp\nfrom rasterio.transform import xy\n\n\ndef resample_raster(source_path, target_path, output_path, resampling_method=rasterio.warp.Resampling.bilinear):\n    \"\"\"\n    Resample a source raster to match the grid of a target raster.\n    \n    Args:\n        source_path (str): Path to the source raster\n        target_path (str): Path to the target raster (for grid reference)\n        output_path (str): Path to save the resampled raster\n        resampling_method: Resampling method to use\n        \n    Returns:\n        tuple: (resampled_array, resampled_meta)\n    \"\"\"\n    # Load source raster\n    with rasterio.open(source_path) as src:\n        source_array = src.read(1)  # Read the first band\n        source_meta = src.meta.copy()\n        source_nodata = src.nodata\n        source_transform = src.transform\n        source_crs = src.crs\n    \n    # Replace NoData values with np.nan\n    source_array = np.where(source_array == source_nodata, np.nan, source_array)\n    \n    # Get target grid properties\n    with rasterio.open(target_path) as src:\n        target_meta = src.meta.copy()\n        target_transform = src.transform\n        target_crs = src.crs\n        target_shape = src.shape\n    \n    # Set up the output metadata\n    resampled_meta = target_meta.copy()\n    resampled_meta.update({\n        'dtype': 'float32',\n        'count': 1,\n        'nodata': np.nan,\n        'driver': 'GTiff'\n    })\n    \n    # Create an empty array for the resampled data\n    resampled_array = np.empty(target_shape, dtype=np.float32)\n    \n    # Perform the resampling\n    rasterio.warp.reproject(\n        source=source_array,\n        destination=resampled_array,\n        src_transform=source_transform,\n        src_crs=source_crs,\n        dst_transform=target_transform,\n        dst_crs=target_crs,\n        resampling=resampling_method\n    )\n    \n    # Replace any remaining NoData values with np.nan\n    resampled_array = np.where(np.isnan(resampled_array), np.nan, resampled_array)\n    \n    # Save the resampled raster\n    with rasterio.open(output_path, 'w', **resampled_meta) as dst:\n        dst.write(resampled_array, 1)\n    \n    return resampled_array, resampled_meta\n\n\ndef coords_to_raster(gdf, transform, bounds, res):\n    \"\"\"\n    Convert geographic coordinates to raster row/column coordinates.\n    \n    Args:\n        gdf (GeoDataFrame): GeoDataFrame containing point geometries\n        transform: The raster's affine transform\n        bounds: The raster's bounds\n        res: The raster's resolution\n        \n    Returns:\n        list: List of (row, col) coordinates\n    \"\"\"\n    raster_coords = []\n    for point in gdf.geometry:\n        x, y = point.x, point.y\n        if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):\n            print(f\"Point {x}, {y} is out of raster bounds.\")\n            continue\n        col = int((x - bounds.left) / res[0])\n        row = int((bounds.top - y) / res[1])\n        raster_coords.append((row, col))\n    return raster_coords\n\n\ndef raster_coord_to_map_coords(row, col, transform):\n    \"\"\"\n    Convert raster (row, col) coordinates to map (x, y) coordinates.\n    \n    Args:\n        row (int): Raster row index\n        col (int): Raster column index\n        transform: Raster affine transform\n        \n    Returns:\n        tuple: (x, y) map coordinates\n    \"\"\"\n    x, y = xy(transform, row, col, offset='center')\n    return x, y\n\n\ndef to_1d(r, c, cols):\n    \"\"\"\n    Convert 2D (row, col) coordinates to 1D index.\n    \n    Args:\n        r (int): Row index\n        c (int): Column index\n        cols (int): Number of columns in the raster\n        \n    Returns:\n        int: 1D index\n    \"\"\"\n    return r * cols + c\n\n\ndef process_raster(cost_array, walking_speed, cell_size=100):\n    \"\"\"\n    Convert cost raster to travel time (in hours).\n    \n    Args:\n        cost_array (numpy.ndarray): Cost array\n        walking_speed (float): Walking speed in m/s\n        cell_size (float): Cell size in meters\n        \n    Returns:\n        numpy.ndarray: Travel time array in hours\n    \"\"\"\n    # Multiply by cell size to get distance\n    cost_array = cost_array * cell_size  \n    # Convert to seconds (m / (m/s) = s)\n    cost_array = cost_array / walking_speed  \n    # Convert seconds to hours\n    cost_array = cost_array / 3600  \n    # Replace infinite values with -1\n    cost_array[np.isinf(cost_array)] = -1\n    \n    return cost_array\n</pre> \"\"\" Raster operations and coordinate conversion utilities. \"\"\"  import numpy as np import rasterio import rasterio.warp from rasterio.transform import xy   def resample_raster(source_path, target_path, output_path, resampling_method=rasterio.warp.Resampling.bilinear):     \"\"\"     Resample a source raster to match the grid of a target raster.          Args:         source_path (str): Path to the source raster         target_path (str): Path to the target raster (for grid reference)         output_path (str): Path to save the resampled raster         resampling_method: Resampling method to use              Returns:         tuple: (resampled_array, resampled_meta)     \"\"\"     # Load source raster     with rasterio.open(source_path) as src:         source_array = src.read(1)  # Read the first band         source_meta = src.meta.copy()         source_nodata = src.nodata         source_transform = src.transform         source_crs = src.crs          # Replace NoData values with np.nan     source_array = np.where(source_array == source_nodata, np.nan, source_array)          # Get target grid properties     with rasterio.open(target_path) as src:         target_meta = src.meta.copy()         target_transform = src.transform         target_crs = src.crs         target_shape = src.shape          # Set up the output metadata     resampled_meta = target_meta.copy()     resampled_meta.update({         'dtype': 'float32',         'count': 1,         'nodata': np.nan,         'driver': 'GTiff'     })          # Create an empty array for the resampled data     resampled_array = np.empty(target_shape, dtype=np.float32)          # Perform the resampling     rasterio.warp.reproject(         source=source_array,         destination=resampled_array,         src_transform=source_transform,         src_crs=source_crs,         dst_transform=target_transform,         dst_crs=target_crs,         resampling=resampling_method     )          # Replace any remaining NoData values with np.nan     resampled_array = np.where(np.isnan(resampled_array), np.nan, resampled_array)          # Save the resampled raster     with rasterio.open(output_path, 'w', **resampled_meta) as dst:         dst.write(resampled_array, 1)          return resampled_array, resampled_meta   def coords_to_raster(gdf, transform, bounds, res):     \"\"\"     Convert geographic coordinates to raster row/column coordinates.          Args:         gdf (GeoDataFrame): GeoDataFrame containing point geometries         transform: The raster's affine transform         bounds: The raster's bounds         res: The raster's resolution              Returns:         list: List of (row, col) coordinates     \"\"\"     raster_coords = []     for point in gdf.geometry:         x, y = point.x, point.y         if not (bounds.left &lt;= x &lt;= bounds.right and bounds.bottom &lt;= y &lt;= bounds.top):             print(f\"Point {x}, {y} is out of raster bounds.\")             continue         col = int((x - bounds.left) / res[0])         row = int((bounds.top - y) / res[1])         raster_coords.append((row, col))     return raster_coords   def raster_coord_to_map_coords(row, col, transform):     \"\"\"     Convert raster (row, col) coordinates to map (x, y) coordinates.          Args:         row (int): Raster row index         col (int): Raster column index         transform: Raster affine transform              Returns:         tuple: (x, y) map coordinates     \"\"\"     x, y = xy(transform, row, col, offset='center')     return x, y   def to_1d(r, c, cols):     \"\"\"     Convert 2D (row, col) coordinates to 1D index.          Args:         r (int): Row index         c (int): Column index         cols (int): Number of columns in the raster              Returns:         int: 1D index     \"\"\"     return r * cols + c   def process_raster(cost_array, walking_speed, cell_size=100):     \"\"\"     Convert cost raster to travel time (in hours).          Args:         cost_array (numpy.ndarray): Cost array         walking_speed (float): Walking speed in m/s         cell_size (float): Cell size in meters              Returns:         numpy.ndarray: Travel time array in hours     \"\"\"     # Multiply by cell size to get distance     cost_array = cost_array * cell_size       # Convert to seconds (m / (m/s) = s)     cost_array = cost_array / walking_speed       # Convert seconds to hours     cost_array = cost_array / 3600       # Replace infinite values with -1     cost_array[np.isinf(cost_array)] = -1          return cost_array"},{"location":"workflow/cost-surface.html","title":"Cost Surface Generation","text":"<p>This page describes the process of generating cost surfaces for volcanic evacuation analysis. Cost surfaces represent the difficulty of traversing different types of terrain and are essential for realistic evacuation route modeling.</p>"},{"location":"workflow/cost-surface.html#overview","title":"Overview","text":"<p>The cost surface generation workflow combines two key factors:</p> <ol> <li>Slope-based costs: Derived from the Digital Elevation Model (DEM) using Tobler's hiking function</li> <li>Land cover costs: Assigned based on different terrain types from land cover classification</li> </ol> <p>These factors are integrated to create comprehensive cost surfaces that account for both topographic and land cover characteristics, resulting in realistic travel time estimates.</p>"},{"location":"workflow/cost-surface.html#prerequisites","title":"Prerequisites","text":"<p>Before generating cost surfaces, ensure you have:</p> <ol> <li>Completed the Data Acquisition step</li> <li>Downloaded DEM and land cover data for your volcano of interest</li> <li>Installed the required dependencies as specified in the Installation Requirements</li> </ol>"},{"location":"workflow/cost-surface.html#workflow-diagram","title":"Workflow Diagram","text":"<p>The cost surface generation process follows these steps:</p> <pre><code>flowchart TD\n    DEM[DEM Data] --&gt; SlopeCalc[Calculate Slope in 8 Directions]\n    SlopeCalc --&gt; ToblerFunc[Apply Tobler's Hiking Function]\n    ToblerFunc --&gt; NormalizeSpeed[Normalize Walking Speed]\n\n    LandCover[Land Cover Data] --&gt; ClassCost[Assign Costs to Land Cover Classes]\n    ClassCost --&gt; Mask[Apply Masks for Impassable Areas]\n\n    NormalizeSpeed --&gt; CombineCosts[Combine Costs]\n    Mask --&gt; CombineCosts\n\n    CombineCosts --&gt; InvertCosts[Invert Cost Surface]\n    InvertCosts --&gt; FinalCost[Final Cost Surface]\n\n    style DEM fill:#d4f1f9,stroke:#333\n    style LandCover fill:#d4f1f9,stroke:#333\n    style FinalCost fill:#75b79e,stroke:#333,stroke-width:2px</code></pre>"},{"location":"workflow/cost-surface.html#step-1-calculate-slope-from-dem","title":"Step 1: Calculate Slope from DEM","text":"<p>First, calculate the slope in eight directions (cardinal and intercardinal) from the DEM:</p> <pre><code>from dem_processing import calculate_slope\nimport numpy as np\nimport rasterio\n\n# Load the DEM\nwith rasterio.open(\"Volcano_DEM_100m_Buffer_UTM.tif\") as src:\n    dem_data = src.read(1)\n    dem_meta = src.meta\n    resolution_x = src.res[0]\n    resolution_y = src.res[1]\n    no_data = src.nodata\n\n# Calculate slope in 8 directions\nslope_array = calculate_slope(dem_data, resolution_x, resolution_y, no_data)\n\n# slope_array shape: (8, rows, cols) - one array for each direction\nprint(f\"Calculated slope in 8 directions. Shape: {slope_array.shape}\")\n</code></pre> <p>The resulting <code>slope_array</code> contains slopes for each of the eight movement directions: North, South, East, West, North-East, North-West, South-East, and South-West.</p>"},{"location":"workflow/cost-surface.html#step-2-apply-toblers-hiking-function","title":"Step 2: Apply Tobler's Hiking Function","text":"<p>Next, apply Tobler's hiking function to convert slopes to walking speeds:</p> <pre><code>from dem_processing import calculate_walking_speed, normalize_walking_speed\n\n# Calculate walking speed using Tobler's function\nwalking_speed_array = calculate_walking_speed(slope_array)\n\n# Normalize walking speed by dividing by max speed (occurs at slight downhill)\nnormalized_ws = normalize_walking_speed(walking_speed_array)\n\nprint(f\"Applied Tobler's hiking function to calculate walking speeds\")\n</code></pre> <p>Tobler's hiking function models how walking speed varies with slope:</p> \\[ V = 6 \\times e^{-3.5 \\times |S + 0.05|} \\] <p>Where: - V is the walking speed in km/h - S is the slope in rise/run (not degrees) - The constant 0.05 accounts for the observation that walking is slightly faster on gentle downhill slopes</p>"},{"location":"workflow/cost-surface.html#step-3-assign-costs-to-land-cover-classes","title":"Step 3: Assign Costs to Land Cover Classes","text":"<p>In parallel, assign cost factors to different land cover types:</p> <pre><code>from cost_calculations import map_landcover_to_cost\nimport rasterio\n\n# Load land cover data\nwith rasterio.open(\"Volcano_LandCover_2019_100m_Buffer_UTM.tif\") as src:\n    landcover_data = src.read(1)\n    lc_meta = src.meta\n\n# Define cost mapping for different land cover classes\n# Values represent difficulty multipliers (1.0 = normal speed, 2.0 = half speed)\nland_cover_cost_mapping = {\n    0: np.nan,       # No data\n    20: 1.0,         # Shrubs\n    30: 1.2,         # Herbaceous vegetation\n    40: 1.5,         # Cropland\n    50: 2.0,         # Urban/built-up\n    60: 1.0,         # Bare/sparse vegetation\n    70: 1.5,         # Snow and ice\n    80: 1.2,         # Permanent water bodies\n    90: 2.0,         # Herbaceous wetland\n    100: 2.0,        # Moss and lichen\n    111: 1.3,        # Closed forest, evergreen needle leaf\n    112: 1.3,        # Closed forest, evergreen broad leaf\n    113: 1.3,        # Closed forest, deciduous needle leaf\n    114: 1.3,        # Closed forest, deciduous broad leaf\n    115: 1.3,        # Closed forest, mixed\n    116: 1.2,        # Closed forest, unknown\n    121: 1.2,        # Open forest, evergreen needle leaf\n    122: 1.2,        # Open forest, evergreen broad leaf\n    123: 1.2,        # Open forest, deciduous needle leaf\n    124: 1.2,        # Open forest, deciduous broad leaf\n    125: 1.2,        # Open forest, mixed\n    126: 1.1,        # Open forest, unknown\n    200: 1000.0      # Impassable (streams, cliffs)\n}\n\n# Map land cover classes to costs\nlandcover_cost = map_landcover_to_cost(landcover_data, land_cover_cost_mapping)\n\nprint(f\"Assigned costs to land cover classes\")\n</code></pre> <p>The land cover cost mapping assigns different travel difficulty factors to each land cover type. For example: - Open terrain (bare/sparse vegetation): 1.0 (no speed reduction) - Forests: 1.2-1.3 (20-30% speed reduction) - Urban areas: 2.0 (50% speed reduction) - Impassable areas (water bodies, steep cliffs): 1000.0 (effectively impassable)</p>"},{"location":"workflow/cost-surface.html#step-4-apply-masks-for-special-features","title":"Step 4: Apply Masks for Special Features","text":"<p>Some features may need special handling. For example, hiking trails should be easier to traverse, while streams may be impassable:</p> <pre><code>from cost_calculations import update_cost_raster, rasterize_layer\nimport geopandas as gpd\n\n# Load hiking path and streams from shapefiles\nhiking_paths = gpd.read_file(\"hiking_path.shp\")\nstreams = gpd.read_file(\"streams.shp\")\n\n# Rasterize hiking paths and streams\nwith rasterio.open(\"Volcano_DEM_100m_Buffer_UTM.tif\") as src:\n    transform = src.transform\n    out_shape = src.shape\n\nhiking_raster = rasterize_layer(\n    hiking_paths.geometry,\n    out_shape,\n    transform,\n    burn_value=1,\n    fill_value=np.nan\n)\n\nstream_raster = rasterize_layer(\n    streams.geometry,\n    out_shape,\n    transform,\n    burn_value=0,\n    fill_value=np.nan\n)\n\n# Update cost raster with hiking paths and streams\n# Hiking paths: set cost to 1 (easy to traverse)\n# Streams: set cost to 0 (impassable)\nupdated_cost_raster = update_cost_raster(landcover_cost, stream_raster, hiking_raster)\n\nprint(f\"Applied masks for hiking paths and streams\")\n</code></pre> <p>This step overlays vector features like hiking paths and streams onto the cost surface, making hiking paths easier to traverse and streams impassable.</p>"},{"location":"workflow/cost-surface.html#step-5-combine-slope-based-and-land-cover-costs","title":"Step 5: Combine Slope-Based and Land Cover Costs","text":"<p>Now combine the slope-based costs (from Tobler's function) with the land cover costs:</p> <pre><code>from cost_calculations import adjust_cost_with_walking_speed\n\n# Create separate bands for each direction\nadjusted_cost_array = np.zeros_like(normalized_ws)\n\n# Adjust cost by multiplying normalized walking speed with land cover costs\nfor direction in range(8):\n    adjusted_cost_array[direction] = adjust_cost_with_walking_speed(\n        normalized_ws[direction],\n        updated_cost_raster\n    )\n\nprint(f\"Combined slope-based and land cover costs\")\n</code></pre> <p>This multiplication effectively reduces walking speed based on both slope steepness and terrain type.</p>"},{"location":"workflow/cost-surface.html#step-6-invert-the-cost-surface-for-path-finding","title":"Step 6: Invert the Cost Surface for Path Finding","text":"<p>For path finding algorithms, we need to invert the cost surface (since higher speeds should result in lower costs):</p> <pre><code>from cost_calculations import invert_cost_array\n\n# Invert the cost array (1/cost) for use in path finding\ninverted_cost_array = invert_cost_array(adjusted_cost_array)\n\nprint(f\"Inverted cost surface for path finding\")\n</code></pre> <p>The inversion converts speed values to cost values, where: - Higher speeds \u2192 Lower costs - Lower speeds \u2192 Higher costs - Impassable areas \u2192 Very high cost or infinity</p>"},{"location":"workflow/cost-surface.html#step-7-save-the-final-cost-surface","title":"Step 7: Save the Final Cost Surface","text":"<p>Finally, save the cost surface for use in evacuation analysis:</p> <pre><code>import os\n\n# Save the inverted cost array\noutput_path = os.path.join(\"output\", \"inverted_cost_8_directions.tif\")\nwith rasterio.open(\n    output_path, \n    'w',\n    driver='GTiff',\n    height=inverted_cost_array.shape[1],\n    width=inverted_cost_array.shape[2],\n    count=inverted_cost_array.shape[0],\n    dtype=inverted_cost_array.dtype,\n    crs=dem_meta['crs'],\n    transform=dem_meta['transform'],\n    nodata=np.nan\n) as dst:\n    dst.write(inverted_cost_array)\n\nprint(f\"Saved final cost surface to {output_path}\")\n</code></pre>"},{"location":"workflow/cost-surface.html#visualizing-the-cost-surface","title":"Visualizing the Cost Surface","text":"<p>To better understand the cost surface, you can visualize it:</p> <pre><code>from plotting_utils import plot_continuous_raster_with_points\nimport geopandas as gpd\n\n# Load summit point\nsummit_point = gpd.read_file(\"summit.shp\")\n\n# Plot one direction (e.g., North)\nnorth_direction = 0\nplot_continuous_raster_with_points(\n    inverted_cost_array[north_direction],\n    [dem_meta['transform'][2], dem_meta['transform'][2] + dem_meta['transform'][0] * dem_meta['width'],\n     dem_meta['transform'][5] + dem_meta['transform'][4] * dem_meta['height'], dem_meta['transform'][5]],\n    summit_point,\n    \"Cost Surface (North Direction)\",\n    \"Cost Value\",\n    \"cost_surface_north.jpg\"\n)\n</code></pre>"},{"location":"workflow/cost-surface.html#scenario-analysis","title":"Scenario Analysis","text":"<p>You can generate multiple cost surfaces to compare different scenarios:</p> <ol> <li>Base Scenario: Standard land cover and slope costs</li> <li>Degraded Terrain: Increase costs for certain land cover types to simulate adverse conditions (e.g., wet ground after rainfall)</li> <li>Infrastructure Damage: Mark certain paths as impassable to simulate damaged infrastructure</li> <li>Seasonal Variation: Adjust costs based on seasonal conditions (e.g., snow cover in winter)</li> </ol> <p>Example of creating a modified scenario:</p> <pre><code># Modify land cover costs for a \"wet terrain\" scenario\nwet_terrain_mapping = land_cover_cost_mapping.copy()\nwet_terrain_mapping.update({\n    20: 1.5,         # Shrubs (increased difficulty when wet)\n    30: 1.8,         # Herbaceous vegetation (increased difficulty when wet)\n    60: 1.3,         # Bare/sparse vegetation (increased difficulty when wet)\n})\n\n# Generate a new cost surface with these modified values\n# ... (repeat steps 3-7 with the new mapping)\n</code></pre>"},{"location":"workflow/cost-surface.html#parameter-sensitivity","title":"Parameter Sensitivity","text":"<p>The cost surface is sensitive to several parameters:</p> <ol> <li>Land cover cost values: Different multipliers for land cover types</li> <li>Walking speed parameters: Constants in Tobler's function</li> <li>Special feature handling: How hiking paths and impassable features are treated</li> </ol> <p>It's recommended to perform sensitivity analysis by varying these parameters and comparing the resulting evacuation routes and times.</p>"},{"location":"workflow/cost-surface.html#next-steps","title":"Next Steps","text":"<p>With the cost surface generated, you can now proceed to the Evacuation Analysis workflow to:</p> <ol> <li>Calculate optimal evacuation routes from various starting points</li> <li>Estimate evacuation times for different scenarios</li> <li>Analyze the accessibility of safe zones</li> </ol>"},{"location":"workflow/cost-surface.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"workflow/cost-surface.html#missing-or-incorrect-land-cover-mapping","title":"Missing or Incorrect Land Cover Mapping","text":"<p>If land cover classes in your study area are not properly represented in the default mapping:</p> <ol> <li>Use GIS software to identify the unique land cover classes in your data</li> <li>Update the <code>land_cover_cost_mapping</code> dictionary to include all classes</li> <li>Consult literature or local knowledge to assign appropriate cost values</li> </ol>"},{"location":"workflow/cost-surface.html#unrealistic-travel-times","title":"Unrealistic Travel Times","text":"<p>If the calculated travel times seem unrealistic:</p> <ol> <li>Check the slope calculation to ensure units are correct (rise/run, not degrees)</li> <li>Verify the land cover cost multipliers are reasonable</li> <li>Consider calibrating the model using known travel times from field measurements</li> </ol>"},{"location":"workflow/cost-surface.html#memory-issues-with-large-datasets","title":"Memory Issues with Large Datasets","text":"<p>For very large study areas, you may encounter memory issues:</p> <ol> <li>Process the data in tiles or with a reduced buffer size</li> <li>Decrease the spatial resolution of the analysis</li> <li>Use a computer with more RAM or consider cloud computing options</li> </ol>"},{"location":"workflow/data-acquisition.html","title":"Data Acquisition","text":"<p>This page explains how to acquire the necessary geospatial data for volcanic evacuation analysis. The primary datasets needed are Digital Elevation Models (DEMs) and land cover classification data for the volcanic region of interest.</p>"},{"location":"workflow/data-acquisition.html#overview","title":"Overview","text":"<p>The data acquisition workflow leverages Google Earth Engine to download high-quality, consistent datasets for any volcano worldwide. The process:</p> <ol> <li>Defines a study area around the volcano</li> <li>Downloads elevation data from the ALOS World 3D-30m dataset</li> <li>Downloads land cover classification from Copernicus Global Land Cover</li> <li>Exports the data in appropriate projection for accurate analysis</li> </ol>"},{"location":"workflow/data-acquisition.html#required-datasets","title":"Required Datasets","text":"Dataset Description Source Resolution Digital Elevation Model (DEM) Elevation data for calculating slopes and terrain characteristics JAXA ALOS World 3D-30m 30m Land Cover Classification Categorizes terrain into different surface types (forest, urban, etc.) Copernicus Global Land Cover 100m"},{"location":"workflow/data-acquisition.html#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure you have:</p> <ol> <li>Installed the required dependencies as specified in the Installation Requirements</li> <li>Signed up for a Google Earth Engine account at https://earthengine.google.com/</li> <li>Authenticated with Google Earth Engine using the <code>earthengine-api</code> package</li> </ol>"},{"location":"workflow/data-acquisition.html#google-earth-engine-authentication","title":"Google Earth Engine Authentication","text":"<p>When running the data acquisition workflow for the first time, you'll need to authenticate with Google Earth Engine:</p> <pre><code>import ee\n\n# Try to initialize Earth Engine\ntry:\n    ee.Initialize()\n    print(\"Already authenticated with Google Earth Engine\")\nexcept:\n    # If initialization fails, authenticate first\n    ee.Authenticate()\n    ee.Initialize()\n    print(\"Authentication successful\")\n</code></pre> <p>The authentication process will open a browser window where you'll need to sign in with your Google account and authorize the Earth Engine application.</p>"},{"location":"workflow/data-acquisition.html#automatic-utm-projection-selection","title":"Automatic UTM Projection Selection","text":"<p>For accurate distance measurements, the data is exported in the appropriate Universal Transverse Mercator (UTM) projection for the volcano's location. The workflow automatically determines the correct UTM zone based on the volcano's coordinates:</p> <pre><code>def get_utm_epsg(latitude, longitude):\n    \"\"\"\n    Determine the EPSG code for the UTM zone appropriate for the given coordinates.\n\n    Args:\n        latitude (float): Latitude in decimal degrees\n        longitude (float): Longitude in decimal degrees\n\n    Returns:\n        str: EPSG code in the format \"EPSG:xxxxx\"\n    \"\"\"\n    # Make sure longitude is between -180 and 180\n    longitude = ((longitude + 180) % 360) - 180\n\n    # Calculate UTM zone number\n    zone_number = int(((longitude + 180) / 6) + 1)\n\n    # Determine EPSG code\n    if latitude &gt;= 0:\n        # Northern hemisphere\n        epsg = f\"EPSG:326{zone_number:02d}\"\n    else:\n        # Southern hemisphere\n        epsg = f\"EPSG:327{zone_number:02d}\"\n\n    return epsg\n</code></pre>"},{"location":"workflow/data-acquisition.html#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"workflow/data-acquisition.html#1-define-area-of-interest","title":"1. Define Area of Interest","text":"<p>First, define an area of interest (AOI) around your volcano's summit by creating a buffer:</p> <pre><code>def define_aoi(coords, buffer_distance=20000):\n    \"\"\"\n    Define an area of interest around a point.\n\n    Args:\n        coords (list): [longitude, latitude] of the point\n        buffer_distance (int): Buffer distance in meters\n\n    Returns:\n        tuple: (ee.Geometry, list) - AOI and region coordinates\n    \"\"\"\n    # Create a point from coordinates\n    point = ee.Geometry.Point(coords)\n\n    # Create a buffer around the point\n    aoi = point.buffer(buffer_distance)\n\n    # Get the region coordinates\n    region_coords = aoi.bounds().getInfo()['coordinates']\n\n    return aoi, region_coords\n</code></pre> <p>The buffer distance (default: 20 km) should be large enough to encompass potential evacuation routes and safe zones.</p>"},{"location":"workflow/data-acquisition.html#2-download-digital-elevation-model","title":"2. Download Digital Elevation Model","text":"<p>Next, download the DEM data for the defined area:</p> <pre><code>def download_dem(aoi, region_coords, scale, description, utm_epsg, \n                collection=\"JAXA/ALOS/AW3D30/V3_2\", band=\"DSM\"):\n    \"\"\"\n    Download DEM data for the specified area.\n    \"\"\"\n    # Load DEM collection\n    dem_collection = ee.ImageCollection(collection).select(band)\n\n    # Create a mosaic and clip to AOI\n    dem_mosaic = dem_collection.mosaic().clip(aoi)\n\n    # Reproject to UTM with specified scale\n    utm_projection = ee.Projection(utm_epsg).atScale(scale)\n    dem_utm = dem_mosaic.reproject(utm_projection)\n\n    # Export to Google Drive\n    task = ee.batch.Export.image.toDrive(\n        image=dem_utm,\n        description=f\"{description}_{scale}m_Buffer_UTM\",\n        scale=scale,\n        region=region_coords,\n        crs=utm_epsg,\n        maxPixels=1e13,\n        fileFormat='GeoTIFF'\n    )\n\n    # Start the task\n    task.start()\n\n    return task\n</code></pre>"},{"location":"workflow/data-acquisition.html#3-download-land-cover-data","title":"3. Download Land Cover Data","text":"<p>Similarly, download the land cover data:</p> <pre><code>def download_landcover(aoi, region_coords, year, scale, description, utm_epsg):\n    \"\"\"\n    Download land cover data for the specified area.\n    \"\"\"\n    # Load land cover dataset for the specified year\n    dataset = ee.Image(f'COPERNICUS/Landcover/100m/Proba-V-C3/Global/{year}').select('discrete_classification')\n\n    # Clip to AOI and reproject\n    clipped_dataset = dataset.clip(aoi)\n    utm_projection = ee.Projection(utm_epsg).atScale(scale)\n    dataset_utm = clipped_dataset.reproject(utm_projection)\n\n    # Export to Google Drive\n    task = ee.batch.Export.image.toDrive(\n        image=dataset_utm,\n        description=f\"{description}_{year}_{scale}m_Buffer_UTM\",\n        scale=scale,\n        region=region_coords,\n        crs=utm_epsg,\n        maxPixels=1e13,\n        fileFormat='GeoTIFF'\n    )\n\n    # Start the task\n    task.start()\n\n    return task\n</code></pre>"},{"location":"workflow/data-acquisition.html#4-combined-workflow-function","title":"4. Combined Workflow Function","text":"<p>A consolidated function makes it easy to download all necessary data for a volcano:</p> <pre><code>def download_volcano_data(volcano_name, lat, lon, buffer_distance=20000, \n                         scale=100, year=2019, utm_epsg=None):\n    \"\"\"\n    Download DEM and land cover data for a volcano with automatic UTM projection detection.\n    \"\"\"\n    # Get the proper UTM EPSG if not provided\n    if utm_epsg is None:\n        utm_epsg = get_utm_epsg(lat, lon)\n\n    # Define area of interest\n    coords = [lon, lat]  # GEE uses [longitude, latitude] order\n    aoi, region_coords = define_aoi(coords, buffer_distance)\n\n    # Download DEM\n    dem_task = download_dem(\n        aoi, region_coords, scale, f\"{volcano_name}_DEM\", utm_epsg\n    )\n\n    # Download land cover\n    lc_task = download_landcover(\n        aoi, region_coords, year, scale, f\"{volcano_name}_LandCover\", utm_epsg\n    )\n\n    return dem_task, lc_task\n</code></pre>"},{"location":"workflow/data-acquisition.html#example-downloading-data-for-mount-marapi","title":"Example: Downloading Data for Mount Marapi","text":"<p>Here's an example of downloading data for Mount Marapi in Indonesia:</p> <pre><code># Mount Marapi coordinates in Sumatra\nMARAPI_LAT = -0.3775\nMARAPI_LON = 100.4721\n\n# Download data for Mount Marapi with automatic UTM detection\nmarapi_tasks = download_volcano_data(\n    volcano_name=\"Marapi\", \n    lat=MARAPI_LAT,\n    lon=MARAPI_LON,\n    buffer_distance=20000,  # 20 km buffer\n    scale=100  # 100 m resolution\n)\n</code></pre>"},{"location":"workflow/data-acquisition.html#monitoring-export-progress","title":"Monitoring Export Progress","text":"<p>Google Earth Engine exports run asynchronously in the cloud. You can monitor the status using:</p> <pre><code>def check_task_status(task):\n    \"\"\"Check the status of an Earth Engine export task.\"\"\"\n    status = task.status()\n    if status['state'] == 'COMPLETED':\n        print(f\"Task {status['description']} completed successfully!\")\n    elif status['state'] == 'FAILED':\n        print(f\"Task {status['description']} failed: {status['error_message']}\")\n    else:\n        print(f\"Task {status['description']} is {status['state']}\")\n    return status\n\n# Check both tasks\ndem_status = check_task_status(marapi_tasks[0])\nlc_status = check_task_status(marapi_tasks[1])\n</code></pre> <p>The export process may take several minutes to complete. Once finished, the files will be available in your Google Drive.</p>"},{"location":"workflow/data-acquisition.html#accessing-downloaded-files","title":"Accessing Downloaded Files","text":"<p>After the export tasks complete:</p> <ol> <li>Go to your Google Drive</li> <li>Look for files named <code>[VolcanoName]_DEM_100m_Buffer_UTM.tif</code> and <code>[VolcanoName]_LandCover_2019_100m_Buffer_UTM.tif</code></li> <li>Download these files to your local project directory for further processing</li> </ol>"},{"location":"workflow/data-acquisition.html#data-validation","title":"Data Validation","text":"<p>Before proceeding to the next step, it's important to validate the downloaded data:</p> <ol> <li>Open the GeoTIFF files in a GIS software (QGIS, ArcGIS)</li> <li>Check that the data covers the expected area around the volcano</li> <li>Verify that there are no major gaps or artifacts in the data</li> <li>Confirm that the data is projected in the correct UTM zone</li> </ol>"},{"location":"workflow/data-acquisition.html#next-steps","title":"Next Steps","text":"<p>Once you have successfully downloaded and validated the required datasets, proceed to the Cost Surface Generation workflow to create cost surfaces for evacuation analysis.</p>"},{"location":"workflow/data-acquisition.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"workflow/data-acquisition.html#export-tasks-failing","title":"Export Tasks Failing","text":"<p>If your export tasks fail, check the following:</p> <ol> <li>Area Size: Very large areas may exceed GEE's processing limits. Try reducing the buffer size.</li> <li>Resolution: Exporting at very high resolution can cause failures. Try increasing the scale parameter.</li> <li>Google Drive Space: Ensure you have enough free space in your Google Drive.</li> <li>Authentication: Verify your Earth Engine authentication is still valid.</li> </ol>"},{"location":"workflow/data-acquisition.html#missing-or-incomplete-data","title":"Missing or Incomplete Data","text":"<p>If the downloaded data has gaps or artifacts:</p> <ol> <li>Cloud Cover: Some regions may have cloud coverage in the original imagery. Try using alternative DEM sources.</li> <li>Regional Availability: Some datasets may have limited coverage in certain regions. Consider using alternative datasets.</li> </ol>"},{"location":"workflow/data-acquisition.html#alternative-data-sources","title":"Alternative Data Sources","text":"<p>If Google Earth Engine doesn't provide suitable data for your area of interest, consider these alternatives:</p> <ol> <li>SRTM DEM: Available from NASA Earth Data (https://earthdata.nasa.gov/)</li> <li>ASTER GDEM: Available from USGS Earth Explorer (https://earthexplorer.usgs.gov/)</li> <li>Local Government Data: Many countries provide national-level DEM and land cover datasets</li> </ol>"},{"location":"workflow/evacuation-analysis.html","title":"Evacuation Analysis","text":"<p>This page explains how to use the generated cost surfaces to analyze evacuation scenarios, calculate optimal routes, and determine evacuation times from volcanic hazards.</p>"},{"location":"workflow/evacuation-analysis.html#overview","title":"Overview","text":"<p>The evacuation analysis builds upon the cost surfaces created in the previous step to:</p> <ol> <li>Calculate optimal evacuation routes using Dijkstra's algorithm</li> <li>Determine travel times to safe zones for different walking speeds</li> <li>Analyze evacuation scenarios based on volcano-specific parameters</li> <li>Visualize results to support evacuation planning</li> </ol>"},{"location":"workflow/evacuation-analysis.html#prerequisites","title":"Prerequisites","text":"<p>Before performing evacuation analysis, ensure you have:</p> <ol> <li>Completed the Cost Surface Generation workflow</li> <li>Generated the inverted cost surface raster</li> <li>Prepared point data for evacuation sources (summit, camps, settlements)</li> <li>Defined safe zone parameters based on volcanic hazard assessments</li> </ol>"},{"location":"workflow/evacuation-analysis.html#workflow-diagram","title":"Workflow Diagram","text":"<p>The evacuation analysis process follows these steps:</p> <pre><code>flowchart TD\n    CostSurface[Inverted Cost Surface] --&gt; GraphConstruct[Construct Graph]\n    SourcePoints[Evacuation Source Points] --&gt; NodeIndex[Convert to Graph Nodes]\n\n    GraphConstruct --&gt; Dijkstra[Apply Dijkstra's Algorithm]\n    NodeIndex --&gt; Dijkstra\n\n    Dijkstra --&gt; EvacPaths[Extract Evacuation Paths]\n    Dijkstra --&gt; TravelTime[Calculate Travel Times]\n\n    SafeZones[Define Safe Zones] --&gt; Analyze[Analyze Safe Zone Accessibility]\n    TravelTime --&gt; Analyze\n\n    EvacPaths --&gt; Visualize[Visualize Results]\n    Analyze --&gt; Visualize\n\n    Visualize --&gt; Report[Generate Reports &amp; Statistics]\n\n    style CostSurface fill:#d4f1f9,stroke:#333\n    style SourcePoints fill:#d4f1f9,stroke:#333\n    style SafeZones fill:#d4f1f9,stroke:#333\n    style Report fill:#75b79e,stroke:#333,stroke-width:2px</code></pre>"},{"location":"workflow/evacuation-analysis.html#step-1-set-up-configuration-parameters","title":"Step 1: Set Up Configuration Parameters","text":"<p>First, configure the analysis parameters:</p> <pre><code>from config import WALKING_SPEEDS, SOURCE_NAMES, SAFE_ZONE_DISTANCES, DIRECTIONS\n\n# Walking speeds in m/s for different scenarios\n# These represent slow, medium, and fast walking paces\nprint(f\"Walking speeds: {WALKING_SPEEDS}\")\n\n# Source locations to analyze (summit and camping areas)\nprint(f\"Evacuation sources: {SOURCE_NAMES}\")\n\n# Safe zone distances in meters\nprint(f\"Safe zone distances: {SAFE_ZONE_DISTANCES}\")\n\n# Movement directions (cardinal and intercardinal)\nprint(f\"Movement directions: {DIRECTIONS}\")\n</code></pre> <p>The key parameters include: - Walking speeds: Different speeds for various population groups or scenarios - Source locations: Starting points for evacuation (summit, camps, settlements) - Safe zone distances: Distances from the summit considered safe - Movement directions: The 8 directions for path finding (cardinal and intercardinal)</p>"},{"location":"workflow/evacuation-analysis.html#step-2-load-cost-surface-and-convert-to-graph","title":"Step 2: Load Cost Surface and Convert to Graph","text":"<p>Next, load the inverted cost surface and convert it to a graph structure for path finding:</p> <pre><code>from io_utils import read_raster\nfrom path_utils import build_adjacency_matrix\nimport numpy as np\n\n# Load the inverted cost raster\ncost_array, meta, transform, nodata, bounds, resolution = read_raster(\"inverted_cost_8_directions.tif\")\n\n# Get dimensions\nbands, rows, cols = cost_array.shape\nprint(f\"Cost surface dimensions: {bands} bands, {rows} rows, {cols} columns\")\n\n# Build adjacency matrix (graph) for path finding\ngraph_csr = build_adjacency_matrix(cost_array, rows, cols, DIRECTIONS)\nprint(f\"Built graph with {graph_csr.shape[0]} nodes and {graph_csr.nnz} edges\")\n</code></pre> <p>This step constructs a sparse graph representation where: - Each cell in the raster becomes a node in the graph - Connections between adjacent cells become edges - Edge weights are determined by the cost of moving between cells</p>"},{"location":"workflow/evacuation-analysis.html#step-3-define-source-locations-and-convert-to-graph-nodes","title":"Step 3: Define Source Locations and Convert to Graph Nodes","text":"<p>Now, convert the source locations (evacuation starting points) to graph nodes:</p> <pre><code>from io_utils import read_shapefile\nfrom grid_utils import coords_to_raster, to_1d\n\n# Load source points (summit, camps, etc.)\nsummit_gdf = read_shapefile(\"summit.shp\")\ncamps_gdf = read_shapefile(\"camps.shp\")\n\n# Convert geographic coordinates to raster row/col coordinates\nsummit_coords = coords_to_raster(summit_gdf, transform, bounds, resolution)\ncamps_coords = coords_to_raster(camps_gdf, transform, bounds, resolution)\n\n# Combine all source coordinates\nsource_coords = summit_coords + camps_coords\nprint(f\"Source coordinates (row, col): {source_coords}\")\n\n# Convert 2D source coordinates to 1D node indices for graph\nsource_nodes = [to_1d(r, c, cols) for r, c in source_coords]\nprint(f\"Source nodes: {source_nodes}\")\n</code></pre> <p>This converts the geographic coordinates of evacuation sources (e.g., volcano summit, camping areas) to the corresponding nodes in the graph.</p>"},{"location":"workflow/evacuation-analysis.html#step-4-run-dijkstras-algorithm-for-shortest-paths","title":"Step 4: Run Dijkstra's Algorithm for Shortest Paths","text":"<p>Apply Dijkstra's algorithm to find the shortest paths from all source points:</p> <pre><code>from analysis import run_dijkstra_analysis\n\n# Run Dijkstra's algorithm to find shortest paths from all sources\ndistances, predecessors = run_dijkstra_analysis(graph_csr, source_nodes)\n\nprint(f\"Calculated shortest paths from {len(source_nodes)} sources\")\n\n# Store predecessor arrays for path reconstruction\n# Assuming summit is the first source\npredecessor_summit = predecessors[0]\n</code></pre> <p>Dijkstra's algorithm efficiently finds the optimal paths from each source point to all other points in the area. The algorithm returns: - Distances: Accumulated cost to reach each cell from each source - Predecessors: For each cell, which neighboring cell leads back to the source</p>"},{"location":"workflow/evacuation-analysis.html#step-5-calculate-distance-from-summit-for-safe-zones","title":"Step 5: Calculate Distance from Summit for Safe Zones","text":"<p>Calculate the Euclidean distance from the summit to define safe zones:</p> <pre><code>from grid_utils import calculate_distance_from_summit\n\n# Calculate distance from summit for defining safe zones\nsummit_row, summit_col = source_coords[0]  # Assuming summit is the first source\ndistance_from_summit = calculate_distance_from_summit(\n    (summit_row, summit_col), rows, cols, cell_size=resolution[0])\n\nprint(f\"Calculated Euclidean distance from summit\")\n</code></pre> <p>This creates a raster where each cell contains its straight-line distance from the volcano summit, which will be used to define safe zones at various distances.</p>"},{"location":"workflow/evacuation-analysis.html#step-6-process-travel-times-for-different-walking-speeds","title":"Step 6: Process Travel Times for Different Walking Speeds","text":"<p>Convert cost distances to travel times for different walking speeds:</p> <pre><code>from analysis import process_travel_times\nimport os\n\n# Create output directory if it doesn't exist\nos.makedirs(\"output\", exist_ok=True)\n\n# Process travel times for each source and walking speed\nfor i, source_name in enumerate(SOURCE_NAMES):\n    for speed_name, speed_value in WALKING_SPEEDS.items():\n        # Convert cost distance to travel time (in hours)\n        # Save results as GeoTIFF files\n        travel_time_array = process_travel_times(\n            f\"cost_distance_{source_name}.tif\",\n            source_name,\n            \"final\",  # dataset key\n            speed_name,\n            speed_value,\n            meta\n        )\n\n        print(f\"Processed travel times for {source_name} at {speed_name} speed\")\n</code></pre> <p>This step converts the abstract cost distances into practical travel times based on different walking speeds, which are more meaningful for evacuation planning.</p>"},{"location":"workflow/evacuation-analysis.html#step-7-analyze-safe-zone-accessibility","title":"Step 7: Analyze Safe Zone Accessibility","text":"<p>Now analyze how long it takes to reach safe zones from each evacuation source:</p> <pre><code>from analysis import analyze_safe_zones\n\n# First, collect all the travel time data\ntravel_time_data = {}\nfor speed_name in WALKING_SPEEDS.keys():\n    travel_time_data[speed_name] = {}\n    for source_name in SOURCE_NAMES:\n        # Load travel time raster\n        travel_time_path = f\"output/cost_distance_{source_name}_final_{speed_name}_hours.tif\"\n        with rasterio.open(travel_time_path) as src:\n            travel_time_array = src.read(1)\n            # Replace nodata with NaN\n            travel_time_array = np.where(\n                travel_time_array == src.nodata, \n                np.nan, \n                travel_time_array\n            )\n\n        travel_time_data[speed_name][source_name] = {\n            'cost_array': travel_time_array,\n            'cost_array_flat': travel_time_array.ravel()\n        }\n\n# Analyze minimum travel times to reach each safe zone\nresults, min_coords = analyze_safe_zones(\n    distance_from_summit,\n    travel_time_data,\n    SAFE_ZONE_DISTANCES,\n    SOURCE_NAMES\n)\n\nprint(f\"Analyzed safe zone accessibility\")\n</code></pre> <p>This analysis identifies: - The minimum travel time to reach each safe zone from each source - The location (coordinates) where that minimum time is achieved - How different walking speeds impact evacuation feasibility</p>"},{"location":"workflow/evacuation-analysis.html#step-8-decompose-factor-contributions","title":"Step 8: Decompose Factor Contributions","text":"<p>To better understand what influences evacuation routes, decompose the contributing factors:</p> <pre><code>from decomposition import run_decomposition_analysis\n\n# Prepare dataset info for decomposition\ndataset_info = {\n    'final': {\n        'pred_summit': predecessor_summit,\n        'cols': cols,\n        'summit_raster_coords': source_coords[0]\n    }\n}\n\n# Run decomposition analysis to understand factor contributions\ndecomp_data = run_decomposition_analysis(dataset_info, min_coords)\n\nprint(f\"Decomposed factor contributions to evacuation paths\")\n</code></pre> <p>This analysis reveals the relative importance of slope versus land cover in determining the optimal evacuation paths, which can inform mitigation strategies.</p>"},{"location":"workflow/evacuation-analysis.html#step-9-save-analysis-results","title":"Step 9: Save Analysis Results","text":"<p>Save the results to various formats for reporting and further analysis:</p> <pre><code>from io_utils import save_analysis_report, save_metrics_csv\nfrom visualization import create_decomposition_table, create_final_evacuation_table\n\n# Save analysis report and metrics as text and CSV\nreport_path = save_analysis_report(\n    \"output/evacuation_analysis_report.txt\",\n    results,\n    min_coords,\n    SOURCE_NAMES,\n    WALKING_SPEEDS,\n    SAFE_ZONE_DISTANCES\n)\n\ncsv_path = save_metrics_csv(\n    \"output/evacuation_metrics.csv\",\n    results,\n    min_coords,\n    SOURCE_NAMES,\n    WALKING_SPEEDS,\n    SAFE_ZONE_DISTANCES\n)\n\n# Create visual tables\ndecomp_table_path = create_decomposition_table(\n    decomp_data,\n    \"output/factor_decomposition_table.png\"\n)\n\nevac_table_path = create_final_evacuation_table(\n    {\"final\": results},\n    SOURCE_NAMES,\n    \"output/evacuation_time_table.png\"\n)\n\nprint(f\"Saved analysis results and visualizations\")\n</code></pre> <p>These reports and visualizations make the analysis results accessible for emergency planning and public communication.</p>"},{"location":"workflow/evacuation-analysis.html#step-10-visualize-evacuation-paths-and-travel-times","title":"Step 10: Visualize Evacuation Paths and Travel Times","text":"<p>Create visualizations to communicate the evacuation scenarios:</p> <pre><code>from path_utils import reconstruct_path\nfrom visualization import plot_travel_time_comparison, plot_cost_surface_with_paths\n\n# Reconstruct evacuation paths for visualization\nevacuation_paths = {}\nevacuation_paths['final'] = {}\n\nfor safe_zone in SAFE_ZONE_DISTANCES:\n    # Get coordinates of minimum travel time point for medium speed\n    target_coords = min_coords['medium'][safe_zone][0]  # Summit source\n    if not np.isnan(target_coords[0]):\n        # Convert to 1D node index\n        target_node = to_1d(int(target_coords[0]), int(target_coords[1]), cols)\n        # Reconstruct path\n        path = reconstruct_path(predecessor_summit, source_nodes[0], target_node, cols)\n        evacuation_paths['final'][safe_zone] = path\n\n# Create comparison plot of travel times\ncomparison_plot = plot_travel_time_comparison(\n    {\"final\": results},\n    SAFE_ZONE_DISTANCES,\n    SOURCE_NAMES,\n    WALKING_SPEEDS\n)\n\n# Create cost surface plot with evacuation paths\ncost_surface_plot = plot_cost_surface_with_paths(\n    dataset_info,\n    cost_arrays,\n    transforms,\n    evacuation_paths,\n    summit_coords,\n    SAFE_ZONE_DISTANCES,\n    hiking_gdf,\n    output_path=\"output/cost_surface_with_paths.jpg\"\n)\n\nprint(f\"Created visualization plots\")\n</code></pre> <p>These visualizations help communicate evacuation options, illustrating: - Optimal evacuation routes to different safe zones - How travel times vary with distance from the volcano - The impact of different walking speeds on evacuation feasibility</p>"},{"location":"workflow/evacuation-analysis.html#interpreting-the-results","title":"Interpreting the Results","text":"<p>The key outputs from the evacuation analysis include:</p> <ol> <li>Evacuation Routes: Optimal paths from each source to safe zones</li> <li>Travel Times: How long it takes to reach safety under different conditions</li> <li>Safe Zone Accessibility: Which safe zones can be reached within specific time frames</li> <li>Factor Contributions: How slope and land cover influence evacuation paths</li> </ol> <p>These results can inform: - Evacuation Planning: Where to direct evacuees based on their location - Resource Allocation: Where to position emergency supplies and support - Risk Communication: Realistic time estimates for public awareness - Mitigation Efforts: Improvements to evacuation infrastructure</p>"},{"location":"workflow/evacuation-analysis.html#scenario-comparison","title":"Scenario Comparison","text":"<p>To analyze multiple scenarios (e.g., original terrain vs. modified terrain), repeat the analysis with different cost surfaces:</p> <pre><code># Load another cost surface for comparison\nmodified_cost_array, modified_meta = read_raster(\"inverted_cost_8_directions_modified.tif\")\n\n# Run analysis on modified scenario\n# ... (repeat steps 2-9 with modified data)\n\n# Compare results\nfor source_idx, source_name in enumerate(SOURCE_NAMES):\n    for speed_name in WALKING_SPEEDS.keys():\n        for safe_zone in SAFE_ZONE_DISTANCES:\n            original_time = results[speed_name][safe_zone][source_idx]\n            modified_time = modified_results[speed_name][safe_zone][source_idx]\n\n            if not np.isnan(original_time) and not np.isnan(modified_time):\n                diff_percent = ((modified_time - original_time) / original_time) * 100\n                print(f\"{source_name}, {speed_name}, {safe_zone}m: Original={original_time:.2f}h, \"\n                      f\"Modified={modified_time:.2f}h, Difference={diff_percent:.1f}%\")\n</code></pre>"},{"location":"workflow/evacuation-analysis.html#next-steps","title":"Next Steps","text":"<p>For more advanced analysis, consider:</p> <ol> <li>Integrating Eruption Probabilities: Incorporate VEI-specific probability surfaces</li> <li>Temporal Analysis: Model how evacuation scenarios change during different eruption phases</li> <li>Population Modeling: Include population distribution to estimate evacuation capacity needs</li> <li>Infrastructure Assessment: Analyze how damaged infrastructure impacts evacuation options</li> </ol> <p>To incorporate eruption probability thresholds, proceed to the Probability Analysis module (if available).</p>"},{"location":"workflow/evacuation-analysis.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"workflow/evacuation-analysis.html#no-valid-paths-found","title":"No Valid Paths Found","text":"<p>If no valid evacuation paths are found:</p> <ol> <li>Check for disconnected regions in your cost surface (e.g., completely impassable areas)</li> <li>Verify source and safe zone coordinates are within the study area</li> <li>Inspect the graph construction for potential errors</li> </ol>"},{"location":"workflow/evacuation-analysis.html#unrealistic-travel-times","title":"Unrealistic Travel Times","text":"<p>If travel times seem improbable:</p> <ol> <li>Check the units in your calculations (meters vs. kilometers, hours vs. minutes)</li> <li>Verify the walking speed values are appropriate for your terrain</li> <li>Ensure the cost surface has been properly normalized and inverted</li> </ol>"},{"location":"workflow/evacuation-analysis.html#memory-limitations","title":"Memory Limitations","text":"<p>For large study areas:</p> <ol> <li>Reduce the spatial resolution of your analysis</li> <li>Process the area in tiles or sectors</li> <li>Use a more memory-efficient sparse graph implementation</li> </ol>"},{"location":"workflow/installation-requirements.html","title":"Installation Requirements","text":"<p>This page details all the required libraries and dependencies needed to run the Volcano Pedestrian Evacuation Analysis toolkit. Install these packages before proceeding with the analysis workflow.</p>"},{"location":"workflow/installation-requirements.html#environment-setup","title":"Environment Setup","text":"<p>We recommend using a virtual environment to manage dependencies. You can create one using either conda or venv:</p> Using condaUsing venv <pre><code>conda create -n volcano-evacuation python=3.8\nconda activate volcano-evacuation\n</code></pre> <pre><code>python -m venv volcano-env\n# On Windows\nvolcano-env\\Scripts\\activate\n# On macOS/Linux\nsource volcano-env/bin/activate\n</code></pre>"},{"location":"workflow/installation-requirements.html#core-dependencies","title":"Core Dependencies","text":"<p>These core packages are essential for the basic functionality of the toolkit:</p> <pre><code>pip install numpy scipy pandas matplotlib tqdm\n</code></pre> Package Version Purpose numpy &gt;=1.20.0 Array processing and numerical operations scipy &gt;=1.7.0 Sparse matrices and graph algorithms for path finding pandas &gt;=1.3.0 Data analysis and statistical summaries matplotlib &gt;=3.4.0 Visualization of results and map creation tqdm &gt;=4.60.0 Progress tracking for long-running operations"},{"location":"workflow/installation-requirements.html#geospatial-processing-libraries","title":"Geospatial Processing Libraries","text":"<p>These libraries handle the geospatial data processing and analysis:</p> <pre><code>pip install rasterio geopandas pyproj fiona affine\n</code></pre> Package Version Purpose rasterio &gt;=1.2.0 Reading and writing raster data (DEM, land cover) geopandas &gt;=0.9.0 Working with vector data (points, paths, polygons) pyproj &gt;=3.1.0 Coordinate system transformations and UTM projections fiona &gt;=1.8.0 Low-level vector data I/O operations affine &gt;=2.3.0 Working with affine transformations in raster data"},{"location":"workflow/installation-requirements.html#google-earth-engine-integration","title":"Google Earth Engine Integration","text":"<p>For remote sensing data acquisition from Google Earth Engine:</p> <pre><code>pip install earthengine-api\n</code></pre> <p>You'll need to authenticate with Google Earth Engine. The data acquisition notebook will guide you through this process, or you can run:</p> <pre><code>earthengine authenticate\n</code></pre>"},{"location":"workflow/installation-requirements.html#visualization-enhancements-optional","title":"Visualization Enhancements (Optional)","text":"<p>For enhanced visualization capabilities:</p> <pre><code>pip install contextily folium seaborn\n</code></pre> Package Purpose contextily Adding basemaps to matplotlib/geopandas maps folium Creating interactive maps with evacuation routes seaborn Enhanced statistical visualizations"},{"location":"workflow/installation-requirements.html#complete-installation","title":"Complete Installation","text":"<p>For a complete installation of all required dependencies, you can use the requirements file:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Create a <code>requirements.txt</code> file in your project root with the following content:</p> <pre><code># Core Scientific Computing\nnumpy&gt;=1.20.0\nscipy&gt;=1.7.0\npandas&gt;=1.3.0\nmatplotlib&gt;=3.4.0\ntqdm&gt;=4.60.0\n\n# Geospatial Libraries\nrasterio&gt;=1.2.0\ngeopandas&gt;=0.9.0\npyproj&gt;=3.1.0\nfiona&gt;=1.8.0\naffine&gt;=2.3.0\n\n# Remote Sensing\nearthengine-api&gt;=0.1.290\n\n# Optional Visualization\ncontextily&gt;=1.2.0\nfolium&gt;=0.12.0\nseaborn&gt;=0.11.0\n</code></pre>"},{"location":"workflow/installation-requirements.html#testing-your-installation","title":"Testing Your Installation","text":"<p>You can test your installation by running the following Python script:</p> <pre><code># test_installation.py\nimport sys\nimport importlib\n\nrequired_packages = [\n    'numpy', 'scipy', 'pandas', 'matplotlib', 'tqdm',\n    'rasterio', 'geopandas', 'pyproj', 'fiona', 'affine', \n    'ee'\n]\n\nfor package in required_packages:\n    try:\n        importlib.import_module(package)\n        print(f\"\u2713 {package} successfully imported\")\n    except ImportError:\n        print(f\"\u2717 {package} MISSING\")\n\nprint(\"\\nIf any packages are missing, install them using pip:\")\nprint(\"pip install &lt;package_name&gt;\")\n</code></pre> <p>Save this as <code>test_installation.py</code> and run it with:</p> <pre><code>python test_installation.py\n</code></pre>"},{"location":"workflow/installation-requirements.html#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"workflow/installation-requirements.html#gdal-related-errors","title":"GDAL-Related Errors","text":"<p>If you encounter issues with GDAL (which underlies rasterio and fiona), try installing it separately:</p> Using condaUsing pip <pre><code>conda install -c conda-forge gdal\n</code></pre> <pre><code># On Windows\npip install pipwin\npipwin install gdal\n\n# On Ubuntu/Debian\nsudo apt-get install libgdal-dev\npip install gdal==$(gdal-config --version) --global-option=build_ext --global-option=\"-I/usr/include/gdal\"\n</code></pre>"},{"location":"workflow/installation-requirements.html#google-earth-engine-authentication-issues","title":"Google Earth Engine Authentication Issues","text":"<p>If you encounter issues authenticating with Google Earth Engine:</p> <ol> <li>Ensure you have a Google account with access to Earth Engine</li> <li>Try re-authenticating: <code>earthengine authenticate --quiet</code></li> <li>Check if your authentication credentials are stored properly:    <pre><code># On Linux/macOS\ncat ~/.config/earthengine/credentials\n# On Windows\ntype %USERPROFILE%\\.config\\earthengine\\credentials\n</code></pre></li> </ol>"},{"location":"workflow/installation-requirements.html#next-steps","title":"Next Steps","text":"<p>Once you have successfully installed all the required dependencies, proceed to the Data Acquisition step to begin downloading the necessary data for your analysis.</p>"},{"location":"workflow/workflow-overview.html","title":"Workflow Overview","text":"<p>The Volcano Pedestrian Evacuation Analysis toolkit follows a structured workflow that takes you from raw geographic data to comprehensive evacuation analysis. This page provides an overview of the complete analysis pipeline.</p>"},{"location":"workflow/workflow-overview.html#analysis-pipeline","title":"Analysis Pipeline","text":"<p>The workflow consists of four main stages:</p> <pre><code>graph LR\n    A[Data Acquisition] --&gt; B[Cost Surface Generation]\n    B --&gt; C[Evacuation Analysis]\n    C --&gt; D[Probability Analysis]\n\n    style A fill:#f9d5e5,stroke:#333,stroke-width:2px\n    style B fill:#eeac99,stroke:#333,stroke-width:2px\n    style C fill:#e06377,stroke:#333,stroke-width:2px\n    style D fill:#c83349,stroke:#333,stroke-width:2px</code></pre> <p>Each stage builds upon the outputs of the previous stage, creating a complete analysis pipeline.</p>"},{"location":"workflow/workflow-overview.html#stage-1-data-acquisition","title":"Stage 1: Data Acquisition","text":"<p>Purpose: Obtain high-quality elevation and land cover data for the study area</p> <p>Key Activities: - Define area of interest around the volcano - Download Digital Elevation Model (DEM) data - Download land cover classification data - Ensure proper projection and resolution</p> <p>Outputs: - DEM raster in GeoTIFF format - Land cover raster in GeoTIFF format</p> <p>Learn more about Data Acquisition \u2192</p>"},{"location":"workflow/workflow-overview.html#stage-2-cost-surface-generation","title":"Stage 2: Cost Surface Generation","text":"<p>Purpose: Create cost surfaces that represent the difficulty of traversing the terrain</p> <p>Key Activities: - Calculate slopes from the DEM - Apply Tobler's hiking function to estimate walking speeds - Assign travel costs to different land cover types - Combine slope-based and land cover-based costs - Generate final cost surfaces for different directions</p> <p>Outputs: - Walking speed rasters based on slope - Land cover cost rasters - Integrated cost surface rasters</p> <p>Learn more about Cost Surface Generation \u2192</p>"},{"location":"workflow/workflow-overview.html#stage-3-evacuation-analysis","title":"Stage 3: Evacuation Analysis","text":"<p>Purpose: Calculate optimal evacuation routes and travel times from various starting points</p> <p>Key Activities: - Define source locations (volcano summit, campsites, settlements) - Apply path-finding algorithms to find optimal evacuation routes - Calculate travel times for different walking speeds - Analyze safe zone accessibility based on distance from the volcano - Decompose evacuation routes to understand influential factors</p> <p>Outputs: - Evacuation route maps - Travel time analyses for different scenarios - Statistical summaries of evacuation scenarios</p> <p>Learn more about Evacuation Analysis \u2192</p>"},{"location":"workflow/workflow-overview.html#stage-4-probability-analysis","title":"Stage 4: Probability Analysis","text":"<p>Purpose: Incorporate eruption probability models to refine evacuation planning</p> <p>Key Activities: - Integrate volcanic hazard probability maps - Define safe zones based on probability thresholds - Compare evacuation scenarios for different eruption intensities (VEI levels) - Generate visualizations that communicate risk levels</p> <p>Outputs: - Probability-based evacuation maps - Scenario comparisons across VEI levels - Statistical reports integrating probability thresholds</p>"},{"location":"workflow/workflow-overview.html#modular-design","title":"Modular Design","text":"<p>The workflow is designed to be modular, allowing you to:</p> <ul> <li>Run individual stages independently</li> <li>Iterate on specific aspects without rerunning the entire pipeline</li> <li>Customize parameters at each stage to suit your specific research questions</li> <li>Compare multiple scenarios with different assumptions</li> </ul>"},{"location":"workflow/workflow-overview.html#data-flow-diagram","title":"Data Flow Diagram","text":"<p>The following diagram illustrates how data flows through the system:</p> <pre><code>graph TD\n    DEM[DEM Data] --&gt; Slope[Slope Calculation]\n    LandCover[Land Cover Data] --&gt; LCCost[Land Cover Cost Assignment]\n\n    Slope --&gt; WalkingSpeed[Walking Speed Calculation]\n    WalkingSpeed --&gt; CostSurface[Cost Surface Integration]\n    LCCost --&gt; CostSurface\n\n    CostSurface --&gt; GraphConstruction[Graph Construction]\n    GraphConstruction --&gt; PathFinding[Path Finding]\n\n    SourceLocations[Source Locations] --&gt; PathFinding\n    SafeZones[Safe Zone Definitions] --&gt; TravelTime[Travel Time Analysis]\n\n    PathFinding --&gt; TravelTime\n    TravelTime --&gt; Results[Result Visualization]\n\n    ProbabilityMap[Probability Maps] -.-&gt; SafeZones\n\n    subgraph \"Stage 1: Data Acquisition\"\n        DEM\n        LandCover\n    end\n\n    subgraph \"Stage 2: Cost Surface Generation\"\n        Slope\n        WalkingSpeed\n        LCCost\n        CostSurface\n    end\n\n    subgraph \"Stage 3: Evacuation Analysis\"\n        GraphConstruction\n        SourceLocations\n        SafeZones\n        PathFinding\n        TravelTime\n    end\n\n    subgraph \"Stage 4: Probability Analysis\"\n        ProbabilityMap\n        Results\n    end</code></pre>"},{"location":"workflow/workflow-overview.html#next-steps","title":"Next Steps","text":"<p>To begin working with the toolkit:</p> <ol> <li>First, ensure you have all required dependencies installed</li> <li>Proceed to the Data Acquisition step to obtain necessary data</li> <li>Follow each stage sequentially, reviewing outputs at each step</li> </ol> <p>Each section of the workflow documentation includes detailed instructions, code examples, and troubleshooting tips to guide you through the process.</p>"}]}